{
  "Tool": {
    "category": "Tool",
    "calculated_at": "2025-04-10T13:21:01",
    "ai_confidence": 32.0,
    "ai_mentions": 10.0,
    "ai_alignment": 40.0,
    "ai_depth": 20.0,
    "non_ai_confidence": 0,
    "final_score": 32.0,
    "reasoning": "The content discusses the practice of experimentation within agile workflows but does not explicitly mention any specific tools or software that facilitate this process. While it aligns conceptually with the themes of continuous improvement and team collaboration, it lacks depth in discussing tools or their practical application in Agile, Lean, or DevOps frameworks.",
    "level": "Ignored"
  },
  "Accountability": {
    "category": "Accountability",
    "calculated_at": "2025-04-10T13:21:01",
    "ai_confidence": 32.0,
    "ai_mentions": 10.0,
    "ai_alignment": 25.0,
    "ai_depth": 25.0,
    "non_ai_confidence": 0,
    "final_score": 32.0,
    "reasoning": "The content discusses experimentation in agile workflows, focusing on hypothesis-driven approaches and the importance of learning from failures. However, it does not explicitly address accountability as a structural mechanism or outcome ownership, which are central to the category. While there are elements of self-management and adaptation, the primary focus is on experimentation rather than accountability itself.",
    "level": "Ignored"
  },
  "Framework": {
    "category": "Framework",
    "calculated_at": "2025-04-10T13:21:01",
    "ai_confidence": 32.0,
    "ai_mentions": 15.0,
    "ai_alignment": 25.0,
    "ai_depth": 40.0,
    "non_ai_confidence": 0,
    "final_score": 32.0,
    "reasoning": "The content discusses experimentation within agile workflows, which is relevant to the Framework category. However, it primarily focuses on the practice of experimentation rather than providing a structured methodology or guidelines for implementing frameworks like Scrum or Kanban. While it touches on concepts of continuous improvement and adaptability, it lacks explicit discussions on specific frameworks or their implementation strategies, leading to a lower confidence score.",
    "level": "Ignored"
  },
  "Tenet": {
    "category": "Tenet",
    "calculated_at": "2025-04-10T13:21:01",
    "ai_confidence": 72.0,
    "ai_mentions": 15,
    "ai_alignment": 32,
    "ai_depth": 25,
    "non_ai_confidence": 0,
    "final_score": 72.0,
    "reasoning": "The content discusses the practice of experimentation within agile workflows, which aligns with the tenet of continuous improvement and evidence-based decision-making. It explicitly mentions the importance of hypothesis-driven approaches and iterative testing, which are actionable practices that guide teams. The depth of discussion is substantial, covering how experimentation fosters a culture of learning and adaptability, although it does not directly reference specific tenets from Agile or Lean methodologies. Overall, the content is focused on actionable practices that support the principles of the category, justifying a relatively high confidence score.",
    "level": "Secondary",
    "reasoning_summary": "This content is a strong fit for the category, as it explores how experimentation supports continuous improvement and informed decision-making in agile environments. By emphasising hypothesis-driven methods and iterative testing, it highlights practical steps teams can take, even though it doesn’t cite specific Agile or Lean tenets. The focus on actionable practices and fostering a learning culture makes the classification appropriate."
  },
  "Method": {
    "category": "Method",
    "calculated_at": "2025-04-10T13:21:01",
    "ai_confidence": 68.0,
    "ai_mentions": 15,
    "ai_alignment": 30,
    "ai_depth": 25,
    "non_ai_confidence": 0,
    "final_score": 68.0,
    "reasoning": "The content discusses experimentation as a practice within agile workflows, which aligns with the structured, step-by-step procedures characteristic of methods. It highlights the importance of hypothesis-driven approaches and iterative testing, which are key components of agile methodologies. However, while it touches on the procedural aspects, it does not delve deeply into specific methods like Scrum or Kanban, which slightly lowers the depth score. Overall, the content is primarily focused on the method of experimentation in agile contexts, justifying a moderate to high confidence score.",
    "level": "Secondary"
  },
  "Strategy": {
    "category": "Strategy",
    "calculated_at": "2025-04-10T13:21:01",
    "ai_confidence": 67.0,
    "ai_mentions": 15,
    "ai_alignment": 30,
    "ai_depth": 25,
    "non_ai_confidence": 0,
    "final_score": 67.0,
    "reasoning": "The content discusses the importance of experimentation in agile workflows, which aligns with strategic decision-making and adaptability. However, it primarily focuses on the practice of experimentation rather than explicitly tying it back to high-level strategic frameworks or organisational goals. While it mentions the significance of aligning with customer needs and organisational goals, the depth of strategic discussion is limited, making it a secondary focus rather than a primary one.",
    "level": "Secondary"
  },
  "Practice": {
    "category": "Practice",
    "calculated_at": "2025-04-10T13:21:01",
    "ai_confidence": 87.0,
    "ai_mentions": 18,
    "ai_alignment": 36,
    "ai_depth": 34,
    "non_ai_confidence": 50,
    "final_score": 87.0,
    "reasoning": "The content explicitly discusses experimentation as a critical practice in agile workflows, aligning closely with the core themes of continuous improvement and value delivery. It provides a detailed exploration of how hypothesis-driven approaches can enhance team effectiveness and adaptability, which are key aspects of the 'Practice' category. The depth of discussion on fostering a culture of experimentation and its impact on team dynamics further supports a high confidence score.",
    "level": "Primary"
  },
  "Philosophy": {
    "category": "Philosophy",
    "calculated_at": "2025-04-10T13:21:01",
    "ai_confidence": 42.0,
    "ai_mentions": 15.0,
    "ai_alignment": 35.0,
    "ai_depth": 30.0,
    "non_ai_confidence": 0,
    "final_score": 42.0,
    "reasoning": "The content discusses the practice of experimentation within agile workflows, focusing on hypothesis-driven approaches and their impact on decision-making and organisational culture. However, it primarily details the 'how' of experimentation rather than exploring the underlying philosophical principles that guide these practices. While there are mentions of fostering a culture of learning and adaptability, the content lacks a deeper exploration of the foundational beliefs that shape these methodologies, which is essential for a stronger alignment with the Philosophy category.",
    "level": "Tertiary"
  },
  "Observability": {
    "category": "Observability",
    "calculated_at": "2025-04-10T13:21:01",
    "ai_confidence": 12.0,
    "ai_mentions": 10.0,
    "ai_alignment": 5.0,
    "ai_depth": 2.0,
    "non_ai_confidence": 0,
    "final_score": 12.0,
    "reasoning": "The content primarily focuses on experimentation within agile workflows, discussing hypothesis-driven approaches and their impact on decision-making and innovation. While it touches on themes of continuous improvement and team collaboration, it does not explicitly address observability or its key components such as metrics, logs, or traces. The discussion lacks depth in relation to observability principles and does not provide insights into tools or practices specifically aimed at enhancing observability.",
    "level": "Ignored"
  },
  "Capability": {
    "category": "Capability",
    "calculated_at": "2025-04-10T13:21:01",
    "ai_confidence": 78.0,
    "ai_mentions": 15,
    "ai_alignment": 32,
    "ai_depth": 30,
    "non_ai_confidence": 0,
    "final_score": 78.0,
    "reasoning": "The content discusses the practice of experimentation in agile workflows, highlighting its role in fostering a culture of learning and adaptability, which aligns well with the concept of capabilities. It explicitly mentions how experimentation enables teams to deliver value predictably and sustainably, which is a core theme of the Capability category. The depth of discussion is substantial, covering the significance of experimentation in driving continuous improvement and systemic change within organisations. However, while it focuses on a critical practice, it does not delve deeply into the broader implications of capabilities as enduring competencies, which slightly lowers the confidence score.",
    "level": "Secondary",
    "reasoning_summary": "This content is a good fit for the Capability category, as it explores how experimentation supports learning and adaptability within agile teams. It clearly connects experimentation to delivering value and continuous improvement, which are central to capabilities. However, it doesn’t fully address capabilities as lasting organisational strengths, so the fit isn’t perfect but still relevant."
  },
  "Model": {
    "category": "Model",
    "calculated_at": "2025-04-10T13:21:01",
    "ai_confidence": 42.0,
    "ai_mentions": 12,
    "ai_alignment": 28,
    "ai_depth": 20,
    "non_ai_confidence": 0,
    "final_score": 42.0,
    "reasoning": "The content discusses experimentation in agile workflows, which relates to the iterative nature of models in Agile and Lean contexts. However, it does not explicitly mention specific models or frameworks like the Cynefin Framework or Lean Startup principles, which are central to the 'Model' category. The focus is more on the practice of experimentation rather than on conceptual models themselves, leading to a moderate confidence score.",
    "level": "Tertiary"
  },
  "Principle": {
    "category": "Principle",
    "calculated_at": "2025-04-10T13:21:01",
    "ai_confidence": 82.0,
    "ai_mentions": 15,
    "ai_alignment": 32,
    "ai_depth": 30,
    "non_ai_confidence": 0,
    "final_score": 82.0,
    "reasoning": "The content explicitly discusses the principle of experimentation within agile workflows, highlighting its role in decision-making and team behaviour. It aligns well with the core themes of empiricism and continuous improvement, as it emphasises making informed decisions based on evidence and fostering a culture of learning. The depth of discussion is substantial, covering how experimentation influences team dynamics and organisational culture, thus reinforcing its importance as a guiding principle in agile practices.",
    "level": "Primary",
    "reasoning_summary": "This content is a strong fit for the category, as it thoroughly explores how experimentation underpins agile practices. By focusing on evidence-based decision-making and fostering a learning culture, it clearly aligns with the core agile values of empiricism and continuous improvement. The discussion’s depth, especially regarding team and organisational impact, further reinforces its relevance to the category."
  },
  "Artifact": {
    "category": "Artifact",
    "calculated_at": "2025-04-10T13:21:01",
    "ai_confidence": 32.0,
    "ai_mentions": 10.0,
    "ai_alignment": 25.0,
    "ai_depth": 50.0,
    "non_ai_confidence": 0,
    "final_score": 32.0,
    "reasoning": "The content discusses experimentation in agile workflows, which is related to the concept of artifacts in terms of empirical evidence and decision-making. However, it does not explicitly mention specific artifacts or their roles, focusing instead on the broader practice of experimentation. The alignment with the category is weak as it does not delve into the structure or purpose of artifacts, and while it touches on transparency and adaptation, it lacks depth in discussing specific artifacts like Product Backlog or Sprint Backlog.",
    "level": "Ignored"
  },
  "Discipline": {
    "category": "Discipline",
    "calculated_at": "2025-04-10T13:21:01",
    "ai_confidence": 62.0,
    "ai_mentions": 15,
    "ai_alignment": 30,
    "ai_depth": 25,
    "non_ai_confidence": 0,
    "final_score": 62.0,
    "reasoning": "The content discusses experimentation in agile workflows, which indirectly relates to discipline through the structured approach to testing and validating ideas. However, while it emphasises systematic exploration and a culture of learning, it does not explicitly focus on discipline as a primary theme. The depth of discussion is substantial, covering the importance of experimentation and its impact on organisational culture, but it lacks a direct connection to the concept of discipline itself.",
    "level": "Secondary"
  },
  "Scrum Values": {
    "resourceId": "Experimentation",
    "category": "Scrum Values",
    "calculated_at": "2025-05-06T11:37:05",
    "ai_confidence": 41.132,
    "ai_mentions": 0.4,
    "ai_alignment": 3.1,
    "ai_depth": 3.6,
    "ai_intent": 2.3,
    "ai_audience": 7.6,
    "ai_signal": 4.6,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 41.0,
    "reasoning": "The content focuses exclusively on the theme of experimentation within agile workflows, especially emphasizing hypothesis-driven development, empirical feedback, and continuous improvement. While there is conceptual overlap with some underlying Scrum Values (such as openness to learning, courage to try new ideas, and commitment to improvement), the content never directly mentions Scrum, nor does it reference the specific five Scrum Values (commitment, courage, focus, openness, respect). \n\nScoring details:\n- Direct Mentions (0.4): The category or the individual Scrum Values are not cited at all. There are no phrases overtly tying the discussion to Scrum values.\n- Conceptual Alignment (3.1): The content is tangentially related to some of the Scrum Values (notably openness, courage, and commitment to learning), but only as a supporting background, and does not explore these values intentionally or explicitly. The main thrust is experimentation, which is a general Agile (and Lean) principle, not a Scrum foundational value.\n- Depth of Discussion (3.6): The discussion is thorough about experimentation, but not about Scrum Values specifically. Depth is awarded for exploring cultural aspects that could relate to values, but none of the Scrum Values are discussed in themselves.\n- Intent/Purpose Fit (2.3): The intent is to explain experimentation as a practice, not to inform or support understanding of Scrum Values or their implementation.\n- Audience Alignment (7.6): The intended audience for this piece is clearly Agile practitioners, which overlaps with Scrum’s audience, thus the relatively higher score. However, the message is broad, targeting anyone in Agile/innovation roles, not specifically Scrum Teams.\n- Signal-to-Noise Ratio (4.6): The content is fairly focused and free of tangents, but all focus is on experimentation, not on Scrum Values, hence a middling score.\n\nNo penalties were applied, as there are no outdated practices or actively contradictory tones present. Overall, this content is classified at the 'Tertiary' level for the 'Scrum Values' category because overlap is minor and implicit, and would not directly educate or train someone on the meaning or application of Scrum Values.",
    "level": "Tertiary"
  },
  "Metrics and Learning": {
    "resourceId": "Experimentation",
    "category": "Metrics and Learning",
    "calculated_at": "2025-05-06T11:36:59",
    "ai_confidence": 84.02,
    "ai_mentions": 4.8,
    "ai_alignment": 9.1,
    "ai_depth": 8.6,
    "ai_intent": 8.7,
    "ai_audience": 8.4,
    "ai_signal": 8.0,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 84.0,
    "reasoning": "The content directly discusses experimentation as a structured, hypothesis-driven approach within agile workflows. While it does not explicitly mention 'metrics' often or use the exact terminology from the category definition, the description aligns closely with core concepts such as empirical decision-making, feedback loops, continuous improvement, and learning cycles.\\n\\n- **Direct Mentions (4.8):** There are no explicit references to 'metrics' or 'measurement', but there are repeated mentions of 'empirical evidence', 'testing', 'feedback loops', and 'learning', which are closely related term-wise; hence a moderately low but still relevant score.\\n\\n- **Conceptual Alignment (9.1):** The main theme is using experimentation (which inherently relies on measurement and validation) to drive continuous improvement in agile contexts. This matches key aspects of 'Metrics and Learning'.\\n\\n- **Depth of Discussion (8.6):** The piece goes well beyond a surface mention by discussing the rationale for experimentation, its effect on culture, learning from failure, and iterative process adjustment. Some specific practical details or tool references are missing, so not absolutely max depth.\\n\\n- **Intent / Purpose Fit (8.7):** The purpose is clearly to advocate for and explain the value of empirical, learning-oriented processes in teams—well within the category's scope.\\n\\n- **Audience Alignment (8.4):** Target audience is agile practitioners, product teams, and those focused on organisational change, directly matching the intended readers for 'Metrics and Learning'.\\n\\n- **Signal-to-Noise Ratio (8.0):** The content is focused, with very little off-topic material or filler. Some minor general statements, but overall high focus.\\n\\n- **Level:** 'Primary' fit, as experimentation and evidence-driven decision-making are described as foundational practices, not ancillary aspects.\\n\\nNo penalties were applied: The content is current, supports the category framing, and does not reference outdated or inappropriate practices.\\n\\nThe confidence score is high, reflecting very strong alignment with the spirit and requirements of the 'Metrics and Learning' category, though direct mention of 'metrics' as a term is not frequent, which slightly lowers that individual dimension.",
    "level": "Primary",
    "reasoning_summary": "This content is a strong fit for the 'Metrics and Learning' category. It emphasises experimentation, empirical evidence, and feedback loops—core elements of learning and improvement in agile teams. While it doesn’t frequently use the word 'metrics', its focus on data-driven decision-making and continuous learning aligns closely with the category’s intent and audience."
  },
  "Lean Principles": {
    "resourceId": "Experimentation",
    "category": "Lean Principles",
    "calculated_at": "2025-05-06T11:37:20",
    "ai_confidence": 67.438,
    "ai_mentions": 1.8,
    "ai_alignment": 7.4,
    "ai_depth": 7.9,
    "ai_intent": 7.3,
    "ai_audience": 7.0,
    "ai_signal": 7.6,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 67.0,
    "reasoning": "The content on 'Experimentation' describes a hypothesis-driven, iterative approach closely linked to continuous improvement—a core Lean principle. The discussion around empirical testing, informed decisions, iterative feedback loops, and promoting a learning culture all align conceptually with Lean's philosophy of kaizen and waste minimisation through ongoing evaluation. However, there are no direct mentions of 'Lean', 'waste reduction', or specific Lean tools (e.g., 5S, Value Stream Mapping). The primary terminology is from the agile and experimentation domain, not Lean jargon. Depth is moderately high as the content moves beyond a superficial overview, exploring both cultural and tactical implications. Intent aligns with Lean's focus on improvement but is broader, aiming also at innovation and adaptability. The audience appears to be practitioners and organisations interested in improving workflows—consistent with Lean's audience, though not explicitly targeting Lean practitioners. Signal-to-noise is strong, with minimal filler or tangents. No penalties are applied as the content is current, positive, and not contradictory or outdated. Overall, this is a Secondary fit: experimentation is foundational to Lean but treated here through a broader agile lens without explicit Lean framing.",
    "level": "Secondary"
  },
  "Market Adaptability": {
    "resourceId": "Experimentation",
    "category": "Market Adaptability",
    "calculated_at": "2025-05-06T11:37:10",
    "ai_confidence": 81.8,
    "ai_mentions": 6.8,
    "ai_alignment": 8.7,
    "ai_depth": 8.5,
    "ai_intent": 8.2,
    "ai_audience": 8.0,
    "ai_signal": 7.8,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 82.0,
    "reasoning": "The content explicitly discusses experimentation within the context of agile workflows, strongly aligning with the core principles of market adaptability as defined. It emphasizes hypothesis-driven approaches, continuous feedback loops, and decision-making based on empirical evidence—all key facets of adaptive, Agile organizations. \n\nMentions (6.8): The text never directly uses the term 'market adaptability' or the names of specific frameworks (like DevOps or Lean), but it repeatedly refers to related agile principles and adaptation, which partially covers direct mentions.\n\nAlignment (8.7): The core ideas—responding to market changes via experimentation, iterative learning, and embracing failure for improvement—align closely with the definition and examples in the classification. It meaningfully addresses adaptation and resilience.\n\nDepth (8.5): The discussion goes beneath the surface by exploring both the mindset (e.g., learning from failure, systemic change) and mechanisms (hypothesis-driven testing, continuous improvement) of experimentation. However, it doesn't include concrete case studies or multiple methodologies, limiting the absolute depth.\n\nIntent (8.2): The content’s purpose is to inform and promote experimentation as a foundational practice to boost adaptability, directly supporting the category’s goals.\n\nAudience (8.0): The writing is aimed at practitioners and leaders interested in agile and organizational improvement, matching the likely audience for market adaptability content—though not specifically addressing executives or cross-functional strategic audiences.\n\nSignal (7.8): Almost all content is relevant; there is minimal filler or tangential information. No explicit off-topic material, yet focus could increase by providing more actionable frameworks or tools.\n\nNo penalties applied: The content is current, constructive, and in full alignment with core category framing (no contradiction or outdatedness). \n\nOverall, the content is a high-confidence Primary fit for the Market Adaptability category, driven by strong conceptual alignment and depth regarding responsive, evidence-based approaches in agile contexts.",
    "level": "Primary",
    "reasoning_summary": "This content is a strong fit for the Market Adaptability category. It thoroughly explores agile experimentation, focusing on hypothesis-driven learning and continuous improvement—key aspects of adapting to market changes. While it doesn’t use specific terms or frameworks, its emphasis on evidence-based decision-making and resilience aligns well with the category’s intent and audience."
  },
  "Evidence Based Management": {
    "resourceId": "Experimentation",
    "category": "Evidence Based Management",
    "calculated_at": "2025-05-06T11:36:54",
    "ai_confidence": 79.7,
    "ai_mentions": 2.7,
    "ai_alignment": 8.9,
    "ai_depth": 6.6,
    "ai_intent": 8.1,
    "ai_audience": 8.8,
    "ai_signal": 8.4,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 80.0,
    "reasoning": "The content directly addresses the use of hypothesis-driven, empirical approaches to decision-making—an important underpinning of Evidence Based Management (EBM). However, explicit direct mentions of 'Evidence Based Management' or its formal terminology are minimal (score 2.7), as the article discusses experimentation more generally. Conceptual alignment is strong (8.9): the content maps directly to EBM themes such as empirical decision-making, value delivery, innovation, and outcome focus. Depth of discussion is moderate (6.6), going beyond a superficial explanation to address culture, iterative learning, and value—but it does not reference EBM’s formal key metrics (e.g., Current Value, Time to Market). Intent is clearly supportive of EBM-like principles (8.1): it promotes empirical, data-driven management as beneficial and transformative. Audience alignment (8.8) is high, as the language and context are suited to organisational leaders, agile practitioners, and strategists—the EBM target demographic. Signal-to-noise ratio is high (8.4); the content is focused with little filler or digression. No penalties are applied, as the content is current, supportive, and well-aligned. Level is 'Secondary' because, while highly relevant to EBM, the discussion frames experimentation as a general agile best practice, with less focus on the unique structures and key metrics that define EBM specifically.",
    "level": "Secondary",
    "reasoning_summary": "This content is a strong fit for the Evidence Based Management (EBM) category, as it champions empirical, hypothesis-driven decision-making and aligns well with EBM’s core principles. However, it doesn’t explicitly reference EBM or its formal metrics, instead discussing experimentation in a broader agile context. The depth and audience targeting are appropriate, but the focus remains more general than EBM’s unique frameworks, making it a secondary rather than primary example."
  },
  "Self Organisation": {
    "resourceId": "Experimentation",
    "category": "Self Organisation",
    "calculated_at": "2025-05-06T11:36:58",
    "ai_confidence": 63.975,
    "ai_mentions": 3.25,
    "ai_alignment": 7.45,
    "ai_depth": 7.7,
    "ai_intent": 7.25,
    "ai_audience": 7.6,
    "ai_signal": 8.1,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 64.0,
    "reasoning": "The content centers on experimentation within agile workflows, emphasizing hypothesis-driven approaches—an important Agile practice. While self-organisation as a concept is not explicitly named or directly discussed, the text describes practices (empowered teams, autonomy in execution, continuous improvement) that align with self-organisation principles. For example, phrases like 'empowers teams to systematically explore uncertainties' and 'fostering a culture of experimentation' indicate environments where self-organising teams could thrive. However, core self-organisation themes (autonomy, ownership, minimal oversight, leadership roles, explicit team agreements) are not explored in detail. There is no direct mention of self-organisation or synonyms, limiting the 'Direct Mentions' score. \n\nOn depth and alignment, the discussion explores the impact of experimentation on adaptability, collaboration, and continuous learning—all thematically adjacent to self-organisation—but without focused, in-depth treatment of self-organisation itself, resulting in solid but not maximal scores. Intent and audience are well-matched for practitioners and contemporary Agile organisations, and the content avoids any outdated or critical tones. The signal-to-noise ratio is quite high: the text is focused and avoids tangents, supporting a strong signal score. No penalty points were warranted. \n\nOverall, this content functions as a strong secondary resource for the Self Organisation category: it illustrates enabling conditions and adjacencies but would require additional, explicit section(s) about self-organisation to be classified as 'Primary.'",
    "level": "Secondary"
  },
  "Remote Working": {
    "resourceId": "Experimentation",
    "category": "Remote Working",
    "calculated_at": "2025-05-06T11:37:18",
    "ai_confidence": 19.92,
    "ai_mentions": 0.8,
    "ai_alignment": 2.5,
    "ai_depth": 2.8,
    "ai_intent": 2.2,
    "ai_audience": 5.0,
    "ai_signal": 3.8,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 20.0,
    "reasoning": "The content focuses on hypothesis-driven experimentation in agile workflows, discussing its value in decision-making, fostering a learning culture, and supporting continuous improvement. However, there is no explicit mention of remote working or distributed teams, nor are there references to remote Agile practices, tools, or challenges.\\n\\n- Mentions (0.80): The content never references 'Remote Working' or associated terms directly; this score reflects an absence of explicit mentions.\\n- Alignment (2.50): While the theme is closely related to Agile methodology, there is no conceptual connection to remote work or the unique circumstances of distributed teams.\\n- Depth (2.80): The discussion is moderately deep regarding experimentation, but it is entirely generic to Agile, not remote contexts.\\n- Intent (2.20): The purpose is misaligned; it aims to inform about experimentation in general, not its application in remote working.\\n- Audience (5.00): The audience is Agile practitioners, which could overlap with remote teams, but there's no specific targeting or advice relevant to remote working.\\n- Signal (3.80): The focus remains on experimentation and Agile, with no content that could be deemed directly relevant to Remote Working practices or challenges.\\n\\nNo penalties for outdated/contradictory content were needed. Overall, this resource only tangentially relates to remote working, and then only by inference (i.e., if a distributed team employed experimentation); thus, it fits as a Tertiary connection to the 'Remote Working' category.",
    "level": "Ignored"
  },
  "GitHub": {
    "resourceId": "Experimentation",
    "category": "GitHub",
    "calculated_at": "2025-05-06T11:37:08",
    "ai_confidence": 13.49,
    "ai_mentions": 0.3,
    "ai_alignment": 1.45,
    "ai_depth": 1.1,
    "ai_intent": 2.45,
    "ai_audience": 4.1,
    "ai_signal": 2.91,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 13.0,
    "reasoning": "The content discusses 'Experimentation' in the context of agile workflows and hypothesis-driven development. However, there is no mention of GitHub, nor any reference to its tools, practices, or features. \n\nDirect Mentions (0.30): GitHub is not mentioned at all; this is a minimal courtesy score since agile and DevOps are tangentially related to GitHub usage, but the content fails the explicit naming requirement.\n\nConceptual Alignment (1.45): The thematic focus is on agile experimentation, which overlaps generally with GitHub-adjacent practices, but nothing about GitHub itself, its functionalities, or methodologies.\n\nDepth of Discussion (1.10): While the discussion goes deep into why experimentation is important in agile, it provides no details or depth specific to the GitHub platform or tools.\n\nIntent/Purpose Fit (2.45): The intent is informative and suitable for those interested in agile and experimentation practices. Still, the purpose is not aligned to GitHub at all, making the fit only slightly applicable due to marginal potential overlap.\n\nAudience Alignment (4.10): The target audience is similar (tech, agile practitioners) who might also use GitHub, but the material is not targeted at GitHub users or implementers specifically.\n\nSignal-to-Noise Ratio (2.91): The content is focused, but none of it pertains to GitHub. Thus, the relevance to this category is extremely low.\n\nNo penalties have been applied because the material is current and does not contradict GitHub. However, the content lies at the periphery ('Tertiary') relative to the GitHub category; it addresses general agile experimentation without connecting to any GitHub-specific tools, features, or use-cases.",
    "level": "Ignored"
  },
  "Social Technologies": {
    "resourceId": "Experimentation",
    "category": "Social Technologies",
    "calculated_at": "2025-05-06T11:36:55",
    "ai_confidence": 84.987,
    "ai_mentions": 5.7,
    "ai_alignment": 9.2,
    "ai_depth": 8.9,
    "ai_intent": 8.6,
    "ai_audience": 7.8,
    "ai_signal": 8.1,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 85.0,
    "reasoning": "The content directly discusses experimentation as a hypothesis-driven, collaborative practice central to agile workflows. While it does not frequently or explicitly mention the term 'Social Technologies,' it thoroughly explores core concepts such as fostering collaboration, enabling informed decision-making, adopting iterative processes, and embedding continuous improvement in organisational culture. The alignment is very strong; it clearly highlights experimentation as a means of improving value delivery, adaptability, and learning—key pillars of Social Technologies. The depth is substantial, discussing the mindset, iterative feedback, cultural implications, and long-term impact of experimentation. The intent is informative, supportive of the organisational and team-based aims typical of Social Technologies, with a clear focus on teams, cross-functional collaboration, and cultural transformation. The content appears aimed at practitioners and strategists within agile organisations—aligned but not exclusively targeted at the full audience for the category, hence a small deduction. The signal-to-noise ratio is high: the content stays thematically focused and relevant, without tangents or filler. No penalties for outdated content or negative tone are warranted. The confidence score reflects this strong conceptual and practical fit, even though direct category terms are not prominent—placing this content primarily within the Social Technologies category.",
    "level": "Primary",
    "reasoning_summary": "This content is a strong fit for the Social Technologies category. It explores how experimentation fosters collaboration, informed decision-making, and continuous improvement—core aspects of Social Technologies. While it doesn’t use the term directly, its focus on agile teamwork and cultural transformation aligns well, making it highly relevant for practitioners interested in these organisational practices."
  },
  "Test Automation": {
    "resourceId": "Experimentation",
    "category": "Test Automation",
    "calculated_at": "2025-05-06T11:37:19",
    "ai_confidence": 25.55,
    "ai_mentions": 0.6,
    "ai_alignment": 2.7,
    "ai_depth": 2.2,
    "ai_intent": 3.1,
    "ai_audience": 5.5,
    "ai_signal": 3.3,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 26.0,
    "reasoning": "The content primarily explores hypothesis-driven experimentation within agile workflows, emphasizing iterative testing, feedback loops, and an empirical mindset. However, there are no direct or explicit mentions of ‘Test Automation’, automation tooling, or frameworks—its language focuses on organizational processes and cultural approaches rather than the automation of software testing. \n\nMentions (0.6): No direct reference to 'Test Automation' or related terms. \nAlignment (2.7): The general idea of testing and feedback aligns abstractly with some test automation principles but not with its specific practices or tools. \nDepth (2.2): The discussion is focused on high-level agile experimentation and learning, lacking substantive coverage on automation specifics. \nIntent (3.1): The content’s primary intent is to promote experimentation culture in agile, not the automation of testing processes; alignment is weak but not wholly tangential. \nAudience (5.5): The audience is likely cross-functional agile practitioners—overlapping somewhat with test automation audiences, though less technical and more process-oriented. \nSignal (3.3): Most content is on-topic (experimentation in agile), but only a peripheral overlap with test automation.\n\nOverall, while there’s thematic overlap via concepts like feedback loops and testing, the lack of specific automation focus relegates relevance to a tertiary level.",
    "level": "Ignored"
  },
  "Cell Structure Design": {
    "resourceId": "Experimentation",
    "category": "Cell Structure Design",
    "calculated_at": "2025-05-06T11:37:22",
    "ai_confidence": 41.45,
    "ai_mentions": 0.3,
    "ai_alignment": 4.3,
    "ai_depth": 3.7,
    "ai_intent": 4.6,
    "ai_audience": 6.1,
    "ai_signal": 7.15,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 41.0,
    "reasoning": "The content discusses experimentation as a practice in agile workflows, highlighting hypothesis-driven testing, continuous improvement, and empirical validation. There are no direct mentions of Cell Structure Design, the Beta Codex, or Niels Pfläging. The core concepts such as decentralised networks, autonomous cells, or complexity theory do not explicitly appear, resulting in a low Direct Mentions score (0.30). Conceptual Alignment (4.30) is moderate because the themes of experimentation, adaptability, and responsiveness do align with some principles of Cell Structure Design, but they are portrayed in generic agile/innovation contexts rather than specifically within the decentralised, cell-based, Beta Codex-informed model. Depth (3.70) reflects that while the discussion is thorough on experimentation, it does not extend meaningfully into organisational design at the level required by the category. Intent (4.60) is slightly higher, as the focus on improvement, adaptability, and systemic change is tangentially relevant, but not specific to the intent of promoting Cell Structure Design. Audience Alignment (6.10) is fairly high since the content appeals to professionals interested in organisational learning and innovation—an audience that overlaps but is not identical with Cell Structure Design's core. Signal (7.15) reflects that the content is focused without much noise, but the relevance to Cell Structure Design specifically is low. No penalties are applied, as the content is neither outdated, contradictory, nor undermining. Overall, this is a tertiary-level fit, since the overlap is generic and not rooted in the category's specific framework, lowering the confidence score accordingly.",
    "level": "Tertiary"
  },
  "Customer Satisfaction": {
    "resourceId": "Experimentation",
    "category": "Customer Satisfaction",
    "calculated_at": "2025-05-06T11:37:11",
    "ai_confidence": 69.41,
    "ai_mentions": 4.7,
    "ai_alignment": 7.8,
    "ai_depth": 7.6,
    "ai_intent": 7.2,
    "ai_audience": 8.3,
    "ai_signal": 9.1,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 69.0,
    "reasoning": "The content focuses on experimentation as a practice within agile workflows, emphasizing hypothesis-driven improvements, iterative testing, feedback loops, and alignment with customer needs. While 'customer satisfaction' is not directly mentioned, the concept is implicitly addressed through references to aligning with customer needs and improving products based on feedback. The alignment score is fairly high, as there’s a clear conceptual connection—experimentation is a means to validate assumptions and meet customer needs—but the depth remains oriented toward experimentation principles in general, not an in-depth exploration of customer satisfaction specifically. The intent and audience scores are high, as agile and DevOps practitioners benefit from these practices, and the content is relevant to teams focused on continuous improvement. However, the direct mentions score reflects the lack of explicit reference to 'customer satisfaction.' There are no penalties applied, as the information is current, neutral, and not undermining the category. The signal-to-noise ratio is high, indicating that the content is focused and not diluted by unrelated topics. Overall, this resource should be classified as 'Secondary'—experimentation is highly relevant to customer satisfaction, but the topic does not center on it exclusively.",
    "level": "Secondary"
  },
  "Change Management": {
    "resourceId": "Experimentation",
    "category": "Change Management",
    "calculated_at": "2025-05-06T11:37:05",
    "ai_confidence": 71.254,
    "ai_mentions": 2.9,
    "ai_alignment": 8.25,
    "ai_depth": 7.85,
    "ai_intent": 7.2,
    "ai_audience": 7.0,
    "ai_signal": 7.354,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 71.0,
    "reasoning": "The content thoroughly explores hypothesis-driven experimentation within Agile workflows, emphasizing learning, adaptability, and continuous improvement—these are all concepts tightly related to Change Management as defined. The description highlights behavioral and cultural shifts (e.g., fostering a culture of experimentation, iteratively responding to change, embedding scientific approaches) that enable sustainable change, aligning well with Change Management’s principles and intent. However, the discussion does not directly mention 'Change Management' or its formal terminology, and it focuses primarily on experimentation itself as a catalyst for change rather than on structured frameworks or practices for managing change. This warrants a lower score for 'Direct Mentions' (2.900) but high 'Alignment' (8.250) and 'Depth' (7.850), as the link to change management is strong though implicit. 'Intent' scored at 7.200, reflecting that the main purpose is to stress experimentation’s value—not change management per se, but there's a solid indirect fit. 'Audience' (7.000) is appropriate: the content aims at Agile practitioners, teams, and to a lesser extent organisational leaders involved in transformation. 'Signal-to-noise' (7.354) is high: the discussion is focused without filler, but small points could be more explicit about the audience or direct change outcomes. No penalty is warranted, since content is contemporary and supportive. Overall, this content is best classified as 'Secondary' for Change Management, since it addresses a driver for meaningful change but does not primarily focus on the formal discipline or its full set of practices.",
    "level": "Secondary",
    "reasoning_summary": "This content fits the 'Secondary' category for Change Management because it centres on experimentation as a key driver of change, aligning with core principles like adaptability and continuous improvement. While it doesn’t directly reference formal change management frameworks, its focus on fostering a culture of learning and behavioural shifts makes it highly relevant, though not the main subject. The audience and intent are also well matched."
  },
  "Continuous Learning": {
    "resourceId": "Experimentation",
    "category": "Continuous Learning",
    "calculated_at": "2025-05-06T11:36:57",
    "ai_confidence": 91.68,
    "ai_mentions": 6.4,
    "ai_alignment": 9.35,
    "ai_depth": 9.2,
    "ai_intent": 9.1,
    "ai_audience": 8.9,
    "ai_signal": 9.0,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 92.0,
    "reasoning": "This content provides a strong treatment of experimentation within agile workflows, explicitly delving into hypothesis-driven learning, fostering a learning culture, and the importance of feedback loops—all key markers for the Continuous Learning category. 'Experimentation' is not directly labeled as 'continuous learning,' but the text repeatedly references the practices (iterative testing, feedback loops, learning from failure, ongoing improvement) that constitute core concepts in the definition. There are numerous indirect mentions of growth mindset ('failure is viewed as a learning opportunity'), and substantial discussion of creating a culture of adaptability and innovation. The depth is strong—the piece goes beyond lip service by describing the systemic value of experimentation, not just at a tactical level but embedded within organizational culture and team operations. The intent is clearly aligned: the purpose is to explore how experimentation drives learning and adaptation, directly fitting the intended use of the category. The target audience is practitioners in agile, DevOps, or Lean settings (teams, organizations), which fits well, though it's written at a slightly higher-level and could be absorbed by strategic or cross-functional teams as well—hence a slightly lower score there. All content is relevant, focused, and on-topic; there is very little, if any, filler or tangential information. There are no penalties as nothing is outdated, off-tone, or in contradiction to the category. Overall, this content is a primary match for the 'Continuous Learning' category, with only minor deductions due to lack of direct naming and audience breadth.",
    "level": "Primary",
    "reasoning_summary": "This content is an excellent fit for the Continuous Learning category. It thoroughly explores experimentation in agile environments, highlighting key practices like hypothesis-driven learning, feedback loops, and learning from failure. While it doesn’t use the exact term, its focus on fostering a learning culture and ongoing improvement clearly aligns with the category’s core principles, making it highly relevant for practitioners and teams."
  },
  "Product Development": {
    "resourceId": "Experimentation",
    "category": "Product Development",
    "calculated_at": "2025-05-06T11:37:01",
    "ai_confidence": 90.443,
    "ai_mentions": 6.7,
    "ai_alignment": 9.5,
    "ai_depth": 9.1,
    "ai_intent": 9.4,
    "ai_audience": 8.8,
    "ai_signal": 9.0,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 90.0,
    "reasoning": "The content discusses experimentation as a foundational practice in agile workflows, directly referencing hypothesis-driven approaches that facilitate iterative learning and customer feedback—both core to Product Development. \n\nDirect Mentions (6.7): The phrase 'product development' appears once near the end, and although explicit mention is limited, much of the terminology ('agile workflows', 'continuous improvement', 'feedback loops') strongly implies product development. \n\nConceptual Alignment (9.5): The themes of evidence-based decision-making, feedback loops, and alignment with customer needs directly map to the category definition. The content strongly supports the continuous improvement and iterative learning central to product development. \n\nDepth of Discussion (9.1): The discussion goes beyond surface-level, covering cultural, process, and organizational dimensions, and highlighting how experimentation impacts iterative delivery, risk mitigation, and team dynamics. \n\nIntent/Purpose Fit (9.4): The content's purpose is to underscore the importance of experimentation within agile, customer-centric environments and to frame it as foundational to effective product development.\n\nAudience Alignment (8.8): The language and focus are suitable for practitioners and strategists in product development, particularly those familiar with agile and cross-functional teams. It could, however, have slightly broader appeal outside the direct product development audience (e.g., innovators in other domains).\n\nSignal/Noise (9.0): The content is highly focused, with very little off-topic or filler material—almost all elements reinforce the product development context via experimentation. \n\nThere are no penalties, as the content is current, respectful, and adheres strictly to the positive framing of experimentation in product development.\n\nOverall, this resource fits squarely within the core intent, substance, and audience of the Product Development category, warranting a 'Primary' level of classification.",
    "level": "Primary",
    "reasoning_summary": "This content is an excellent fit for the Product Development category. It thoroughly explores how experimentation underpins agile workflows, focusing on iterative learning, customer feedback, and evidence-based decisions—all central to product development. The discussion is in-depth, relevant, and tailored to practitioners, making it highly suitable as a primary resource for this category."
  },
  "Flow Efficiency": {
    "resourceId": "Experimentation",
    "category": "Flow Efficiency",
    "calculated_at": "2025-05-06T11:37:09",
    "ai_confidence": 46.7,
    "ai_mentions": 0.3,
    "ai_alignment": 5.2,
    "ai_depth": 5.9,
    "ai_intent": 6.1,
    "ai_audience": 9.0,
    "ai_signal": 8.3,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 47.0,
    "reasoning": "The content focuses on experimentation in agile workflows, specifically using hypothesis-driven approaches to validate assumptions. It strongly emphasizes empirical decision-making, learning from failure, continuous improvement, and team adaptability—all of which are broadly compatible with Lean and Agile mindsets. However, there are no explicit mentions of 'flow efficiency', bottleneck reduction, or throughput optimization. Key flow concepts (cycle/lead time, WIP limits, Kanban boards) are absent. \n\nFor 'Direct Mentions' (0.3), the resource never references 'flow efficiency' or synonyms. \n\nFor 'Conceptual Alignment' (5.2), experimentation is a supporting principle of continuous improvement and learning (which are prerequisites for flow efficiency), but the content's main ideas stop short of directly discussing value stream flow, throughput, or bottleneck elimination.\n\nThe 'Depth of Discussion' (5.9) is moderate: the article thoroughly explores experimentation, but stays high-level with respect to process implications. It does not thoroughly tie experimentation to concrete flow efficiency outcomes or practices.\n\nFor 'Intent/Purpose Fit' (6.1), while the tone and theme support organizational improvement (which may benefit flow efficiency), the primary intent is not about optimizing flow or throughput directly.\n\n'Audience Alignment' (9.0) is high, as the content targets Agile practitioners—likely the same audience interested in flow efficiency topics.\n\nThe 'Signal-to-Noise' (8.3) score is strong, as the article is focused and relevant, with almost no off-topic or distracting material, albeit not centered on flow efficiency.\n\nNo penalties are warranted, as the material is current, supportive, and not contradictory. \n\nOverall, the content's fit under 'Flow Efficiency' is tertiary: it is relevant as an enabling mindset but is not directly focused on flow optimization or measurement.",
    "level": "Tertiary"
  },
  "Agile Philosophy": {
    "resourceId": "Experimentation",
    "category": "Agile Philosophy",
    "calculated_at": "2025-05-06T11:37:06",
    "ai_confidence": 85.86,
    "ai_mentions": 6.1,
    "ai_alignment": 8.6,
    "ai_depth": 8.8,
    "ai_intent": 8.5,
    "ai_audience": 8.1,
    "ai_signal": 8.3,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 86.0,
    "reasoning": "The content thoroughly discusses experimentation as a central element in agile workflows, focusing on hypotheses, feedback loops, learning from failure, and adaptability. It explicitly connects experimentation to underlying agile values such as continuous improvement, value delivery, adaptability, and organisational culture change, aligning well with the Agile Philosophy definition. \n\nDirect Mentions (6.1): 'Agile' and its workflows are directly mentioned, but the term 'Agile Philosophy' or its Manifesto are not named explicitly; the discussion remains at a broader agile level rather than naming the philosophy verbatim. Conceptual Alignment (8.6): Core agile themes—customer-focused adaptation, learning from feedback, cultural mindset change—are tightly aligned with the philosophy. Depth (8.8): Content explores both the practical and cultural aspects of experimentation, extending to organisational change and continuous learning, going deeper than mere surface mentions. Intent (8.5): Its main purpose is clearly to inform and promote experimentation as a mindset and foundation within agile—not just a tool—matching the category's focus. Audience (8.1): Aimed at organisational stakeholders, agile leaders, and teams—matching the philosophy's likely audience, though it is somewhat general and not only for strategists. Signal (8.3): Nearly all content addresses agile mindset, experimentation, and their place in cultural transformation, with very little tangential or filler content. \n\nNo penalties: The content is current, genuine, and positive in tone. The confidence is high, but not perfect, as direct reference to 'Agile Philosophy' or the Agile Manifesto itself is not present, and there could be a bit more explicit comparison with the broader philosophy. However, its focus on values, continuous improvement, and adaptation firmly places it as a Primary-level fit for Agile Philosophy.",
    "level": "Primary",
    "reasoning_summary": "This content is a strong fit for the Agile Philosophy category. It explores experimentation as a core agile value, linking it to continuous improvement, adaptability, and cultural change. While it doesn’t name the Agile Philosophy or Manifesto directly, its focus on agile mindsets, learning, and organisational transformation clearly aligns with the philosophy’s principles and intended audience."
  },
  "Collaboration Tools": {
    "resourceId": "Experimentation",
    "category": "Collaboration Tools",
    "calculated_at": "2025-05-06T11:37:11",
    "ai_confidence": 34.539,
    "ai_mentions": 1.4,
    "ai_alignment": 3.3,
    "ai_depth": 3.9,
    "ai_intent": 4.1,
    "ai_audience": 6.2,
    "ai_signal": 6.4,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 35.0,
    "reasoning": "The content provides a comprehensive overview of experimentation within Agile workflows, emphasizing hypothesis-driven approaches, learning from failure, and driving innovation. However, there are minimal—virtually no—direct mentions of collaboration tools, platforms, nor any references to software specifically designed to facilitate Agile collaboration. The main concepts align only weakly with 'Collaboration Tools': while experimentation may foster team interaction or collaboration, the content does not discuss tools or technological enablers for such collaboration, nor does it address best practices, features, or comparative aspects of collaboration tools in Agile. The depth of discussion focuses on the culture and principles behind experimentation, not on tools that enhance communication or coordination. Intent and purpose are related to Agile ways of working, but only tangentially touch on collaboration as a byproduct of experimentation—not as the content's focus. The audience is broadly Agile practitioners, coinciding somewhat with the audience for collaboration tools. Signal-to-noise ratio remains respectable, as the content is focused without tangential filler, but it's simply off-focus for the intended category. No penalty was needed as the tone is positive, relevant, and current. Overall, this resource is only a tertiary fit for the 'Collaboration Tools' category.",
    "level": "Ignored"
  },
  "Test Driven Development": {
    "resourceId": "Experimentation",
    "category": "Test Driven Development",
    "calculated_at": "2025-05-06T11:36:57",
    "ai_confidence": 21.512,
    "ai_mentions": 0.3,
    "ai_alignment": 1.2,
    "ai_depth": 1.0,
    "ai_intent": 2.0,
    "ai_audience": 9.1,
    "ai_signal": 8.7,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 22.0,
    "reasoning": "The content discusses hypothesis-driven experimentation within agile workflows but does not directly mention Test Driven Development (TDD) or its methodology. Explicit mentions (0.3) are nearly absent; there are no references to TDD, the Red-Green-Refactor cycle, or typical TDD terminology. Conceptual alignment (1.2) is low as the content focuses on validating ideas at the process or business level rather than testing code via automated unit tests before implementation. Depth (1.0) is minimal regarding TDD; while the discussion of testing and feedback loops is present, it doesn't explore TDD’s principles, technical practices, or tools. The intent (2.0) is more about general agile experimentation and not specifically aligned with the TDD category's purpose. The audience alignment (9.1) is higher, as the technical audience interested in agile practices may overlap with those interested in TDD. The signal-to-noise ratio (8.7) is also relatively high, as the content is focused on experimentation in agile—though none of this directly pertains to TDD specifics. No penalties were necessary as the content is not outdated or contrary, but it is clearly only peripherally related, qualifying the categorization as tertiary at best.",
    "level": "Ignored"
  },
  "Product Backlog": {
    "resourceId": "Experimentation",
    "category": "Product Backlog",
    "calculated_at": "2025-05-06T11:36:57",
    "ai_confidence": 29.15,
    "ai_mentions": 0.8,
    "ai_alignment": 2.7,
    "ai_depth": 3.2,
    "ai_intent": 2.3,
    "ai_audience": 7.5,
    "ai_signal": 6.4,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 29.0,
    "reasoning": "The content focuses on the general principle of experimentation in agile workflows, specifically the value of hypothesis-driven approaches for validation and learning. However, there is no direct or explicit mention of the Product Backlog, its role, or its mechanics. \n\n1. Direct Mentions (0.8): The Product Backlog is not mentioned at all, nor are key terms uniquely associated with backlog management (e.g., user stories, backlog refinement).\n2. Conceptual Alignment (2.7): While experimentation is part of agile culture, it's not a core concept or theme of Product Backlog management. There is only a tangential alignment in that experimentation can influence items prioritized in the backlog, but this linkage is not made in the text.\n3. Depth (3.2): The discussion of experimentation is relatively thorough for its own subject but does not connect or elaborate on how experimentation influences or is managed within the Product Backlog.\n4. Intent/Purpose Fit (2.3): The intent is to promote experimentation as a mindset, not to discuss backlog management or practices.\n5. Audience Alignment (7.5): The content targets an agile team or product development audience, which overlaps with those interested in the Product Backlog.\n6. Signal-to-Noise (6.4): The text is focused on experimentation, which is partially relevant for agile practitioners, but provides almost no information about the Product Backlog itself.\n\nNo penalties are justified: The tone, accuracy, and frame are all professional and do not reference outdated practices or directly contradict the Product Backlog concept. \n\nOverall, the linkage to the Product Backlog is weak and tertiary at best, with most discussion unrelated to backlog management.",
    "level": "Ignored"
  },
  "Release Management": {
    "resourceId": "Experimentation",
    "category": "Release Management",
    "calculated_at": "2025-05-06T11:37:01",
    "ai_confidence": 28.31,
    "ai_mentions": 0.25,
    "ai_alignment": 2.3,
    "ai_depth": 2.75,
    "ai_intent": 3.6,
    "ai_audience": 6.05,
    "ai_signal": 6.34,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 28.0,
    "reasoning": "The content primarily discusses experimentation and hypothesis-driven development within agile workflows, focusing on validating ideas, learning from failure, and fostering a culture of empirical decision-making. There are no direct mentions of Release Management, its core practices, or terminology (mentions: 0.25). The conceptual alignment is weak (2.30), as experimentation as described is mostly about general agile/product innovation rather than planning, scheduling, or controlling software releases. The discussion goes slightly deeper than surface-level about experimentation and its place in organizational culture (depth: 2.75), but does not address the specifics of release management such as version control, CI/CD, or risk management in releases. The intent is not centered on Release Management but on broader agile teaming and product learning (intent: 3.60). The audience is generally technical/practitioner (audience: 6.05), and the content stays focused on experimentation without straying far into unrelated tangents (signal: 6.34). However, the overall fit for the category is very weak, so this resource would only be relevant as a tertiary reference—perhaps as background context for cultural aspects that could affect release processes, but not as a Release Management resource itself. No penalty deductions were necessary as the content is current, relevant, and not contradictory.",
    "level": "Ignored"
  },
  "Engineering Practices": {
    "resourceId": "Experimentation",
    "category": "Engineering Practices",
    "calculated_at": "2025-05-06T11:37:04",
    "ai_confidence": 62.14,
    "ai_mentions": 2.1,
    "ai_alignment": 7.6,
    "ai_depth": 6.7,
    "ai_intent": 7.4,
    "ai_audience": 9.1,
    "ai_signal": 8.7,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 62.0,
    "reasoning": "The content focuses on 'Experimentation' with an emphasis on hypothesis-driven approaches in agile workflows. While it clearly aligns with agile principles such as empirical process, continuous improvement, and focusing on value delivery through iterative testing, it does not directly discuss the specific engineering practices as defined in the classification (e.g., clean code, TDD, CI/CD, automation). \n\nMentions (2.1): The core term 'engineering practices' is not directly mentioned, nor are related key terms such as TDD, CI/CD, or clean code. The content refers to 'practices in agile workflows,' but this is broad and indirect, warranting a low score. \n\nAlignment (7.6): There is strong conceptual alignment with the spirit of agile engineering—emphasizing empirical validation, iteration, and team learning. However, since it focuses on broader experimentation rather than concrete engineering techniques, the score is lower than for a direct match. \n\nDepth (6.7): The content discusses the philosophy and benefits of experimentation in agile, including cultural, adaptive, and process aspects. However, it lacks concrete examples, actionable practices, or technical depth relevant to engineering (e.g., specific testing or coding techniques), keeping depth in the mid-high range but not at the top. \n\nIntent (7.4): The piece’s intent is closely aligned with agile teams aiming for continuous improvement and empiricism, but it is not explicitly about engineering-centric practices. The primary purpose is more about mindset and organizational culture than explicit technical practice. \n\nAudience (9.1): The context, language, and themes are suitable for an agile practitioner, including engineers, scrum masters, and team leads, with a strong practitioner focus. \n\nSignal (8.7): The entire piece is coherent, focused, and relevant to agile processes and improvement, with almost no tangential or noisy content. \n\nNo penalties are applied, as the content is current and not contradictory. \n\nOverall, this content represents a 'Secondary' fit: it supports the values that underlie engineering practices (experimentation, learning, adaptation), but does not explicitly address the concrete methodologies (TDD, CI/CD, automation), so it does not reach 'Primary' level confidence.",
    "level": "Secondary"
  },
  "Technical Debt": {
    "resourceId": "Experimentation",
    "category": "Technical Debt",
    "calculated_at": "2025-05-06T11:36:58",
    "ai_confidence": 16.053,
    "ai_mentions": 0.8,
    "ai_alignment": 2.4,
    "ai_depth": 2.1,
    "ai_intent": 1.5,
    "ai_audience": 4.6,
    "ai_signal": 2.8,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 16.0,
    "reasoning": "The content primarily addresses experimentation, hypothesis-driven approaches, and empirical validation within agile workflows. There are no explicit or even implied references to technical debt, its management, or its strategic remediation. \n\nMentions (0.800): The term 'technical debt' does not appear at all, and none of its synonyms or related concepts (suboptimal code, code quality, legacy remediation) are referenced.\n\nConceptual Alignment (2.400): While continuous improvement and long-term sustainability are briefly touched upon, these are broad Agile principles and not uniquely linked to technical debt. The suggested focus is on innovation, adaptation, and learning, not the trade-offs or consequences related to technical debt.\n\nDepth (2.100): The discussion remains general about the value of experimentation, offering no depth specific to technical debt themes. There are no mentions of trade-offs, remediation techniques, or technical debt metrics.\n\nIntent (1.500): The content’s intent is to promote experimentation for innovation and value delivery, not to inform, support, or prioritize technical debt management.\n\nAudience (4.600): The target audience is likely Agile practitioners, team leads, and perhaps organizational change agents, which moderately overlaps with those interested in technical debt, but only incidentally (i.e., not as a primary concern).\n\nSignal (2.800): Most of the content is relevant to experimentation and Agile, with no technical debt focus at all, so the signal-to-noise ratio is low for the technical debt category.\n\nNo penalties are applied as the content is neither outdated nor actively contradicts technical debt as a concept. Overall, this resource is only peripherally connected to technical debt, justifying a low confidence and tertiary categorization.",
    "level": "Ignored"
  },
  "Time to Market": {
    "resourceId": "Experimentation",
    "category": "Time to Market",
    "calculated_at": "2025-05-06T11:37:02",
    "ai_confidence": 39.65,
    "ai_mentions": 1.7,
    "ai_alignment": 4.8,
    "ai_depth": 4.6,
    "ai_intent": 5.2,
    "ai_audience": 8.0,
    "ai_signal": 9.5,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 40.0,
    "reasoning": "The content explicitly centers on the concept of experimentation within agile workflows, emphasizing hypothesis-driven development, learning through testing, and fostering adaptability and innovation. However, it does not directly reference—or even mention—the term 'Time to Market' or associated metrics like lead time, cycle time, or delivery speed. \n\nMentions (1.7): The content does not explicitly use the phrase 'Time to Market' nor its metric terminology; only indirect references to delivering value and adapting to changing demands relate tangentially. \n\nAlignment (4.8): The theme of delivering value and learning aligns in principle with 'Time to Market,' since faster learning can reduce the time to value, but this connection is implicit. The direct alignment to the efficiency of delivering market-ready products is not established.\n\nDepth (4.6): The content discusses experimentation deeply in the context of agile and organisational change but does not connect this depth specifically to Time to Market outcomes or improvement strategies.\n\nIntent (5.2): The purpose is educational and relevant to agile practitioners, supporting continuous improvement, but Time to Market is not the main focus; the intent is more broadly about experimentation culture.\n\nAudience (8.0): The content targets agile professionals, product teams, and those interested in continuous improvement—an audience overlapping significantly with individuals concerned with Time to Market.\n\nSignal (9.5): The content remains very focused on the key topic of experimentation, with virtually no irrelevant or off-topic digressions, keeping the noise low.\n\nOverall, the discussion is only tangentially related to Time to Market, as experimentation may enable faster delivery, but this connection is not articulated or explored directly. Thus, the 'Tertiary' level designation is appropriate.",
    "level": "Ignored"
  },
  "Systems Thinking": {
    "resourceId": "Experimentation",
    "category": "Systems Thinking",
    "calculated_at": "2025-05-06T11:36:59",
    "ai_confidence": 61.525,
    "ai_mentions": 2.7,
    "ai_alignment": 6.9,
    "ai_depth": 6.7,
    "ai_intent": 6.8,
    "ai_audience": 7.5,
    "ai_signal": 8.0,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 62.0,
    "reasoning": "The content centers on experimentation within agile workflows, emphasizing hypothesis-driven approaches, feedback loops, learning from failure, and empirical decision-making. These themes are conceptually adjacent to Systems Thinking—especially given the references to feedback, adaptation, and systemic change—but the language and focal point are experimentation and agile, not foundational Systems Thinking principles. \n\n1. Direct Mentions (2.7): The content does not name 'Systems Thinking,' nor does it reference key tools, foundational thinkers, or the terminology specific to the discipline. However, the inclusion of feedback loops and systemic change aligns partially.\n\n2. Conceptual Alignment (6.9): The piece implicitly touches on Systems Thinking themes (e.g., interconnections, feedback, holistic improvement) but falls short of directly exploring the field's core principles, such as mapping system structures, explicit analysis of system interdependencies, or use of systems frameworks.\n\n3. Depth of Discussion (6.7): It goes beyond surface-level discussion, exploring cultural, adaptive, and process-oriented aspects. Yet, the analysis is anchored in agile experimentation rather than full-system mapping or the explicit exploration of system dynamics.\n\n4. Intent / Purpose Fit (6.8): The main purpose is to support agile and innovation-oriented audiences in understanding experimentation. While this overlaps with Systems Thinking interests, it is not the primary intent.\n\n5. Audience Alignment (7.5): Agile practitioners, team leads, and organizational change agents are the primary audience—adjacent to those interested in Systems Thinking but not exclusively targeting them.\n\n6. Signal-to-Noise Ratio (8.0): The content is tightly focused and well-written, with little tangential or filler material, maintaining high relevance throughout.\n\nNo penalty deductions for outdatedness or tone are warranted, as the content is contemporary and not contradictory. Overall, while the content demonstrates secondary alignment with several Systems Thinking dimensions, it neither directly addresses nor deeply explores its foundational elements. Thus, the confidence is moderate and this resource would be considered 'Secondary' for the Systems Thinking category.",
    "level": "Secondary"
  },
  "Cycle Time": {
    "resourceId": "Experimentation",
    "category": "Cycle Time",
    "calculated_at": "2025-05-06T11:37:09",
    "ai_confidence": 26.513,
    "ai_mentions": 0.1,
    "ai_alignment": 2.2,
    "ai_depth": 2.0,
    "ai_intent": 2.5,
    "ai_audience": 7.0,
    "ai_signal": 5.5,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 27.0,
    "reasoning": "The content focuses on hypothesis-driven experimentation within agile workflows, emphasizing learning, adaptation, and innovation. There are no explicit mentions of 'Cycle Time' or direct reference to work item completion measurements. Conceptual alignment is minimal, as experimentation may indirectly affect metrics like Cycle Time, but this connection is not discussed. The depth of discussion is moderate, outlining benefits, methods, and cultural impact, yet remains unrelated to the measurement, management, or reduction of Cycle Time as defined in the classification. The intent is to advocate for experimentation, not to inform about Cycle Time. The audience overlaps somewhat (agile practitioners), reflected in the higher audience score. Signal-to-noise is moderate, as the content stays on-topic for experimentation but is off-topic regarding Cycle Time. No penalties for outdated information or tone are needed. In summary, the relevance to Cycle Time is tertiary at best—the content is largely orthogonal to the defined category.",
    "level": "Ignored"
  },
  "Miscellaneous": {
    "resourceId": "Experimentation",
    "category": "Miscellaneous",
    "calculated_at": "2025-05-06T11:37:01",
    "ai_confidence": 23.835,
    "ai_mentions": 1.2,
    "ai_alignment": 3.7,
    "ai_depth": 3.9,
    "ai_intent": 2.8,
    "ai_audience": 6.3,
    "ai_signal": 4.5,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 24.0,
    "reasoning": "The content centers on experimentation as a practice within agile workflows, explicitly mentioning 'agile workflows' and using language that closely aligns with recognised agile values (iterative progress, feedback loops, empirical learning). \n\n1. **Direct Mentions (1.2/10)**: The Miscellaneous category is not mentioned at all, nor are any of its key topics explicitly named; the focus remains on experimentation in the agile context, with multiple references to agile concepts.\n\n2. **Conceptual Alignment (3.7/10)**: The concept of experimentation is treated as an important facet within Agile, but it's discussed largely through the lens of recognised agile principles (e.g., iterative testing, empirical evidence, continuous improvement). Thus, it marginally aligns with 'miscellaneous' in that it avoids naming a specific framework, but directly supports established Agile practice.\n\n3. **Depth of Discussion (3.9/10)**: The content is substantial in explaining the rationale and benefits of experimentation. However, its depth is grounded in well-known Agile practices (empirical process control, adaptation, feedback loops), rather than offering disconnected or loosely related perspectives expected under Miscellaneous.\n\n4. **Intent/Purpose Fit (2.8/10)**: The content’s intent is not to be tangential or anecdotal but to inform or advocate for a widely recognised agile practice, so its suitability for Miscellaneous is limited to a low, incidental fit.\n\n5. **Audience Alignment (6.3/10)**: The likely audience consists of Agile practitioners, coaches, or leaders—a reasonable fit for the broader audience who might encounter Miscellaneous, but more directly aimed at those with an agile practice interest.\n\n6. **Signal-to-Noise Ratio (4.5/10)**: The discussion is focused and on-topic, but its relevance to Miscellaneous is only partial. It lacks digressions, but its central topic is not distinctly Miscellaneous.\n\nNo penalties applied: The content is current, not satirical, and does not undermine the category framing but simply doesn’t fit it closely.\n\nOverall, the confidence score is low because the content’s substance is directly embedded in agile values and practices, missing the detachment or ambiguity required for Miscellaneous. Its framing, terminology, and audience position it much closer to a core agile category, just without a named methodology, which is not sufficient for a stronger alignment.",
    "level": "Ignored"
  },
  "Technical Leadership": {
    "resourceId": "Experimentation",
    "category": "Technical Leadership",
    "calculated_at": "2025-05-06T11:37:31",
    "ai_confidence": 72.214,
    "ai_mentions": 2.3,
    "ai_alignment": 8.6,
    "ai_depth": 7.9,
    "ai_intent": 7.5,
    "ai_audience": 7.2,
    "ai_signal": 7.0,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 72.0,
    "reasoning": "The content focuses on experimentation in agile workflows, emphasizing hypothesis-driven approaches, feedback loops, and the importance of a culture of learning. These are conceptually aligned with technical leadership, especially regarding fostering continuous improvement, empirical decision-making, and driving team adaptability. The alignment score is strong (8.6), as the content describes team dynamics, value delivery, customer alignment, and organizational culture—all factors relevant to technical leadership roles. However, the content does not explicitly mention 'technical leadership,' nor does it directly discuss leadership principles, coaching, or agile ceremonies, which lowers the 'mentions' score (2.3). The 'depth' score (7.9) reflects its substantive exploration of experimentation, but it lacks detail about leadership behaviors or methods specific to technical leaders. 'Intent' is reasonably targeted (7.5) as it aims to promote key agile mindsets and practices that a technical leader might drive, while 'audience' (7.2) and 'signal' (7.0) are above average, as it addresses practitioners within agile teams but isn’t solely aimed at technical leaders. There are no off-topic tangents or obsolete references, so no penalties apply. Overall, this resource supports technical leadership in agile contexts by promoting a learning-oriented culture and systematic improvement, but does not function as a core resource in the category—thus, the confidence is solidly 'Secondary.'",
    "level": "Secondary",
    "reasoning_summary": "This content aligns well with technical leadership by highlighting experimentation, feedback, and a learning culture—key aspects leaders often champion in agile environments. However, it doesn’t directly address leadership roles, behaviours, or coaching, making it more supportive than central to the category. It’s valuable for technical leaders seeking to foster improvement, but isn’t a primary resource on technical leadership itself."
  },
  "Operational Practices": {
    "resourceId": "Experimentation",
    "category": "Operational Practices",
    "calculated_at": "2025-05-06T11:37:16",
    "ai_confidence": 86.1,
    "ai_mentions": 7.6,
    "ai_alignment": 9.5,
    "ai_depth": 8.9,
    "ai_intent": 8.8,
    "ai_audience": 8.2,
    "ai_signal": 8.3,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 86.0,
    "reasoning": "The content centers clearly on hypothesis-driven experimentation within agile workflows. \n\n— Mentions (7.6): The term 'experiment' and related language ('experimentation', 'hypothesis-driven', 'empirical evidence') is used explicitly multiple times, but 'operational practices' itself is not named directly, warranting a strong but not perfect score.\n\n— Alignment (9.5): The content's main ideas—systematic experimentation, empirical decision-making, iterative testing, continuous improvement—are highly aligned with operational efficiency, agility, and performance enhancement as described in the classification. It conceptually fits several key topics including 'evidence-based decision-making' and 'continuous improvement'.\n\n— Depth (8.9): The discussion thoroughly explores why and how experimentation informs operational practice, moving from specific methods (hypothesis-driven approaches) to cultural outcomes (organizational learning), with clear ties to delivery and process enhancement.\n\n— Intent (8.8): The primary purpose is to inform and encourage adoption of experimentation as a core operational practice in agile (and possibly Lean/DevOps) settings. The intent is highly supportive, directly serving the category's purpose.\n\n— Audience (8.2): The audience appears to be practitioners or organizational leaders in agile or Lean environments, which aligns well, though it is not explicitly technical or managerial—hence, slightly less than full marks.\n\n— Signal (8.3): The vast majority of the content is highly relevant and tightly focused on experimentation in operational settings, with little to no off-topic filler.\n\n— No penalties were applied, as the content is current, supportive, and matches the category's ethos.\n\n— Overall, this resource is best categorized as Primary for Operational Practices, rooted in its foundational focus on pragmatic, evidence-based improvement within agile workflows.",
    "level": "Primary",
    "reasoning_summary": "This content is an excellent fit for the Operational Practices category. It strongly emphasises hypothesis-driven experimentation and empirical decision-making within agile workflows, aligning closely with the category’s focus on continuous improvement and evidence-based operations. The discussion is thorough and relevant, making it a valuable resource for practitioners aiming to enhance operational efficiency and learning."
  },
  "Continuous Integration": {
    "resourceId": "Experimentation",
    "category": "Continuous Integration",
    "calculated_at": "2025-05-06T11:37:05",
    "ai_confidence": 18.22,
    "ai_mentions": 0.25,
    "ai_alignment": 2.2,
    "ai_depth": 2.1,
    "ai_intent": 2.0,
    "ai_audience": 6.0,
    "ai_signal": 4.0,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 18.0,
    "reasoning": "The content focuses broadly on experimentation within agile workflows, emphasizing hypothesis-driven approaches, iterative testing, and fostering a learning culture. However, it never directly mentions Continuous Integration (CI), nor does it reference related concepts such as code integration, CI tools, automated testing, or merging strategies. \n\n- Mentions (0.25): There are virtually no direct or explicit references to CI or its terminology, leading to a very low score.\n- Alignment (2.20): While experimentation and iterative improvement are adjacent to the agile/DevOps ecosystem in which CI operates, the main themes here are general agile practices, not the integration of code changes or specific CI principles.\n- Depth (2.10): The discussion is deep regarding experimentation, but not about CI or related practices, resulting in only surface-level alignment with the category.\n- Intent (2.00): The main purpose is to promote experimentation and hypothesis-driven improvement, not to inform or support CI-specific understanding.\n- Audience (6.00): The audience includes technical practitioners who may overlap with CI audiences, but the focus is broader (agile teams in general), not CI practitioners specifically.\n- Signal (4.00): The content is on-topic for agility and experimentation, but not for CI, so much is tangential in this context.\n\nNo penalties were applied, as the content is not outdated and does not contradict or undermine CI, just fails to address it directly. The correct classification level is \"Tertiary,\" since the link to CI is highly indirect—experimenting is a value in agile and DevOps contexts, but the content does not explore Continuous Integration itself.",
    "level": "Ignored"
  },
  "Customer Retention": {
    "resourceId": "Experimentation",
    "category": "Customer Retention",
    "calculated_at": "2025-05-06T11:37:09",
    "ai_confidence": 44.86,
    "ai_mentions": 1.3,
    "ai_alignment": 4.2,
    "ai_depth": 4.5,
    "ai_intent": 5.5,
    "ai_audience": 7.0,
    "ai_signal": 6.1,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 45.0,
    "reasoning": "The content foregrounds 'experimentation' within agile workflows, emphasizing hypothesis-driven development, iterative testing, and feedback loops as part of a continuous improvement process. However, there are no direct or explicit mentions of customer retention (score: 1.3), nor terminology specifically tied to the retention objective—rather, it is discussed in the context of general product and team improvement. While the concept of refining offerings to align with customer needs is present (alignment: 4.2), it is not the primary focus. The depth of discussion (4.5) into experimentation as a practice is solid, but exploring its impact on customer retention is only implicit—no specific retention strategies, metrics, or success stories are provided. The intent (5.5) partially overlaps, as improving products may support retention, but the content’s main purpose is not to inform directly about keeping customers engaged or minimizing churn. The audience is generally product teams or agile practitioners (7.0), which does align well with customer retention strategies in many cases. The signal-to-noise ratio (6.1) is above average since most of the content is relevant to agile improvement, but remains indirect in application to retention. No penalties are applied as the content is up-to-date and appropriately framed. Overall, the content only tangentially connects to the customer retention category and should be considered as Tertiary—useful as a supporting idea but not a primary resource for this classification.",
    "level": "Tertiary"
  },
  "Value Stream Mapping": {
    "resourceId": "Experimentation",
    "category": "Value Stream Mapping",
    "calculated_at": "2025-05-06T11:37:06",
    "ai_confidence": 19.835,
    "ai_mentions": 0.2,
    "ai_alignment": 2.25,
    "ai_depth": 2.5,
    "ai_intent": 3.25,
    "ai_audience": 6.65,
    "ai_signal": 7.7,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 20.0,
    "reasoning": "The content is focused on experimentation and hypothesis-driven approaches within agile workflows, emphasizing empirical learning, adaptation, and continuous improvement. However, it makes no direct mention of Value Stream Mapping (VSM) or its principles. There is only an indirect conceptual overlap in that both experimentation and VSM aim to improve processes and deliver value, but the main ideas, language, and examples here are unrelated to VSM’s visualization, mapping of value streams, or analysis of end-to-end product flow. The discussion lacks depth concerning VSM (scoring low in both direct mentions and conceptual alignment), and any alignment is broad (continuous improvement, learning mindset), not specific (no reference to mapping, wastes, or Lean techniques). The intent is tangential since the purpose is to encourage experimentation in general, not specifically via VSM. The audience could partially overlap—both topics likely address agile or Lean practitioners—but the content is generic to agile teams, not uniquely VSM professionals. Signal-to-noise is relatively strong, as the content is focused, but it’s off-target for VSM. No penalties were applied, as the content is current, relevant to modern practices, and does not contradict Lean principles. Overall, the content is at a tertiary connection level: only very generally related via the shared agile/Lean ethos, but not within scope for the VSM category.",
    "level": "Ignored"
  },
  "Sprint Review": {
    "resourceId": "Experimentation",
    "category": "Sprint Review",
    "calculated_at": "2025-05-06T11:37:12",
    "ai_confidence": 11.05,
    "ai_mentions": 0.1,
    "ai_alignment": 2.0,
    "ai_depth": 1.8,
    "ai_intent": 1.4,
    "ai_audience": 3.2,
    "ai_signal": 2.5,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 11.0,
    "reasoning": "The content titled 'Experimentation' discusses hypothesis-driven methods and the importance of testing and learning in agile workflows, but does not mention the Sprint Review event, its unique purpose, processes, or roles within Scrum. There is no direct mention of Sprint Reviews, nor does the content describe reviews of increments, stakeholder engagement as per Scrum, or any of the specific key topics outlined for the category. Conceptually, while agile experimentation is broadly relevant to adaptation and learning (themes tangentially present in Sprint Reviews), the alignment is superficial—there's no substantive link to the actual Sprint Review ceremony or its mechanics. The depth is low, as 'experiment' is explored in a general agile or organizational culture context rather than addressing inspection, feedback, or backlog adaptation through Sprint Review. The intent is general promotion of a scientific mindset in agile, not focusing on Spring Reviews or practitioners of this Scrum ceremony. The likely audience includes agile practitioners and organizational leaders, which partially overlaps, but not specifically Sprint Review participants. Most of the content is on-topic for experimentation in agile, but irrelevant for Sprint Review (signal-to-noise is low for the target category). No penalties are applied as tone and practices are not wrong or outdated. In sum: this resource is tertiary to Sprint Review at best—it might encourage the kind of mindset that helps Sprint Reviews be effective, but does not address the event itself.",
    "level": "Ignored"
  },
  "Throughput": {
    "resourceId": "Experimentation",
    "category": "Throughput",
    "calculated_at": "2025-05-06T11:37:02",
    "ai_confidence": 17.84,
    "ai_mentions": 0.4,
    "ai_alignment": 1.7,
    "ai_depth": 1.8,
    "ai_intent": 2.2,
    "ai_audience": 6.8,
    "ai_signal": 4.1,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 18.0,
    "reasoning": "The content is focused on the philosophy, value, and approaches behind hypothesis-driven experimentation in agile workflows. It does not mention throughput explicitly or describe throughput-related measurement, calculation, analysis, or visualisation. There is a brief reference to 'delivering value predictably and sustainably,' which very indirectly brushes against delivery metrics but does not engage with throughput as defined for the category. \n\nMentions: The term 'throughput' does not appear, and the closest proxy is a vague reference to delivery, justifying a minimal score (0.4).\nAlignment: The main concepts are experimentation, learning, and culture—not throughput metrics—resulting in a low, but non-zero alignment (1.7), since the topic is in the broader delivery space.\nDepth: The discussion is moderately detailed, but only on experimentation; there is no depth on throughput or metrics, so this is also very low (1.8).\nIntent: The intent is to inform on experimentation in agile, which is tangential at best to the throughput category's purpose (2.2).\nAudience: The audience seems well-matched to agile practitioners (6.8), which overlaps with the likely audience for throughput discussions.\nSignal-to-noise: The content is focused and on-topic for its own subject, but not for throughput (4.1).\n\nNo penalties were applied because the content is not outdated, nor does it undermine or contradict the throughput framing. Given the very weak direct and conceptual connections to throughput, this is clearly a tertiary match, with the low confidence score appropriate for such marginal relevance.",
    "level": "Ignored"
  },
  "Software Development": {
    "resourceId": "Experimentation",
    "category": "Software Development",
    "calculated_at": "2025-05-06T11:37:02",
    "ai_confidence": 70.6,
    "ai_mentions": 3.6,
    "ai_alignment": 7.9,
    "ai_depth": 7.7,
    "ai_intent": 7.2,
    "ai_audience": 7.6,
    "ai_signal": 7.2,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 71.0,
    "reasoning": "The content focuses on hypothesis-driven experimentation within agile workflows, which aligns with software development methodologies like Agile. However, direct references to 'software development' or its key terms (e.g., SDLC, coding, CI/CD) are absent, leading to a low 'Direct Mentions' score (3.6). Conceptual alignment (7.9) is strong because hypothesis-driven experimentation is relevant and valuable in software development, especially in agile contexts where testing assumptions is integral. The depth (7.7) reflects that the article provides reasoning and impact but lacks technical specifics or process detail, mentioning iterative testing and feedback loops but not methods like TDD, CI/CD, or code-level practices. The intent (7.2) is constructive and relevant for process improvement in development teams, though is framed more as general product/process improvement rather than strictly software engineering. Audience alignment (7.6) is fairly high, as it primarily targets agile teams—common in software organizations—but also references general organizations and cross-functional teams, not just software engineers. Signal-to-noise ratio (7.2) is high overall; the majority of the text is relevant, focused on the purposeful integration of experimentation in product/process development, but it is not exclusively software development (mentions 'organisations' generally and does not cite software-specific practices). No outdated or contradictory content, so no penalties apply. In summary, experimentation as described is a secondary, supportive theme in software development contexts—especially Agile—but with broader application, so the level is 'Secondary'. The confidence score (70.6) accurately reflects strong alignment and substance, but with indirect framing and some generalization.",
    "level": "Secondary",
    "reasoning_summary": "This content is a good fit for the category as it explores hypothesis-driven experimentation, a concept central to agile software development. However, it doesn’t directly reference software-specific terms or practices, making its relevance more supportive than primary. Its focus on agile teams and iterative improvement aligns well with software development, but the broader framing means it’s best classified as a secondary match."
  },
  "Install and Configuration": {
    "resourceId": "Experimentation",
    "category": "Install and Configuration",
    "calculated_at": "2025-05-06T11:37:03",
    "ai_confidence": 7.04,
    "ai_mentions": 0.1,
    "ai_alignment": 0.25,
    "ai_depth": 0.3,
    "ai_intent": 0.15,
    "ai_audience": 3.0,
    "ai_signal": 2.8,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 7.0,
    "reasoning": "The content focuses entirely on the theoretical and cultural value of experimentation in agile workflows, with no direct or indirect mention of install, setup, or configuration activities. \n\nMentions (0.10): The content never references 'install' or 'configuration', nor any related terms. The minimal score is awarded simply because experiment-driven work sometimes overlaps with tool setup in broader agile contexts, but no explicit tie-in exists here.\n\nAlignment (0.25): The main concept—hypothesis-driven experimentation and cultural adaptation—bears almost no relation to technical installation or configuration as defined for this category. It only aligns insofar as experimentation might eventually lead to new tool adoption, but that is a significant stretch and not discussed.\n\nDepth (0.30): The entire discussion is at a high conceptual and process level, focused on mindset, workflow, and cultural impacts. No procedures, settings, tool walkthroughs, or technical steps are described or even alluded to.\n\nIntent (0.15): The purpose is motivational and conceptual, not instructive or technical. There is no intent to provide actionable setup guidance or configuration support, which is core to the Install and Configuration category.\n\nAudience (3.00): While the audience is broadly within the agile practitioner context, this piece is targeted at those interested in team process, cultural change, and product discovery, rather than technical practitioners or system admins tasked with installs/configs. It may be *read* by technical people, but does not serve their configuration-related needs.\n\nSignal (2.80): The content is focused and coherent, but entirely irrelevant to the configuration theme. All of it is 'noise' for someone looking for install or configuration guidance.\n\nNo penalties are applied since the content is recent and not satirical or contrarian, just misaligned with the category. The tertiary level assignment reflects that there is at best a remote and largely theoretical overlap with the intended scope of 'Install and Configuration.' The score is proportionately extremely low in recognition of the near-total misfit with the classification.",
    "level": "Ignored"
  },
  "Asynchronous Development": {
    "resourceId": "Experimentation",
    "category": "Asynchronous Development",
    "calculated_at": "2025-05-06T11:37:07",
    "ai_confidence": 21.317,
    "ai_mentions": 0.5,
    "ai_alignment": 2.2,
    "ai_depth": 2.6,
    "ai_intent": 3.7,
    "ai_audience": 5.1,
    "ai_signal": 4.3,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 21.0,
    "reasoning": "This content focuses entirely on the value and methodology of experimentation within agile workflows, specifically using hypothesis-driven approaches for improvement. There are no explicit mentions of asynchronous development, its principles, or practices; nor is there reference to distributed teams, asynchronous communication, or comparison with synchronous methods. The conceptual alignment is weak: while experimentation is a useful practice that could be employed in asynchronous teams, nothing in the content specifies or discusses the asynchronous context. The depth is moderate for its own topic (experimentation), but not for asynchronous development. The intent is broader—informing on experimentation as an agile/team practice, but not with a clear link to the purpose or use case of the 'Asynchronous Development' category. The audience (agile teams, practitioners) could overlap somewhat, but the focus is on generic improvement processes rather than remote, asynchronous teamwork. The signal-to-noise ratio is decent for the given theme but only minimally relevant to asynchronous development. No penalties for outdated content or contradictory tone apply. Therefore, this resource is only tangentially related (tertiary level), and the low confidence score reflects how little it touches on asynchronous development as defined.",
    "level": "Ignored"
  },
  "Agile Leadership": {
    "resourceId": "Experimentation",
    "category": "Agile Leadership",
    "calculated_at": "2025-05-06T11:37:03",
    "ai_confidence": 68.077,
    "ai_mentions": 2.8,
    "ai_alignment": 7.9,
    "ai_depth": 7.3,
    "ai_intent": 6.7,
    "ai_audience": 6.2,
    "ai_signal": 6.3,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 68.0,
    "reasoning": "The content extensively discusses experimentation as a core practice within agile workflows, emphasizing hypothesis-driven approaches, empirical validation, and cultivating a culture of learning and adaptability—concepts that strongly align with Agile principles. However, while these themes are adjacent to Agile Leadership, explicit focus on the leadership role is limited. \n\n- **Mentions (2.8):** The content does not directly reference 'Agile Leadership' nor explicitly discuss leadership practices or leaders, but it does mention empowerment and cultural cultivation, which can be linked to leadership indirectly.\n- **Conceptual Alignment (7.9):** The main ideas—empowerment, fostering a culture of experimentation, continuous improvement, adaptability—fit well with the core meaning of Agile Leadership, even if not explicitly naming it.\n- **Depth (7.3):** The article explores experimentation's impact on team culture, adaptability, learning, and long-term change at a thoughtful level but stops short of analyzing the leader's specific role in these dynamics.\n- **Intent (6.7):** The content aims to be informative and supportive of broader Agile principles and cultural change, tangentially matching Agile Leadership's intent but not directly targeting leadership-specific purposes.\n- **Audience (6.2):** The audience is likely Agile practitioners or teams broadly, rather than exclusively leaders, missing a more executive- or leadership-oriented perspective.\n- **Signal (6.3):** Most content is focused on relevant Agile cultural and process themes, but significant portions remain general or practitioner-oriented rather than centered on leadership practice specifically.\n\nNo dimensions warranted penalty points: there is no evidence of outdated concepts or undermining tone. Overall, this rates as a 'Secondary' fit: the article covers themes highly relevant to Agile Leadership but does not focus on leadership roles, strategies, or direct practices, hence the confidence score (68.077) reflects a strong but not primary alignment.",
    "level": "Secondary"
  },
  "Project Management": {
    "resourceId": "Experimentation",
    "category": "Project Management",
    "calculated_at": "2025-05-06T11:37:08",
    "ai_confidence": 62.952,
    "ai_mentions": 2.8,
    "ai_alignment": 6.7,
    "ai_depth": 6.1,
    "ai_intent": 7.3,
    "ai_audience": 6.6,
    "ai_signal": 7.2,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 63.0,
    "reasoning": "The content thoroughly explores experimentation within agile workflows, describing hypothesis-driven processes, the value of learning from failure, and fostering adaptability. However, it does not directly mention 'project management,' any project management methodologies, or roles such as project manager, nor does it address classic project management topics like scope, time, cost, or governance structures. The alignment and depth scores are moderate: experimentation is highly relevant to agile and iterative project management, but the discussion is focused almost exclusively on the cultural and process benefits of experimentation in agile contexts, not on its explicit integration into the broader discipline of project management. The intent targets improving team and organizational outcomes—aligned with management goals—but remains more relevant for agile practitioners and teams rather than for a general project management audience. The signal-to-noise ratio is quite strong, with the content remaining focused, although some generalizations about culture and innovation shift it slightly away from project management specifics. Since there are no direct mentions or references to obsolete practices, no penalties have been applied. This content most appropriately fits as a 'Secondary' level: useful for project managers interested in agile and continuous improvement, but not central or comprehensive coverage of project management as defined by the category.",
    "level": "Secondary"
  },
  "Open Space Agile": {
    "resourceId": "Experimentation",
    "category": "Open Space Agile",
    "calculated_at": "2025-05-06T11:37:07",
    "ai_confidence": 57.57,
    "ai_mentions": 0.65,
    "ai_alignment": 6.45,
    "ai_depth": 6.1,
    "ai_intent": 6.85,
    "ai_audience": 7.4,
    "ai_signal": 7.95,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 58.0,
    "reasoning": "Direct Mentions (0.65): The content does not explicitly mention 'Open Space Agile' or 'Open Space Technology' at all. General agile concepts are referenced, but without direct linkage to the category. \nConceptual Alignment (6.45): The content centers on experimentation, iterative testing, psychological safety, and adaptability, which are conceptually aligned with principles found in Open Space Agile (e.g., emergence, continuous improvement). However, the connection to Open Space Technology or the unique aspects of Open Space Agile (collaborative agenda-setting, self-organisation, collective prioritisation) are not drawn out. \nDepth of Discussion (6.1): The discussion goes somewhat in depth into the philosophy and mechanics of agile experimentation, exploring its cultural and organisational implications. However, it falls short of delving into Open Space Agile's specific practices or frameworks beyond general agile experimentation. \nIntent / Purpose Fit (6.85): The primary purpose is informative and supportive, aimed at enhancing agile practices through experimentation, which is compatible with but not uniquely about Open Space Agile.\nAudience Alignment (7.4): The target audience is agile practitioners and organisations seeking transformation—broadly aligned with Open Space Agile, but not specifically those interested in open space approaches.\nSignal-to-Noise (7.95): The content is focused and relevant to agile experimentation, with little to no filler, but lacks focus on 'Open Space Agile'.\n\nNo penalties apply, as there are no outdated or critical/undermining elements. \n\nLevel: Tertiary—the connection is indirect and primarily through conceptual overlap with agile practices such as emergence and experimentation, rather than explicit or in-depth discussion of Open Space Agile itself.",
    "level": "Tertiary"
  },
  "Product Owner": {
    "resourceId": "Experimentation",
    "category": "Product Owner",
    "calculated_at": "2025-05-06T11:37:04",
    "ai_confidence": 41.382,
    "ai_mentions": 0.6,
    "ai_alignment": 4.6,
    "ai_depth": 3.3,
    "ai_intent": 4.9,
    "ai_audience": 5.1,
    "ai_signal": 5.8,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 41.0,
    "reasoning": "The content focuses on the practices of hypothesis-driven experimentation within Agile workflows. While experimentation is essential in Agile environments, there is no direct mention of the Product Owner role, nor explicit reference to their accountability as defined. The main theme aligns indirectly, as Product Owners may facilitate or support hypothesis-driven work, but the topic isn't uniquely about their accountability—the discussion is generic to teams and organizations rather than focusing on backlog prioritization, stakeholder communication, or maximizing value through explicit Product Owner actions. The depth is moderate, providing a broad case for experimentation, but not delving into specific responsibilities or frameworks pertinent to Product Owners. The intent is generally informative for Agile teams or organizations as a whole rather than targeted to Product Owners. The audience might include Product Owners among others, but it's not specialized for them. The signal is moderately good—the content is on-topic for Agile, but not tightly scoped to the Product Owner accountability. No penalties were applied, as the content is current and not contradictory.",
    "level": "Tertiary"
  },
  "Azure Repos": {
    "resourceId": "Experimentation",
    "category": "Azure Repos",
    "calculated_at": "2025-05-06T11:37:07",
    "ai_confidence": 7.57,
    "ai_mentions": 0.1,
    "ai_alignment": 0.2,
    "ai_depth": 0.15,
    "ai_intent": 0.15,
    "ai_audience": 7.3,
    "ai_signal": 7.6,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 8.0,
    "reasoning": "This content does not mention Azure Repos at all (score: 0.10 for mentions). The discussion is high-level, focusing solely on experimentation and hypothesis-driven approaches in agile workflows. There is no reference to source control, versioning practices, Git/TFVC, branching strategies, pull requests, or any Azure Repos functionality. Conceptual alignment (0.20) and depth of discussion (0.15) with Azure Repos are extremely limited, as the themes revolve around organisational culture and general agile principles rather than any aspect of Azure Repos. The intent of the content (0.15) is to inform about experimentation in agile, not to support users of Azure Repos or discuss its use. Audience alignment (7.30) is somewhat present because the content seems aimed at teams working in agile/software development settings, which is part of the Azure Repos target audience, even though the match isn't precise. The signal-to-noise ratio (7.60) is relatively high for its actual topic, as the content is focused and free of filler, but almost all information is off-topic for the Azure Repos category. No penalties are applied since the content is neither outdated nor critical. This is a clear 'Tertiary' level match as Azure Repos is only potentially related to the distant context of agile but not substantively engaged here.",
    "level": "Ignored"
  },
  "Business Agility": {
    "resourceId": "Experimentation",
    "category": "Business Agility",
    "calculated_at": "2025-05-06T11:37:05",
    "ai_confidence": 83.124,
    "ai_mentions": 6.7,
    "ai_alignment": 8.4,
    "ai_depth": 8.9,
    "ai_intent": 8.2,
    "ai_audience": 8.3,
    "ai_signal": 7.7,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 83.0,
    "reasoning": "The content 'Experimentation' centers on the value of hypothesis-driven testing and learning within agile workflows—core aspects of both agile and broader business agility principles. \n\n(1) Mentions (6.7): The term 'agile' is explicitly used multiple times (e.g., 'agile workflows'), and there are several indirect references to agility (e.g., 'adapt', 'respond to changing market demands'), but the term 'business agility' itself is not directly named. Hence, an above-average but not top score.\n\n(2) Alignment (8.4): Conceptually, the text closely aligns with business agility principles: encouraging innovation, adaptability, responding to change, and building a culture of resilience and learning. It avoids focus on unrelated frameworks.\n\n(3) Depth (8.9): The content thoroughly discusses experimentation beyond simple mentions—exploring cultural impact, the purpose behind experimentation, and its systemic benefits. However, it does not directly discuss all of the nuances of broad business agility (such as leadership or organizational alignment), so the score is high but not perfect.\n\n(4) Intent (8.2): The intent supports informing readers about a key practice for innovation and adaptability—very much in line with the business agility category's purpose. However, as it focuses specifically on experimentation rather than the whole business agility picture, the score is slightly moderated.\n\n(5) Audience (8.3): The language and substance target practitioners and leaders interested in agile and innovation—overlapping very well with business agility audiences (though not exclusively executives or strategists).\n\n(6) Signal (7.7): The content is focused, relevant, and has little to no tangential material, though there are minor instances of generalization (e.g., 'complex, fast-paced landscape') that offer broad context rather than strict focus.\n\nNo penalties were applied: The content is current, neutral/positive in tone, and does not reference obsolete practices.\n\nLevel: Secondary—While highly relevant, the content is centered on a foundational practice (experimentation) that is central to business agility but not a comprehensive exploration or case study of business agility itself. Hence, it is mapped as 'Secondary'.\n\nThe final confidence score (83.124) appropriately reflects that the resource is a strong fit for the category but is not an archetypal or primary resource about business agility as a whole.",
    "level": "Primary",
    "reasoning_summary": "This content is a strong match for the business agility category, as it explores experimentation—a key agile practice that fosters adaptability and learning. While it doesn’t cover every aspect of business agility, it effectively addresses core principles like innovation and responsiveness, making it highly relevant for practitioners. However, since it focuses on a single practice rather than the full spectrum, it’s best classified as a secondary resource."
  },
  "Forecasting": {
    "resourceId": "Experimentation",
    "category": "Forecasting",
    "calculated_at": "2025-05-06T11:37:16",
    "ai_confidence": 47.057,
    "ai_mentions": 0.3,
    "ai_alignment": 4.2,
    "ai_depth": 4.5,
    "ai_intent": 4.4,
    "ai_audience": 8.2,
    "ai_signal": 7.9,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 47.0,
    "reasoning": "The content focuses on experimentation within Agile practices, emphasizing hypothesis-driven approaches, iterative testing, and empirical learning. While these are thematically related to core Agile concepts, the content does not directly address forecasting methodologies, empirical prediction of timelines, risk management through forecasts, or metrics like velocity, burn-down charts, or cumulative flow diagrams. \n\n- Mentions (0.3): The term 'forecasting' is not mentioned, nor are explicitly related terms. All relatedness is via indirect conceptual overlap.\n- Conceptual Alignment (4.2): Experimentation and empirical validation align tangentially with the empirical roots of forecasting but do not engage directly with the intent or practices of forecasting within Scrum/Agile.\n- Depth (4.5): The discussion is moderately detailed in articulating the benefits of experimentation but lacks specific ties to how experimentation informs or enhances forecasting practices.\n- Intent/Purpose (4.4): The intent is to advocate for and elaborate on experimentation as a critical Agile skill, not directly on forecasting or predictive practices, but there is some minor adjacency in that experimentation may eventually inform forecasting.\n- Audience (8.2): The content targets Agile practitioners, which is consistent with the intended audience for forecasting discussions.\n- Signal (7.9): The content is focused and concise, though its relevance to the forecasting domain is limited.\n\nNo penalties were applied, as the content is neither outdated nor contradictory in tone. However, as the focus is not primarily on forecasting but rather touches on organizational learning and adaptability through experimentation, this evaluation rates the overall fit as tertiary with a low-to-moderate confidence score.",
    "level": "Tertiary"
  },
  "Deployment Frequency": {
    "resourceId": "Experimentation",
    "category": "Deployment Frequency",
    "calculated_at": "2025-05-06T11:37:22",
    "ai_confidence": 32.58,
    "ai_mentions": 0.7,
    "ai_alignment": 4.1,
    "ai_depth": 3.8,
    "ai_intent": 3.4,
    "ai_audience": 7.2,
    "ai_signal": 6.6,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 33.0,
    "reasoning": "The content centers on experimentation within agile workflows, specifically emphasizing hypothesis-driven development and empirical validation. While these concepts are central to Agile and DevOps, there are very few direct references to Deployment Frequency: the phrase is never mentioned, nor are there explicit discussions of deployment intervals, release cadence, CI/CD, or other core topics from the Deployment Frequency classification. \n\n1. Mentions (0.7): The term 'deployment frequency' is not mentioned. 'Feedback loops' and 'continuous improvement' are referenced but not as they relate specifically to deployment cycles.\n2. Alignment (4.1): The content is conceptually adjacent—experimentation and feedback loops do support Agile goals, but there is no explicit linkage to optimizing deployment intervals or releases.\n3. Depth (3.8): The discussion is substantial around experimentation but lacks any exploration of deployment frequency, its metrics, practices, or impacts.\n4. Intent (3.4): The main purpose is to promote a culture of experimentation—not optimizing deployment frequency. It is supportive of broader Agile/DevOps goals but not this specific category.\n5. Audience (7.2): The audience overlaps—practitioners interested in Agile—but is not specifically targeted at those responsible for deployment optimization (e.g., release managers, DevOps specialists).\n6. Signal (6.6): The content is focused, with minimal off-topic discussion, but relevance to deployment frequency remains peripheral.\n\nNo penalties were applied because the content is not obsolete nor does it contradict the category. However, due to the lack of direct connection, this resource would only be tangential (‘Tertiary’) for the Deployment Frequency category.",
    "level": "Ignored"
  },
  "Automated Testing": {
    "resourceId": "Experimentation",
    "category": "Automated Testing",
    "calculated_at": "2025-05-06T11:37:31",
    "ai_confidence": 32.25,
    "ai_mentions": 0.7,
    "ai_alignment": 3.1,
    "ai_depth": 3.6,
    "ai_intent": 2.5,
    "ai_audience": 7.0,
    "ai_signal": 7.9,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 32.0,
    "reasoning": "The content provided discusses 'Experimentation' within agile workflows, focusing on hypothesis-driven approaches, testing ideas, validating assumptions, and fostering innovation through empirical evidence. However, there is no direct mention of automated testing or any of its associated principles, practices, or tools (e.g., test automation frameworks, test maintenance, CI/CD, or the role of automated testing in Agile/DevOps). \n\nFor Direct Mentions (0.7): There are no explicit references to 'automated testing,' 'test automation,' or named tools. References to 'testing' are conceptual and non-technical. \n\nFor Conceptual Alignment (3.1): The text aligns at a high level with the idea of testing and feedback loops in agile, but it does not link these to automated software testing practices. The focus is on experimentation as a cultural and methodological concept, not as test automation. \n\nFor Depth of Discussion (3.6): There is some discussion of testing and feedback loops, but no substantial exploration of how automated testing contributes to software quality, reliability, frameworks, or technical implementation. \n\nIntent/Purpose Fit (2.5): The content's purpose is to promote experimentation as a learning and innovation driver, which relates only tangentially to automated testing and does not aim to educate or guide on its methods or purpose. \n\nAudience Alignment (7.0): The content is targeted at agile practitioners, which overlaps partially with the audience for automated testing content. However, it is not aimed directly at test automation professionals. \n\nSignal-to-Noise Ratio (7.9): The content is focused, but nearly all of it centers on experimentation as a general agile principle, not on automated testing, so the signal relating to the category is very low.\n\nLevel—Tertiary: This resource is several conceptual steps removed from automated testing, possibly relevant for context or philosophy, but not as a primary or secondary source for automated testing discussions.\n\nNo penalties are applied, as the content is not outdated or actively undermining the category.",
    "level": "Ignored"
  },
  "Complexity Thinking": {
    "resourceId": "Experimentation",
    "category": "Complexity Thinking",
    "calculated_at": "2025-05-06T11:37:12",
    "ai_confidence": 41.925,
    "ai_mentions": 1.7,
    "ai_alignment": 4.8,
    "ai_depth": 4.3,
    "ai_intent": 5.3,
    "ai_audience": 7.9,
    "ai_signal": 8.3,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 42.0,
    "reasoning": "The content primarily discusses the role of experimentation and hypothesis-driven approaches in agile workflows. It references concepts like uncertainty, adaptation, feedback, and emergent learning, which have tangential connections to complexity thinking. However, there are no direct or explicit mentions of complexity science principles, key frameworks (such as Cynefin), or recognized complexity theorists. The alignment is partial: experimentation and learning from failure are valuable in complex environments but are not in themselves unique to complexity thinking, and the content lacks depth in exploring complexity-specific themes like emergence, self-organization, or non-linear dynamics. The main intent is to promote experimentation within an agile context, which overlaps with but does not directly serve the aims or core audience of complexity thinking. The audience is practitioners interested in improvement and innovation (aligned somewhat with complexity, but more general). The signal-to-noise ratio is high as the content is focused and avoids filler. No penalties are applied, as the content is recent, neutral in tone, and avoids misrepresentation. Given these factors, this resource deserves a 'Tertiary' classification — it is adjacent to, but not central or even secondary, to Complexity Thinking.",
    "level": "Tertiary"
  },
  "Azure Pipelines": {
    "resourceId": "Experimentation",
    "category": "Azure Pipelines",
    "calculated_at": "2025-05-06T11:37:36",
    "ai_confidence": 7.95,
    "ai_mentions": 0.2,
    "ai_alignment": 0.8,
    "ai_depth": 0.9,
    "ai_intent": 0.6,
    "ai_audience": 0.55,
    "ai_signal": 0.55,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 8.0,
    "reasoning": "The content discusses experimentation within agile workflows, focusing on hypothesis-driven testing and iterative improvement. While these topics are tangentially related to some DevOps and CI/CD principles (e.g., testing, continuous improvement), there is no direct or explicit mention of Azure Pipelines, CI/CD, or any of the platform-specific practices highlighted in the classification definition. \n\nMentions (0.20): Azure Pipelines is never mentioned, nor are its synonyms or direct components. There is only a general context of testing and automation. \n\nAlignment (0.80): The discussion aligns minimally with the Azure Pipelines category in that it addresses practices (experimentation, testing) that are sometimes implemented within CI/CD pipelines. However, the connection is conceptual and not directly mapped to Azure Pipelines principles, practices, or tooling. \n\nDepth (0.90): The content explores the value of experimentation comprehensively, but not as it relates to pipeline automation or Azure Pipelines implementation. Thus, depth relating to Azure Pipelines is nearly absent. \n\nIntent (0.60): The intent is clearly to inform and support agile team practices, which could include users of Azure Pipelines—however, the purpose is not to discuss pipelines or their management specifically, so this alignment is weak. \n\nAudience (0.55): The target audience appears to be agile practitioners or team leads, not specifically technical practitioners working on pipeline automation or Azure DevOps projects. \n\nSignal (0.55): The content is focused and relevant to experimentation in agile, with no irrelevant filler, but little to no signal linking it to Azure Pipelines or the specifics of CI/CD tooling. \n\nNo penalties applied, as the content is not outdated nor critical of the category. \n\nOverall, this is a tertiary fit at most; the subject matter is relevant only in the broadest sense—relating to cultural underpinnings of automation and improvement, but wholly lacking Azure Pipelines references or details.",
    "level": "Ignored"
  },
  "Windows": {
    "resourceId": "Experimentation",
    "category": "Windows",
    "calculated_at": "2025-05-06T11:37:10",
    "ai_confidence": 7.667,
    "ai_mentions": 0.7,
    "ai_alignment": 0.8,
    "ai_depth": 0.7,
    "ai_intent": 0.6,
    "ai_audience": 2.1,
    "ai_signal": 0.8,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 8.0,
    "reasoning": "The content provided centers entirely around the concept of experimentation in the context of agile workflows, with a focus on hypothesis-driven approaches, iterative testing, and organizational change. There is no mention or allusion to the Windows operating system, its configuration, installation, troubleshooting, or any of the other key topics specific to the Windows category. The main themes are about agile methodology and business process improvement—not technical aspects related to Windows. Direct Mentions (0.7): No explicit or implicit reference to Windows in title, description, or content. Conceptual Alignment (0.8): The themes are not aligned with the specific Windows category; minor technical workflow concepts are business-agnostic and only align peripherally at best. Depth (0.7): The discussion is substantive for experimentation in agile, but wholly unrelated to Windows. Intent (0.6): The intent is to inform on experimentation in agile, not on Windows management, troubleshooting, or usage. Audience Alignment (2.1): Targets process-oriented professionals, such as product managers or agile coaches, not technical users seeking Windows-specific content. Signal (0.8): Entirely focused but on an off-topic area for the Windows category; no filler or tangential info, but signal is not relevant. No penalties are applied, as the content is recent and the tone is appropriate. The content fits the Windows category at only the faintest, indirect (tertiary) level, as some agile principles might be applied in IT projects generally—hence non-zero scores, but confidence is extremely low.",
    "level": "Ignored"
  },
  "Lean Thinking": {
    "resourceId": "Experimentation",
    "category": "Lean Thinking",
    "calculated_at": "2025-05-06T11:37:06",
    "ai_confidence": 62.94,
    "ai_mentions": 1.7,
    "ai_alignment": 7.2,
    "ai_depth": 6.9,
    "ai_intent": 7.1,
    "ai_audience": 6.8,
    "ai_signal": 7.4,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 63.0,
    "reasoning": "The content thoroughly explores the value of hypothesis-driven experimentation in agile workflows. While Lean Thinking emphasizes continuous improvement (kaizen), waste reduction, and empirical process control, the content here explicitly focuses on experimentation as a cultural and practical catalyst for learning and improvement. There are strong conceptual ties: fostering a culture of experimentation aligns with Lean principles like continuous improvement, empirical validation, and delivering value. However, the text does not directly mention Lean Thinking, its principles (e.g., Value Stream Mapping, 5S, Muda), or key terminology. The audience—agile practitioners—overlaps moderately with Lean audiences, but could also include those outside strict Lean contexts. Depth is solid: the content goes beyond surface-level encouragement and discusses organizational change and learning loops. The signal is high due to a focused, relevant discussion with minimal tangents or filler. No penalties were applied: the tone is positive, contemporary, and aligned, and there are no outdated references. The scores reflect moderate to strong secondary alignment to Lean Thinking: experimentation is foundational to Lean but the text does not directly identify, reference, or explicitly anchor in Lean frameworks. Thus, this is best classified as 'Secondary' relevance: someone interested in Lean would find much to appreciate—especially on continuous improvement and validated learning—but Lean is not the explicit topic.",
    "level": "Secondary"
  },
  "Azure Boards": {
    "resourceId": "Experimentation",
    "category": "Azure Boards",
    "calculated_at": "2025-05-06T11:37:07",
    "ai_confidence": 24.79,
    "ai_mentions": 0.22,
    "ai_alignment": 2.1,
    "ai_depth": 2.0,
    "ai_intent": 3.19,
    "ai_audience": 4.5,
    "ai_signal": 3.8,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 25.0,
    "reasoning": "The content on 'Experimentation' broadly discusses hypothesis-driven approaches, learning cycles, and their importance in agile workflows. While there is some indirect conceptual overlap with the Agile principles that underpin Azure Boards, there is no explicit mention of Azure Boards, its features, or even Azure DevOps in general. \n\n- Mentions (0.22): There are zero direct mentions or implicit references to Azure Boards; the content never names the tool or its features.\n- Conceptual Alignment (2.1): While experimentation aligns with agile principles, the content remains general to agile workflows and does not connect its concepts to Azure Boards or related tooling, reducing alignment.\n- Depth (2.0): The discussion is deep in general agile experimentation theory, but offers no insight or exploration about how Azure Boards enables or supports experimentation, limiting depth for this classification.\n- Intent (3.19): The intent is aligned with supporting agile practices and continuous improvement, which is related to Azure Boards' purpose in theory, but the relevance is somewhat tangential as the tool's use is not addressed.\n- Audience (4.5): The content targets agile practitioners and teams – a partially overlapping audience with Azure Boards content, but it is not specifically geared towards practitioners using Azure Boards or wanting tool-based advice.\n- Signal (3.8): While the content is focused with minimal off-topic filler, all focus is devoted to generalized agile experimentation rather than Azure Boards specifically.\n\nNo outdated practices or contradictory tone were detected, so no penalties were applied. The overall confidence is tertiary: the content would be at best tangential or peripheral in an 'Azure Boards' category, being too abstract and lacking tool or feature focus.",
    "level": "Ignored"
  },
  "Value Delivery": {
    "resourceId": "Experimentation",
    "category": "Value Delivery",
    "calculated_at": "2025-05-06T11:37:10",
    "ai_confidence": 85.99,
    "ai_mentions": 7.7,
    "ai_alignment": 9.5,
    "ai_depth": 8.8,
    "ai_intent": 8.9,
    "ai_audience": 8.7,
    "ai_signal": 9.2,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 86.0,
    "reasoning": "Direct Mentions (7.7): The content does not explicitly use the phrase 'value delivery', but does mention 'deliver value' and frequently discusses outcomes closely aligned to value delivery (e.g., 'enhance their ability to innovate', 'refine their products... ensuring alignment with customer needs'). Still, the direct category language is limited, so slightly above average is warranted.\n\nConceptual Alignment (9.5): The entire piece revolves around hypothesis-driven experimentation as a mechanism to achieve agile goals. It describes continuous improvement, empirical validation, customer alignment, and adaptability—central to value delivery. It strongly matches the philosophy of incremental value, though it does not enumerate typical frameworks (e.g., Scrum, DevOps) explicitly.\n\nDepth of Discussion (8.8): The content provides substantive explanation, discussing both immediate and cultural impacts of experimentation. It mentions iterative feedback loops and systemic, organisational embedding. However, it lacks detailed practical methodologies or examples (e.g., CI/CD, value stream mapping), so doesn't reach full score for depth.\n\nIntent/Purpose Fit (8.9): The purpose is instructive, advocating for experimentation to better deliver value, foster adaptation, and align with customer needs. The content fits squarely within the value delivery intent, though the explicit call-out of agile frameworks is implicit rather than detailed.\n\nAudience Alignment (8.7): The tone and content are appropriate for agile practitioners, team leads, or organisational strategists interested in agile principles—a direct match to the expected audience for value delivery discussions.\n\nSignal-to-Noise Ratio (9.2): The text is clear, focused, and contains no off-topic content. All statements reinforce the relevance of experimentation to value, with only minor filler ('not merely a tactic but a foundational element' is general but thematically on point). High signal overall.\n\nNo Penalties: The content is current, affirms value delivery, and exhibits no critical or satirical tone. It fits within established theories of value management in Agile contexts.\n\nLevel (Secondary): Experimentation is clearly positioned as a foundational or enabling practice for achieving value delivery, but it does not exclusively focus on value delivery itself. The link is strong but not the sole or primary framing.",
    "level": "Primary",
    "reasoning_summary": "The content strongly aligns with the value delivery category, focusing on how experimentation drives continuous improvement and customer alignment—key aspects of value delivery in agile contexts. While it doesn’t use the exact category language or detail specific frameworks, its intent, depth, and audience fit make it highly relevant, positioning experimentation as a core enabler of delivering value."
  },
  "Revenue per Employee": {
    "resourceId": "Experimentation",
    "category": "Revenue per Employee",
    "calculated_at": "2025-05-06T11:37:16",
    "ai_confidence": 14.143,
    "ai_mentions": 0.4,
    "ai_alignment": 1.7,
    "ai_depth": 1.2,
    "ai_intent": 1.6,
    "ai_audience": 2.0,
    "ai_signal": 2.2,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 14.0,
    "reasoning": "The content focuses on the theme of experimentation in agile workflows, specifically regarding hypothesis-driven approaches, empirical validation, and learning from failure. However, there is no direct mention of 'Revenue per Employee', nor any discussion tying experimentation back to financial observability, workforce efficiency, or use of business metrics to assess organisational throughput. \n\n1. Mentions (0.4): The metric 'Revenue per Employee' is not referenced even indirectly; closest alignment is to organisational effectiveness but without financial terms.\n2. Alignment (1.7): While the content touches on systemic organisational improvement, it does not address the use of revenue-based or workforce efficiency metrics.\n3. Depth (1.2): The discussion is moderately deep about experimentation but never connects to financial measures or the core quantitative observability lens required by the tag.\n4. Intent (1.6): The intent is mainly to explain the role of experimentation in agile, not to inform or analyze from a Revenue per Employee perspective.\n5. Audience (2.0): Some crossover to audiences (agile practitioners, organisational leaders) who might be interested in metrics, but primarily aimed at teams and cultural transformation rather than executives focused on financial metrics.\n6. Signal (2.2): The content remains tightly on experimentation; it's focused and relevant, but not to the revenue/employee metric.\n\nNo penalties applied, as the content is not outdated, nor critical or contradictory. The confidence score is low and appropriately tertiary, reflecting only tangential (if any) connections to the Revenue per Employee category.",
    "level": "Ignored"
  },
  "Sociotechnical Systems": {
    "resourceId": "Experimentation",
    "category": "Sociotechnical Systems",
    "calculated_at": "2025-05-06T11:37:18",
    "ai_confidence": 82.738,
    "ai_mentions": 6.3,
    "ai_alignment": 8.7,
    "ai_depth": 8.1,
    "ai_intent": 7.8,
    "ai_audience": 7.5,
    "ai_signal": 8.0,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 83.0,
    "reasoning": "The content does not explicitly mention 'sociotechnical systems' or named frameworks (Direct Mentions: 6.3); it refers instead to concepts such as organisational culture, team empowerment, collaboration across cross-functional teams, and iterative feedback—strongly aligned with sociotechnical principles (Alignment: 8.7). The discussion moves beyond a surface description, covering experimentation's role in organisational adaptation, learning culture, and system-level change (Depth: 8.1). The primary intent is to advocate for organisational adoption of experimentation as foundational to team and company success, which is closely aligned but not *wholly centered* on sociotechnical systems (Intent: 7.8). The audience seems to align with practitioners and leaders interested in agile and organisational improvement, which only partially overlaps with the sociotechnical systems audience (Audience: 7.5). The content remains focused throughout, with nearly all of it relevant to team/organisational improvement, though it stops short of referencing classic sociotechnical theory or explicit technical elements (Signal: 8.0). There are no penalties for being outdated, critical, or off-tone. Level is marked 'Secondary': the content is highly relevant to sociotechnical systems but frames experimentation as a mechanism within agile/organisational context rather than giving sociotechnical systems as a *primary subject*.",
    "level": "Primary",
    "reasoning_summary": "This content strongly aligns with sociotechnical principles by emphasising organisational culture, team empowerment, and cross-functional collaboration. However, it doesn’t directly reference sociotechnical systems or their frameworks, instead focusing on experimentation within agile and organisational contexts. While highly relevant, it treats sociotechnical systems as a secondary theme rather than the main subject, making the fit partial but significant."
  },
  "Team Motivation": {
    "resourceId": "Experimentation",
    "category": "Team Motivation",
    "calculated_at": "2025-05-06T11:37:14",
    "ai_confidence": 72.94,
    "ai_mentions": 3.7,
    "ai_alignment": 8.2,
    "ai_depth": 7.6,
    "ai_intent": 8.3,
    "ai_audience": 7.9,
    "ai_signal": 7.7,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 73.0,
    "reasoning": "The content directly addresses experimentation in agile workflows, framing it as a means to empower teams, foster a culture of learning, and promote engagement. In terms of 'Direct Mentions', the explicit phrase 'team motivation' is absent, and direct references to that concept are limited, hence a conservative score of 3.7. 'Conceptual Alignment' is strong (8.2); the text discusses how experimentation leads to engagement, collaboration, psychological safety (e.g., 'failure is viewed as a learning opportunity'), and motivation ('as team members see the tangible impact of their contributions'). 'Depth' is also robust (7.6): the discussion moves beyond surface-level mentions to explore implications for team culture, learning, and adaptation, though it does not focus exclusively or in great depth on the psychological mechanics of motivation. For 'Intent/Purpose', the content's primary aim is to advocate for experimentation as a beneficial team practice, closely aligned but not solely focused on team motivation (score: 8.3). 'Audience Alignment' (7.9) reflects that the piece targets agile teams and organizations, matching the category's relevant audience. The 'Signal-to-Noise Ratio' is high (7.7), as nearly all content is relevant to team dynamics and culture, with minimal tangential material. No penalty is applied as content is current, and the tone aligns with the framing of team motivation. The level is 'Secondary' because team motivation is a significant but not the central theme; the piece uses it as a justification for experimentation rather than as the core topic. Overall, the confidence score proportionally reflects substantial—but not primary—relevance to the category.",
    "level": "Secondary",
    "reasoning_summary": "This content fits the category because it explores how experimentation within agile teams can enhance engagement, collaboration, and psychological safety—factors closely linked to team motivation. While 'team motivation' isn't directly emphasised, the discussion highlights its importance as a benefit of experimentation, making motivation a strong secondary theme rather than the main focus. The content is relevant and well-aligned with the intended audience."
  },
  "Acceptance Test Driven Development": {
    "resourceId": "Experimentation",
    "category": "Acceptance Test Driven Development",
    "calculated_at": "2025-05-06T11:37:18",
    "ai_confidence": 16.35,
    "ai_mentions": 0.2,
    "ai_alignment": 1.6,
    "ai_depth": 2.1,
    "ai_intent": 2.4,
    "ai_audience": 5.0,
    "ai_signal": 4.3,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 16.0,
    "reasoning": "The provided content discusses 'Experimentation' in agile workflows, focusing on hypothesis-driven development, learning through testing, and organizational culture change. There is no direct mention of Acceptance Test Driven Development (ATDD) or its core terminology (e.g., acceptance criteria, stakeholder collaboration, testable requirements). The conceptual alignment with ATDD is weak: while both practices value testing and learning, ATDD is specifically about pre-defining acceptance tests as requirements, which is not addressed here. The content remains at a general methodology and cultural level, lacking depth in ATDD-related methods, tools, or examples. The primary intent is to advocate for empirical and experimental practices, not to instruct or explore ATDD. Although the audience (agile practitioners, teams) may overlap with those interested in ATDD, the content is not focused on the specific needs or frameworks relevant to ATDD. Signal-to-noise is moderate; most content is on-topic for general agile experimentation, but it is not targeted at or deep in discussion about ATDD. No penalties were applied because the content is current and does not criticize or misrepresent ATDD. The content's relation to the ATDD category is clearly tertiary—it does not serve as a primary or secondary resource for that subject.",
    "level": "Ignored"
  },
  "Organisational Culture": {
    "resourceId": "Experimentation",
    "category": "Organisational Culture",
    "calculated_at": "2025-05-06T11:37:09",
    "ai_confidence": 91.66,
    "ai_mentions": 8.3,
    "ai_alignment": 9.7,
    "ai_depth": 9.3,
    "ai_intent": 9.1,
    "ai_audience": 8.6,
    "ai_signal": 8.0,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 92.0,
    "reasoning": "This content explicitly links experimentation to 'organisational culture,' with direct mentions such as 'fostering a culture of experimentation' and 'embedding a scientific approach into the organisational culture.' The main theme revolves around how a culture that embraces experimentation empowers teams to innovate, adapt, and improve—all highly aligned with the category's definition. The depth is strong, touching on psychological safety, continuous improvement, learning from failure, collaboration, and adaptability—core cultural dimensions relevant to Agile and DevOps adoption. The intent is to highlight experimentation as a cultural enabler rather than a technical tactic, reinforcing the centrality of culture. The audience seems to be leaders and practitioners interested in transformation and agility. Signal-to-noise is high, though there are brief nods to process (e.g., 'iterative testing') that are still tied back to cultural impact, not technical detail. No penalties apply as the content is up-to-date, positive, and fully consistent with Agile/DevOps culture theory. Scores vary subtly to reflect slightly heavier direct alignment and depth relative to explicit frequency of category mentions and the slight breadth in targeting change agents and practitioners.",
    "level": "Primary",
    "reasoning_summary": "This content is a strong fit for the category, as it centres on how fostering a culture of experimentation drives organisational innovation and adaptability. It explores key cultural aspects like psychological safety and learning from failure, making it highly relevant for leaders and practitioners focused on Agile or DevOps transformation. The emphasis remains on cultural change rather than technical processes."
  },
  "Enterprise Agility": {
    "resourceId": "Experimentation",
    "category": "Enterprise Agility",
    "calculated_at": "2025-05-06T11:37:32",
    "ai_confidence": 67.02,
    "ai_mentions": 3.4,
    "ai_alignment": 8.7,
    "ai_depth": 7.8,
    "ai_intent": 7.3,
    "ai_audience": 8.5,
    "ai_signal": 7.2,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 67.0,
    "reasoning": "The content emphasizes experimentation as a catalyst for adaptability and continuous improvement, aligning well with enterprise agility principles. It explicitly references organisational benefits and culture ('organisations can enhance their ability to innovate, adapt, and respond...'; 'embedding a scientific approach into the organisational culture'). However, there are few if any direct, explicit references to 'enterprise agility' or formal enterprise-level frameworks, resulting in a low 'mentions' score. The conceptual alignment is high because the discussion covers fostering adaptability, learning, and systemic change—all key aspects of enterprise agility. The depth is solid: the article discusses cultural and organisational impact rather than surface-level team practices, but it isn't a deep enterprise agility how-to or case study. For intent, the piece is somewhat general, focusing on the practice of experimentation with strong but not exclusive ties to enterprise agility as a whole. The target audience aligns well with both agile practitioners and those influencing organisational change (e.g., 'organisations', 'cross-functional teams', 'organisational culture'), leading to a high alignment score. The signal-to-noise ratio is good, with nearly all content relevant, save for a small degree of overlap with team-level practices. No penalties are applied, as the content is current, positive, and not out-of-date. This content is best classified as 'Secondary' because, while it strongly supports enterprise agility themes, it is not solely or directly about the enterprise transformation process or frameworks as primary examples would be.",
    "level": "Secondary"
  },
  "Liberating Structures": {
    "resourceId": "Experimentation",
    "category": "Liberating Structures",
    "calculated_at": "2025-05-06T11:37:10",
    "ai_confidence": 14.86,
    "ai_mentions": 0.12,
    "ai_alignment": 0.87,
    "ai_depth": 0.95,
    "ai_intent": 1.22,
    "ai_audience": 7.38,
    "ai_signal": 7.92,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 15.0,
    "reasoning": "The content provides a general overview of experimentation within agile workflows, focusing on hypothesis-driven approaches and their value in fostering learning and adaptability within organizations. \n\n1. **Direct Mentions (0.12/10):** There are no explicit references to 'Liberating Structures' or any of its methods. The terminology used ('experimentation', 'hypothesis-driven', 'empirical evidence') does not allude to or mention any Liberating Structures, nor does it discuss facilitation toolkits or any named structures (such as 1-2-4-All, Troika Consulting), resulting in the minimum nonzero score.\n\n2. **Conceptual Alignment (0.87/10):** The core ideas (experimentation, empirical learning, engagement) are valued within Liberating Structures, but there is no mention of specific facilitation techniques or the toolkit itself. The content is adjacent, not directly aligned: experimentation is a broad concept while Liberating Structures is a specific toolkit to facilitate similar outcomes. Alignment is, at best, superficial.\n\n3. **Depth of Discussion (0.95/10):** The content does provide depth about experimentation principles, learning culture, and iterative workflows in agile, but this discussion is centered on experimentation as a general practice, not on the application or depth of Liberating Structures. The score reflects slightly more exploration than pure alignment, as practical benefits and organizational impacts are discussed—but still with no reference to the category.\n\n4. **Intent / Purpose Fit (1.22/10):** The intent is to inform about the value and role of experimentation within agile, which could theoretically support discussions about Liberating Structures, but this fit is very weak since there is no mention or application to the category's tools or methods.\n\n5. **Audience Alignment (7.38/10):** The content is aimed at agile teams, organizations, and teams dealing with iterative or innovative work. This partially overlaps with the target audience of Liberating Structures (Scrum Masters, Agile Coaches, leaders), but does not specifically address facilitators or practitioners of facilitation techniques. Score is slightly above average for general agile professionals.\n\n6. **Signal-to-Noise Ratio (7.92/10):** The content remains focused on experimentation throughout, with minimal tangents or filler. However, since the primary topic is not Liberating Structures itself, there is significant topical deviation per the strict classification definition. The score is adjusted accordingly.\n\n**Level:** Tertiary — The relationship is only conceptual and indirect; Liberating Structures are not discussed, referenced, or applied. The content pertains to a general method (experimentation) that could theoretically be facilitated by Liberating Structures, but it neither suggests nor explores this.\n\n**No penalty adjustments** were warranted since the content is current and does not contradict or undermine the Liberating Structures framing.",
    "level": "Ignored"
  },
  "Increment": {
    "resourceId": "Experimentation",
    "category": "Increment",
    "calculated_at": "2025-05-06T11:37:27",
    "ai_confidence": 34.504,
    "ai_mentions": 0.3,
    "ai_alignment": 3.4,
    "ai_depth": 3.6,
    "ai_intent": 4.2,
    "ai_audience": 6.0,
    "ai_signal": 4.6,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 35.0,
    "reasoning": "The content focuses primarily on the value and process of experimentation in agile, emphasizing hypothesis-driven approaches, empirical validation, and fostering a culture of learning. There are no direct mentions of 'Increment' nor any references to tangible, usable output or working software produced at the end of an iteration as required for the Increment category. Conceptually, the piece loosely aligns with aspects of Agile that could lead to increments through learning, but it does not engage with the specific Scrum artifact or its role in delivering working software. The depth of discussion is solid for experimentation itself, but increment is only tangentially supported as a possible downstream effect. The intent is more about improving agile processes and culture than about producing or managing increments. The audience is reasonably aligned with agile practitioners, yet not specifically those concerned with Scrum Increments. Signal is moderate since the content is focused but diverges from the Increment category, and there is no notable off-topic fluff. No penalties apply as the information is current and neutrally presented. The evaluation, therefore, lands at a low tertiary level of fit for the Increment category, with a confidence score reflecting its peripheral relevance.",
    "level": "Ignored"
  },
  "Mentoring": {
    "resourceId": "Experimentation",
    "category": "Mentoring",
    "calculated_at": "2025-05-06T11:37:15",
    "ai_confidence": 31.875,
    "ai_mentions": 0.6,
    "ai_alignment": 3.5,
    "ai_depth": 3.7,
    "ai_intent": 3.9,
    "ai_audience": 5.0,
    "ai_signal": 5.5,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 32.0,
    "reasoning": "The content focuses on hypothesis-driven experimentation within agile workflows, emphasising continuous improvement, empirical validation, and fostering a culture of learning and adaptability. However, there are no direct mentions of mentoring, coaching, or guidance roles associated with skill or professional development. \n\nMentions (0.6): The content never refers to mentoring, coaching, or related roles explicitly; any connection to mentoring is only implicit through its emphasis on learning culture.\nAlignment (3.5): The discussion conceptually overlaps with aspects of mentoring, such as supporting continuous learning and improvement, yet it does not meaningfully reference the act of one individual guiding another, nor the specific mentoring process as defined.\nDepth (3.7): Experimentation and its value for team and organisational growth are explored in some depth, but the focus is on process/culture, not on substantial elements of mentoring (role-modeling, skill feedback, development strategies).\nIntent (3.9): The content intends to inform and advocate for experimentation as a core agile value, not to provide guidance or coaching; any mentoring alignment is clearly secondary or less.\nAudience (5.0): The audience is broadly agile professionals—team members and possibly leaders—consistent with the mentoring audience, but not directly tailored to mentors/mentees.\nSignal (5.5): Most of the content is relevant to agile improvement and team development, yet it is not focused on mentoring, making its relevance only indirect.\nNo penalties were applied as the content is neither outdated nor contrary in tone.\nOverall, the content is tertiary to mentoring: it describes a process and culture that may create a fertile environment for mentoring, but does not address mentoring directly or in depth.",
    "level": "Ignored"
  },
  "Strategic Goals": {
    "resourceId": "Experimentation",
    "category": "Strategic Goals",
    "calculated_at": "2025-05-06T11:37:10",
    "ai_confidence": 63.728,
    "ai_mentions": 2.6,
    "ai_alignment": 7.8,
    "ai_depth": 7.4,
    "ai_intent": 6.8,
    "ai_audience": 6.2,
    "ai_signal": 7.3,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 64.0,
    "reasoning": "The content focuses on experimentation as a key element of agile workflows, emphasizing hypothesis-driven approaches, empirical decision-making, and fostering a culture of learning and adaptability. However, it only tangentially references strategic goals—mentioning 'organisational goals' in the context of product refinement and alluding to 'long-term, systemic change' and 'continuous improvement.' These points align directionally with aspects of strategic goals, particularly the fostering of adaptability and business agility, but the content does not directly define, develop, or explicitly discuss strategic goal-setting, alignment frameworks, or measurement methods. The main intent is to advocate for experimentation as a cultural and operational practice within agile work, which supports but does not centrally focus on strategic goals themselves. Key audiences include agile teams and organisational leaders interested in agility and continuous improvement, but the content stops short of targeting the strategic planning or executive audience explicitly. There is strong relevance throughout (signal-to-noise ratio is high), though most of the discussion lands at the intersection of agile practices and cultural change rather than long-term business objectives. No penalties are applied: the content is current, does not contradict Agile principles, and the tone is constructive. Thus, the content best fits as a 'Secondary' resource for the Strategic Goals category: it enables and supports the achievement of strategic objectives in agile organisations, but is not directly focused on the strategy itself.",
    "level": "Secondary"
  },
  "Market Share": {
    "resourceId": "Experimentation",
    "category": "Market Share",
    "calculated_at": "2025-05-06T11:37:20",
    "ai_confidence": 22.385,
    "ai_mentions": 0.9,
    "ai_alignment": 2.1,
    "ai_depth": 2.3,
    "ai_intent": 2.7,
    "ai_audience": 7.4,
    "ai_signal": 5.9,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 22.0,
    "reasoning": "The content focuses on experimentation in agile workflows using hypothesis-driven approaches. While it discusses innovation, adaptation, and responding to market demands, it does not mention market share directly or elaborate on strategies specifically aimed at increasing market presence or competitive advantage. \n\nMentions (0.9): There is no explicit mention or reference to 'market share' or its direct synonyms. \nAlignment (2.1): The alignment is weak—the themes (testing, learning, innovation) are generally beneficial to business competitiveness, but the content lacks any direct focus on market share as defined by the category.\nDepth (2.3): The discussion is surface-level and broad regarding product development processes and organizational culture. There is no deep dive into metrics, competitive analysis, or market capturing strategies keyed to market share. \nIntent (2.7): The main purpose is to promote experimentation for better product development and adaptation, not targeting the expansion of market share or competitive advantage explicitly. \nAudience (7.4): The audience is primarily practitioners and agile teams, somewhat overlapping with strategists interested in innovation, but not specifically targeted at market share-focused executives. \nSignal (5.9): While relevant for agile/product development topics, most of the content is not about market share—signal to noise is moderate due to some indirect relevance via innovation and business impact.\n\nNo penalties were applied as the tone is neutral and current. Overall, the content at best has tertiary, tangential relevance to the core definition of the 'Market Share' category.",
    "level": "Ignored"
  },
  "System Configuration": {
    "resourceId": "Experimentation",
    "category": "System Configuration",
    "calculated_at": "2025-05-06T11:37:10",
    "ai_confidence": 13.85,
    "ai_mentions": 0.35,
    "ai_alignment": 1.55,
    "ai_depth": 1.65,
    "ai_intent": 2.35,
    "ai_audience": 4.1,
    "ai_signal": 4.6,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 14.0,
    "reasoning": "The content focuses on 'experimentation' within agile workflows, specifically highlighting hypothesis-driven approaches to product and process improvement. There are no direct or even indirect references to system configuration, configuration management, hardware/software integration, or technical tools for system setup, maintenance, or automation. \n\n- **Mentions (0.35):** There is zero explicit mention of system configuration or related terms; the closest is some tangential discussion of 'processes,' but these are general organizational processes, not technical configurations. \n- **Alignment (1.55):** The content’s main ideas are entirely about organizational experimentation and agile culture, not the core activities discussed in system configuration as defined. \n- **Depth (1.65):** The discussion is deep regarding experimentation but irrelevant to system configuration; no systemic, technical, or operational exploration of configuration topics is present.\n- **Intent (2.35):** The intent is to promote a culture and practice of experimentation in agile teams (organizational improvement), not to inform, guide, or support system configuration per se. \n- **Audience (4.10):** The audience could, in some cases, overlap with technical practitioners involved in agile or DevOps, but the focus is on cultural/organizational roles rather than system administrators or engineers who deal directly with configuration.\n- **Signal (4.60):** The content is highly focused—but entirely on experimentation in agile, without off-topic drift. However, the relevant signal for 'system configuration' is virtually absent; nearly all content is unrelated to the category.\n\nNo penalties are applied—the information is modern and professionally delivered, without tone issues. The overall categorization is 'Tertiary': the content is only very distantly adjacent to any aspect of system configuration, with minimal topical overlap or relevance.",
    "level": "Ignored"
  },
  "Hypothesis Driven Development": {
    "resourceId": "Experimentation",
    "category": "Hypothesis Driven Development",
    "calculated_at": "2025-05-06T11:37:14",
    "ai_confidence": 80.025,
    "ai_mentions": 7.7,
    "ai_alignment": 8.6,
    "ai_depth": 7.8,
    "ai_intent": 8.1,
    "ai_audience": 7.5,
    "ai_signal": 7.65,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 80.0,
    "reasoning": "The content directly references 'hypothesis-driven approaches' and emphasizes testing ideas and validating assumptions, which are central to Hypothesis Driven Development (HDD). The discussion strongly aligns conceptually with HDD principles—validation through experimentation, empirical evidence over intuition, and fostering an adaptive, learning-focused culture. Several key HDD topics are mentioned: hypothesis-driven mindset, experimentation as a process, feedback loops, and continuous improvement. However, there is less granular discussion of the explicit mechanics (like specific experiment design, A/B testing, or metrics/KPIs) that would signal thorough depth. The intent is clearly aligned, aiming to inform and advocate for approaches central to HDD. The audience is consistent with practitioners in agile and product development environments, although it could be slightly more technical or case-study focused for maximal audience fit. The signal-to-noise ratio is high—the content remains focused but includes some general advocacy rather than detailed, technical exploration. No penalties were applied: the material is up-to-date, affirms the category's framing, and does not diverge into satire or general Agile commentary. The level is Primary due to close conceptual alignment and intent, even though deeper executional specifics could further boost the confidence score.",
    "level": "Secondary",
    "reasoning_summary": "This content is a strong fit for the Hypothesis Driven Development category. It clearly promotes hypothesis-driven thinking, experimentation, and learning from evidence—core HDD principles. While it could include more technical details or case studies, its focus and intent align well with practitioners interested in adopting or understanding HDD approaches."
  },
  "Scrum": {
    "resourceId": "Experimentation",
    "category": "Scrum",
    "calculated_at": "2025-05-06T11:37:11",
    "ai_confidence": 56.52,
    "ai_mentions": 1.8,
    "ai_alignment": 6.4,
    "ai_depth": 7.7,
    "ai_intent": 6.6,
    "ai_audience": 7.5,
    "ai_signal": 7.2,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 57.0,
    "reasoning": "The content provides a thorough discussion of experimentation in agile workflows, focusing on hypothesis-driven approaches, empirical evidence, iterative testing, feedback loops, and continuous improvement. These concepts are broadly relevant to Scrum, as empirical process control, adaptation, and team collaboration are central to the Scrum framework. The content's depth is commendable—the discussion extends beyond surface-level mentions to explore the role of experimentation in fostering innovation and adaptability. However, the content does not directly mention 'Scrum,' nor does it reference any specific Scrum roles, events, or artifacts (mentions: 1.8). Its main audience appears to align with agile practitioners and teams (audience: 7.5), and its intent fits with encouraging organizational change and continuous improvement (intent: 6.6), which is consistent with Scrum philosophy but not exclusive to it. Conceptual alignment (6.4) is moderate because the described principles of feedback, adaptiveness, and empirical decision-making clearly mirror Scrum's philosophy, though they are not uniquely tied to it and could apply to Agile more generally. The signal-to-noise ratio and depth scores are strengthened by the focused, rich content with minimal digression (signal: 7.2, depth: 7.7). Overall, while there is solid philosophical overlap between the described experimentation and Scrum's empirical roots, the lack of direct naming or reference to specific Scrum constructs positions this content as 'Secondary' rather than 'Primary' for the Scrum category.",
    "level": "Tertiary"
  },
  "Product Delivery": {
    "resourceId": "Experimentation",
    "category": "Product Delivery",
    "calculated_at": "2025-05-06T11:37:20",
    "ai_confidence": 74.037,
    "ai_mentions": 4.312,
    "ai_alignment": 8.741,
    "ai_depth": 7.988,
    "ai_intent": 8.221,
    "ai_audience": 8.964,
    "ai_signal": 8.627,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 74.0,
    "reasoning": "The content 'Experimentation' distinctly addresses hypothesis-driven practices in agile workflows, which conceptually aligns with several key aspects of Product Delivery—iterative development, feedback loops, and cross-functional collaboration. \n\n1. Direct Mentions (4.312): 'Product delivery' itself is not explicitly named anywhere in the text. However, terms central to Product Delivery, such as 'agile workflows', 'teams', 'iterative testing', and 'feedback loops', are referenced. The discussion is implicit but not direct or frequent, warranting a moderately low score here.\n\n2. Conceptual Alignment (8.741): The core of the piece—experimentation within agile teams to validate assumptions and adjust based on feedback—strongly fits Product Delivery’s method-centric, customer-value-driven focus. Iterative improvement and leveraging feedback are highlighted. However, it doesn’t address every dimension (e.g., detailed deployment or release strategies), hence not a perfect score.\n\n3. Depth of Discussion (7.988): The discussion covers reasons for experimentation, its impact on team culture and learning, and the role of feedback—but remains high-level, without diving into concrete delivery practices or detailed process engineering. Substantial, but not comprehensive.\n\n4. Intent / Purpose Fit (8.221): The content is strongly aligned with the intent of improving practices related to team delivery and customer value, and is supportive and informative. The main purpose is supportive, not tangential, but it isn’t entirely delivery process-centric, focusing a bit more on culture and learning.\n\n5. Audience Alignment (8.964): Written in a tone and at a depth that targets practitioners in agile product delivery environments: product owners, agile coaches, delivery managers, and cross-functional team leads. The focus is technical-practitioner level and aligns well with the intended audience.\n\n6. Signal-to-Noise Ratio (8.627): The content is tightly focused on experimentation and its value in agile and product development, with little visible filler or tangential material. Nearly all content is relevant, though some general statements about culture are present.\n\nNo penalties were applied, as the material is current and non-contradictory. \n\nOverall, the content fits as a secondary representation of the Product Delivery category: it aligns strongly and exemplifies essential principles and mindsets, but is not wholly specific to the mechanics, metrics, or stages of product delivery. The final score appropriately reflects strong alignment with some gaps in explicit naming and a slightly broader focus (team culture, learning) than product delivery alone.",
    "level": "Secondary",
    "reasoning_summary": "This content fits well within the Product Delivery category, as it explores experimentation in agile teams—a core aspect of iterative product development and feedback-driven improvement. While it doesn’t explicitly mention “product delivery” or delve into detailed delivery mechanics, its focus on agile practices, team learning, and customer value makes it highly relevant for practitioners in this space, though with a slightly broader lens on culture and learning."
  },
  "Current Value": {
    "resourceId": "Experimentation",
    "category": "Current Value",
    "calculated_at": "2025-05-06T11:37:25",
    "ai_confidence": 41.775,
    "ai_mentions": 0.7,
    "ai_alignment": 4.2,
    "ai_depth": 4.6,
    "ai_intent": 5.8,
    "ai_audience": 9.1,
    "ai_signal": 7.8,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 42.0,
    "reasoning": "The content robustly discusses experimentation within Agile workflows, focusing on hypothesis-driven development, iterative testing, and the cultural impact of experimentation. While these topics are highly relevant to value delivery frameworks such as Agile and Evidence-Based Management, there is only an indirect linkage to 'Current Value' as defined in the classification. \n\nMentions (0.7): The content does not mention 'Current Value' directly, nor does it cite any closely related indicators or metrics (e.g., customer satisfaction, revenue impact), leading to a very low score.\n\nAlignment (4.2): The themes of empirical validation and learning align tangentially with the measurement ethos of Current Value, but the content remains conceptually focused on process (experimentation) rather than the real-time measurement of value.\n\nDepth (4.6): It discusses the practical and cultural aspects of experimentation with reasonable detail but does not engage in specific discussions about value realization or measurement, leaving the depth with only partial credit.\n\nIntent (5.8): The intent is constructive and relevant for Agile practitioners, leaning into organizational improvement, but lacks a focus specifically on measuring or demonstrating Current Value.\n\nAudience (9.1): The audience is clearly Agile practitioners, teams, and organizational leaders, which strongly overlaps with the intended audience for Current Value discussions.\n\nSignal (7.8): The content is highly focused on its topic (experimentation); while relevant to evidence-based approaches, it is largely absent of filler, but slightly misses on strict category-topic focus.\n\nNo penalties are warranted: The content is up to date, constructive in tone, and references modern Agile practices. \n\nLevel: 'Tertiary' is appropriate because, while the content is peripherally related to the category, it is neither central nor sustained in emphasis on Current Value, instead focusing on the process of learning and improvement.",
    "level": "Tertiary"
  },
  "Cross Functional Teams": {
    "resourceId": "Experimentation",
    "category": "Cross Functional Teams",
    "calculated_at": "2025-05-06T11:37:27",
    "ai_confidence": 65.45,
    "ai_mentions": 4.7,
    "ai_alignment": 7.3,
    "ai_depth": 7.0,
    "ai_intent": 6.8,
    "ai_audience": 7.9,
    "ai_signal": 7.5,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 65.0,
    "reasoning": "The content focuses on experimentation in agile workflows, discussing hypothesis-driven approaches and their benefits for innovation, continuous improvement, and product development. There is one brief explicit mention of cross-functional teams ('...enhances collaboration across cross-functional teams...'), but otherwise the central focus is on experimentation as a general agile value. \n\n- Mentions (4.7): Cross-functional teams are only specifically named once, and the majority of the content refers generically to 'teams,' not distinguishing cross-functional structure. This limits the direct, explicit reference score.\n- Alignment (7.3): The content conceptually aligns with the cross-functional teams category by discussing practices (experimentation, collaboration, learning) valued in cross-functional environments. However, the main theme is not the structure or unique attributes of cross-functional teams, so alignment is partial but not perfect.\n- Depth (7.0): While the benefits and process of experimentation are covered in some depth, there is only a minor connection drawn between this and cross-functional team organization or management. The exploration relevant to cross-functional teams is thus moderately deep but not full.\n- Intent (6.8): The intent is to promote experimentation as a key agile practice, not to directly inform about cross-functional teams. The fit is supportive but not central or fully aligned.\n- Audience (7.9): The target audience seems to be agile practitioners and organizational leaders—similar to the cross-functional teams category—though it remains general rather than tailored directly to cross-functional team leads or members.\n- Signal (7.5): The majority of the content is focused and relevant for agile ways of working (with tangential value for the cross-functional teams category), but the proportion of truly 'on-category' material (structure, challenges, best practices for cross-functional teams specifically) is not high enough to rate higher.\n\nNo penalties were applied as the content is current, supportive in tone, and does not reference outdated practices. The confidence score (65.45) reflects a moderate to strong secondary relevance to the Cross Functional Teams category, mainly because experimentation is a concept applied by cross-functional teams, but the content does not thoroughly focus on the formation, management, or unique aspects of such teams.",
    "level": "Secondary"
  },
  "Decision Making": {
    "resourceId": "Experimentation",
    "category": "Decision Making",
    "calculated_at": "2025-05-06T20:57:10",
    "ai_confidence": 91.126,
    "ai_mentions": 7.4,
    "ai_alignment": 9.4,
    "ai_depth": 8.3,
    "ai_intent": 8.6,
    "ai_audience": 8.9,
    "ai_signal": 9.3,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 91.0,
    "reasoning": "The content explicitly discusses hypothesis-driven approaches and experimentation as mechanisms for making informed, evidence-based decisions in agile environments. While the exact phrase 'decision making' is not heavily repeated, the entire discussion is anchored around structured, empirical practices that align directly with the Decision Making category definition. It emphasizes leveraging empirical evidence (core to Evidence-Based Management), iteratively testing assumptions, and refining decisions through feedback loops—all key topics highlighted in the category's scope. The discussion is substantial, addressing both the cultural and procedural impacts of experimentation on how teams decide and adapt, rather than a superficial mention. The intended audience (agile teams, organisations focusing on process improvement and innovation) is well-aligned. The content maintains strong focus with minimal off-topic material, yielding a high signal-to-noise ratio. No penalties were necessary, as the practices described are current, empirical, and support the category without contradiction. The confidence score is slightly below perfect because terms like 'decision making' are mentioned more conceptually than explicitly, but overall the fit is extremely strong.",
    "level": "Primary",
    "reasoning_summary": "This content is a strong fit for the Decision Making category. It centres on using hypothesis-driven experimentation and empirical evidence to guide choices in agile settings. The discussion goes beyond surface-level mentions, exploring how structured testing and feedback shape team decisions, which aligns well with the category’s focus on evidence-based, adaptive decision processes."
  },
  "Organisational Agility": {
    "resourceId": "Experimentation",
    "category": "Organisational Agility",
    "calculated_at": "2025-05-06T20:57:10",
    "ai_confidence": 91.5,
    "ai_mentions": 7.6,
    "ai_alignment": 9.7,
    "ai_depth": 8.9,
    "ai_intent": 9.1,
    "ai_audience": 9.3,
    "ai_signal": 9.0,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 92.0,
    "reasoning": "The content clearly addresses core aspects of organisational agility through the lens of experimentation within agile workflows. Direct mentions of 'agile workflows', 'continuous improvement', 'adapt', 'cross-functional teams', and 'organisational culture' establish strong conceptual alignment. The depth is substantial, discussing both practices (hypothesis-driven approaches, empirical evidence) and underlying culture (embracing learning from failure, systemic change). The intent is tightly fitted to the category—emphasizing how organisations can become more adaptable. The target audience appears to be both agile practitioners and organisational leaders, aligning well with the category's typical readership. Signal-to-noise ratio is high; the discussion is focused and relevant. There are no outdated references or contradictory tones present, so no penalties are applied. Final confidence score reflects strong evidence that the content fits the category very well.",
    "level": "Primary",
    "reasoning_summary": "This content is an excellent fit for the category, as it thoroughly explores organisational agility through experimentation in agile workflows. It covers both practical methods and cultural aspects, making it highly relevant for agile practitioners and leaders. The discussion is focused, up-to-date, and directly addresses how organisations can enhance adaptability, ensuring strong alignment with the category’s intent and audience."
  },
  "Backlog Refinement": {
    "resourceId": "Experimentation",
    "category": "Backlog Refinement",
    "calculated_at": "2025-05-06T20:57:10",
    "ai_confidence": 24.1,
    "ai_mentions": 0.2,
    "ai_alignment": 2.2,
    "ai_depth": 2.4,
    "ai_intent": 2.8,
    "ai_audience": 6.3,
    "ai_signal": 8.0,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 24.0,
    "reasoning": "The content focuses exclusively on experimentation in agile workflows, emphasising hypothesis-driven approaches, continuous improvement, and learning from failure. There is no direct mention of backlog refinement, nor are any of its specific themes, techniques, or outcomes discussed (e.g., backlog grooming, prioritisation methods, user stories, or acceptance criteria). While the audience and tone may align with agile practitioners who also practice backlog refinement, the content never addresses or even implies backlog refinement-specific practices. Some conceptual overlap exists regarding continuous improvement and iterative processes, but these are far too general to meaningfully align with 'Backlog Refinement' as a distinct category. The discussion is deep and focused, but entirely on experimentation, not backlog refinement.",
    "level": "Ignored"
  },
  "Scrum Team": {
    "resourceId": "Experimentation",
    "category": "Scrum Team",
    "calculated_at": "2025-05-06T20:57:11",
    "ai_confidence": 31.38,
    "ai_mentions": 0.7,
    "ai_alignment": 3.2,
    "ai_depth": 3.6,
    "ai_intent": 4.3,
    "ai_audience": 4.5,
    "ai_signal": 3.4,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 31.0,
    "reasoning": "The content discusses experimentation as a concept central to agile workflows and teams generally but never directly references the Scrum Team accountability, its structure, or purpose as defined in the Scrum Guide. There are no explicit mentions of the term 'Scrum Team,' nor any breakdown of the distinct accountabilities (Scrum Master, Product Owner, Developers) or the unique features of the Scrum Team. While terms like 'cross-functional teams,' 'iteration,' and 'empirical evidence' are present and tangentially related to Scrum principles, they are not uniquely identifying to Scrum Teams or their accountability per the Scrum Guide. The alignment and depth are moderate, as some practices (empiricism, adaptation) could be performed by Scrum Teams, but the focus is on team experimentation generically across agile contexts, not specifically on the Scrum Team as a unique accountability. The audience seems to be agile team practitioners, which partially overlaps with the Scrum Team target audience, but without specificity. The signal-to-noise ratio is moderate, as all content is relevant for agile teams but not specifically for the Scrum Team as required by the category definition. No penalties are applied, as the content is neither outdated nor critical or satirical regarding Scrum Teams.",
    "level": "Ignored"
  },
  "Agile Strategy": {
    "resourceId": "Experimentation",
    "category": "Agile Strategy",
    "calculated_at": "2025-05-06T20:57:01",
    "ai_confidence": 82.7,
    "ai_mentions": 3.7,
    "ai_alignment": 8.9,
    "ai_depth": 8.4,
    "ai_intent": 7.8,
    "ai_audience": 8.2,
    "ai_signal": 8.5,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 83.0,
    "reasoning": "The content thoroughly discusses hypothesis-driven experimentation as part of agile workflows and links it to organisational adaptability, alignment with customer needs, and strategic improvement. 'Agile workflows' and related concepts are mentioned directly, but the phrase 'Agile Strategy' itself is not explicitly named, resulting in a lower mentions score. There is strong conceptual alignment (discussing empirical learning, continuous improvement, adaptability, and value delivery), directly resonating with key Agile Strategy topics. Depth is high, with nuanced exploration of culture, mindset, organisational learning, and value realization. The intent centers on using experimentation as a foundational practice to drive strategic and cultural change, which fits the category, but does not address all aspects of Agile Strategy (e.g., leadership roles, direct strategic planning implications), tempering the intent score. The audience appears to be both agile practitioners and those in a position to influence strategy, such as managers or team leads, fitting the expected audience. Signal-to-noise ratio is high, as the discussion is focused and free of tangents or off-topic material. No penalty points were applied as there is no indication of outdated or critical/contradictory tone. The final confidence reflects high relevance with some reservations regarding direct category mention and full strategic context.",
    "level": "Primary",
    "reasoning_summary": "This content strongly aligns with the Agile Strategy category, as it explores how hypothesis-driven experimentation supports organisational adaptability and value delivery—core Agile principles. While it doesn’t explicitly use the term ‘Agile Strategy’ or cover every strategic aspect, its focus on empirical learning and cultural change makes it highly relevant for agile practitioners and leaders seeking strategic improvement."
  },
  "Product Validation": {
    "resourceId": "Experimentation",
    "category": "Product Validation",
    "calculated_at": "2025-05-06T20:57:13",
    "ai_confidence": 85.05,
    "ai_mentions": 6.8,
    "ai_alignment": 9.1,
    "ai_depth": 8.7,
    "ai_intent": 8.5,
    "ai_audience": 8.6,
    "ai_signal": 8.4,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 85.0,
    "reasoning": "The content explicitly discusses using hypothesis-driven approaches to test ideas and validate assumptions, which aligns closely with the core meaning of Product Validation. While 'Product Validation' is not repeatedly or directly named, the content refers multiple times to the act of testing, validating, iterative feedback, and evidence-based decision-making. The discussion demonstrates strong conceptual alignment with core practices such as experimentation, empirical testing, and the use of feedback loops, although specific terms like 'user testing,' 'A/B testing,' or 'prototyping' are not used. Depth is substantial, focusing on both the rationale and cultural value behind experimentation, not merely surface definitions. The intent directly supports practitioners interested in embedding validation practices in their workflows, with an audience likely being product teams or agile practitioners. Most sentences remain tightly focused on the relevance and application of experimentation in agile environments, resulting in a high signal-to-noise ratio. No penalties were applied as there are no outdated practices or tonal issues present. The slightly lowered score for direct mentions reflects the absence of repeated or explicit reference to 'Product Validation' as a category, but the overall confidence remains high due to strong practical and conceptual fit.",
    "level": "Primary",
    "reasoning_summary": "This content is a strong fit for the Product Validation category, as it thoroughly explores hypothesis-driven testing, iterative feedback, and evidence-based decision-making. While it doesn’t repeatedly use the term 'Product Validation', its focus on experimentation and validation practices makes it highly relevant for product teams aiming to embed these methods in their workflows."
  },
  "Site Reliability Engineering": {
    "resourceId": "Experimentation",
    "category": "Site Reliability Engineering",
    "calculated_at": "2025-05-06T20:57:01",
    "ai_confidence": 32.89,
    "ai_mentions": 0.3,
    "ai_alignment": 2.6,
    "ai_depth": 2.8,
    "ai_intent": 3.1,
    "ai_audience": 3.6,
    "ai_signal": 3.3,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 33.0,
    "reasoning": "The content does not directly mention Site Reliability Engineering or any of its core practices, tools, or principles. Instead, it centers on experimentation within agile workflows—a concept that is foundational in general software development and agile methodologies, but not specifically aligned with SRE as defined in the classification. There are no references to SRE practices such as incident response, SLOs, monitoring, or reliability-focused automation. The alignment and depth scores reflect that while experimentation is tangentially useful in reliability contexts, the main ideas and audience are more relevant to agile practitioners or product teams rather than SRE. The intent is to promote empirical learning and iterative improvement in product development, not to address reliability, scalability, or operational excellence in production systems. Some minor alignment exists insofar as experimentation can contribute to system improvement, but the content is not targeted at SRE or at those responsible for production reliability. No penalties were warranted since the content is recent, neutral in tone, and not misleading. Overall, the confidence score is low, reflecting only minimal relevance to the SRE category.",
    "level": "Ignored"
  },
  "Company as a Product": {
    "resourceId": "Experimentation",
    "category": "Company as a Product",
    "calculated_at": "2025-05-06T20:57:13",
    "ai_confidence": 71.3,
    "ai_mentions": 1.4,
    "ai_alignment": 8.7,
    "ai_depth": 7.9,
    "ai_intent": 8.3,
    "ai_audience": 8.0,
    "ai_signal": 8.3,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 71.0,
    "reasoning": "The content centers on experimentation within agile workflows, focusing on hypothesis-driven approaches, iterative testing, continuous improvement, and the impact of experimentation on organisational culture. While the text does not directly mention 'Company as a Product,' it strongly aligns with several CaaP key topics: fostering a culture of continuous improvement and experimentation, embedding a scientific approach for organisational evolution, supporting cross-functional collaboration, and aligning with customer needs and organisational goals. The discussion of how experimentation shapes not just product development but also team operation and culture reflects deep conceptual alignment with CaaP principles. However, the lack of explicit reference to CaaP or its naming, and the primary framing around experimentation (rather than fully zoomed out to the 'company as product' lens), modestly limits the direct mentions score and depth. The content is clearly intended for an audience interested in organisational agility, experimentation, and cultural transformation, matching the strategic/leadership audience for CaaP. There are no outdated references or critical tone present. Overall, the content is highly relevant to the spirit of CaaP, though it falls short of top scores due to implicit rather than explicit connection.",
    "level": "Secondary",
    "reasoning_summary": "This content is highly relevant to the 'Company as a Product' category, as it explores experimentation, continuous improvement, and cultural change—core CaaP themes. While it doesn’t explicitly mention CaaP, its focus on agile, hypothesis-driven practices and organisational evolution aligns well with the category’s intent, making it a strong, though not perfect, fit for leaders interested in CaaP principles."
  },
  "Kanban": {
    "resourceId": "Experimentation",
    "category": "Kanban",
    "calculated_at": "2025-05-06T20:57:01",
    "ai_confidence": 31.8,
    "ai_mentions": 0.2,
    "ai_alignment": 2.6,
    "ai_depth": 3.8,
    "ai_intent": 3.1,
    "ai_audience": 5.5,
    "ai_signal": 4.3,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 32.0,
    "reasoning": "The content does not mention Kanban directly and rarely references practices uniquely associated with the Kanban methodology. It discusses experimentation, learning, feedback loops, and continuous improvement—concepts that are found in Kanban but also broadly applicable across agile methodologies. There is moderate thematic overlap (some alignment and depth in continuous improvement and feedback loops), but no discussion of Kanban boards, WIP limits, flow management, or Kanban-specific metrics. The audience is likely adjacent, as Kanban practitioners value experimentation, though the intent and purpose are not tightly focused on Kanban. Consequently, the confidence score is low, reflecting the topic's peripheral connection.",
    "level": "Ignored"
  },
  "Empirical Process Control": {
    "resourceId": "Experimentation",
    "category": "Empirical Process Control",
    "calculated_at": "2025-05-06T20:57:01",
    "ai_confidence": 92.7,
    "ai_mentions": 8.3,
    "ai_alignment": 9.7,
    "ai_depth": 9.4,
    "ai_intent": 9.1,
    "ai_audience": 8.5,
    "ai_signal": 8.8,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 93.0,
    "reasoning": "The content explicitly discusses experimentation within agile workflows, closely aligning with the principles of empirical process control—especially decision-making based on observed evidence and adaptation via feedback loops. While the term 'Empirical Process Control' is not directly mentioned, the concepts of hypothesis-driven validation, iterative testing, and feedback are thoroughly explored, satisfying the key topics such as transparency, inspection, adaptation, and evidence-based improvement. The discussion goes beyond surface-level and outlines both practical application and organizational impact, indicating strong depth. The content is clearly intended for Agile practitioners and teams (audience fit), and remains tightly focused (signal), without unrelated tangents or outdated references. No penalties were necessary as it is accurate, up-to-date, and wholly supportive of empirical process control concepts.",
    "level": "Primary",
    "reasoning_summary": "This content is a great fit for the category, as it thoroughly explores how agile teams use experimentation and feedback to guide decisions—core aspects of empirical process control. Even without naming the concept directly, it covers key principles like transparency, adaptation, and evidence-based improvement, making it highly relevant and practical for Agile practitioners. The discussion is focused, current, and clearly intended for its target audience."
  },
  "Digital Transformation": {
    "resourceId": "Experimentation",
    "category": "Digital Transformation",
    "calculated_at": "2025-05-06T20:57:14",
    "ai_confidence": 67.95,
    "ai_mentions": 2.4,
    "ai_alignment": 7.8,
    "ai_depth": 7.4,
    "ai_intent": 7.2,
    "ai_audience": 8.1,
    "ai_signal": 8.3,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 68.0,
    "reasoning": "The content focuses on hypothesis-driven experimentation within agile workflows, highlighting the importance of empirical testing, embracing uncertainty, and cultivating a learning-oriented culture. These ideas align with Digital Transformation themes—especially fostering innovation, enhancing agility, and changing culture through digital practices. However, the article does not explicitly mention 'digital transformation' or directly discuss the strategic integration or adoption of digital technologies. The main depth resides in organisational culture, learning, and process innovation rather than technological enablement. The audience is likely process owners, agile coaches, or leaders interested in transformation and innovation. The intent and signal are strong, with the content being highly relevant to transformation-minded organisations, but the direct tie to digital technologies is more implied than overt. No penalties were applied, as the piece is current, constructive, and on-topic, but the lack of explicit digital focus and direct mentions reduces the score, resulting in a moderate–high confidence that it fits under the category.",
    "level": "Secondary"
  },
  "Daily Scrum": {
    "resourceId": "Experimentation",
    "category": "Daily Scrum",
    "calculated_at": "2025-05-06T20:57:01",
    "ai_confidence": 9.3,
    "ai_mentions": 0.0,
    "ai_alignment": 1.2,
    "ai_depth": 0.9,
    "ai_intent": 1.0,
    "ai_audience": 2.1,
    "ai_signal": 1.0,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 9.0,
    "reasoning": "The content does not reference the Daily Scrum explicitly or implicitly. It focuses broadly on experimentation, hypothesis testing, and continuous improvement in agile workflows, without any mention or discussion of the Scrum framework, let alone the Daily Scrum event. The main ideas—organizational culture, agile experimentation, learning from failure—are unrelated to the structure, purpose, or execution of the Daily Scrum. While the target audience (agile practitioners, team members) could overlap in a general sense, none of the specific topics, intentions, or practices align with the provided classification definition. The content is entirely off-topic for the 'Daily Scrum' category, resulting in extremely low scores across all dimensions, with the final confidence score reflecting only a negligible, general audience overlap.",
    "level": "Ignored"
  },
  "Value Stream Management": {
    "resourceId": "Experimentation",
    "category": "Value Stream Management",
    "calculated_at": "2025-05-06T20:57:13",
    "ai_confidence": 57.95,
    "ai_mentions": 0.8,
    "ai_alignment": 6.6,
    "ai_depth": 5.4,
    "ai_intent": 6.4,
    "ai_audience": 7.1,
    "ai_signal": 8.2,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 58.0,
    "reasoning": "The content does not explicitly mention 'Value Stream Management' or related terms. Direct mentions are minimal (0.8), as the text consistently focuses instead on experimentation within agile workflows. Conceptual alignment is moderate (6.6); while the text deals with continuous improvement, delivering value, and aligning with organisational goals—concepts relevant to Value Stream Management—it frames them purely in the context of experimentation, not the end-to-end flow of value streams. Depth of discussion (5.4) is mostly limited to experimentation as a cultural and workflow practice, rather than a systemic value stream analysis. Intent (6.4) partially aligns, since the purpose is to inform and encourage practices (like continuous improvement) that are important to Value Stream Management, but it's not the main focus. Audience alignment (7.1) is relatively high, as the readers (agile teams, tech leaders, organisational change agents) overlap with those interested in Value Stream Management, but the framing is slightly more general. Signal-to-noise ratio (8.2) is high, as the content remains focused and devoid of tangents. No penalties are applied as the discussion is current and not critical of the category or its principles. The final confidence score reflects a moderate degree of indirect relevance: the content supports foundational ideas for Value Stream Management, but does not specifically discuss, map, or analyse value streams, nor does it explicitly target the processes and metrics typical of the category.",
    "level": "Tertiary"
  },
  "Lead Time": {
    "resourceId": "Experimentation",
    "category": "Lead Time",
    "calculated_at": "2025-05-06T20:57:03",
    "ai_confidence": 13.45,
    "ai_mentions": 0.3,
    "ai_alignment": 1.2,
    "ai_depth": 0.7,
    "ai_intent": 1.1,
    "ai_audience": 5.3,
    "ai_signal": 6.1,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 13.0,
    "reasoning": "The content focuses entirely on the value of experimentation within agile workflows, specifically hypothesis-driven approaches and empiricism. There are no direct or indirect references to Lead Time, its measurement, improvement, or significance as an observability metric. Instead, the main themes are innovation, learning, and organizational adaptability. The alignment with the Lead Time category is minimal, at most tangential in the sense that experimentation might influence process efficiency generally. There is no discussion of metrics, cycle time, or process optimization related to Lead Time. The audience match is moderate, as both topics are relevant to agile practitioners, and the text is focused and free from filler. No penalties were warranted, as the perspective and information are current and neutral.",
    "level": "Ignored"
  },
  "Lean Product Development": {
    "resourceId": "Experimentation",
    "category": "Lean Product Development",
    "calculated_at": "2025-05-06T20:57:15",
    "ai_confidence": 77.33,
    "ai_mentions": 2.9,
    "ai_alignment": 8.2,
    "ai_depth": 7.9,
    "ai_intent": 8.5,
    "ai_audience": 8.0,
    "ai_signal": 7.7,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 77.0,
    "reasoning": "The content focuses on experimentation in agile workflows, strongly emphasizing hypothesis-driven testing, learning from failure, and continuous improvement—all of which conceptually align with Lean Product Development's core principles. However, the explicit mention of 'Lean Product Development' or its exact terminology is missing; the discussion leans more toward agile and experimentation than Lean itself, resulting in a relatively low 'Direct Mentions' score. Depth is high, as the piece deeply explores experimentation and its organizational effects, but it does not discuss Lean-specific frameworks or waste reduction tools such as Value Stream Mapping. The intent effectively promotes a culture and processes harmonious with Lean, and the audience is well-aligned (teams involved in improving product development practices). Signal-to-noise is strong, with the vast majority of the content directly relevant, though some focus is generalized across agile rather than Lean proper. No penalties are applied, as the content is current and supportive. The confidence score reflects the strong conceptual and practical overlap with Lean Product Development, offset by the lack of direct reference or explicit Lean frameworks.",
    "level": "Secondary",
    "reasoning_summary": "This content aligns well with Lean Product Development in spirit, as it highlights experimentation, learning from failure, and continuous improvement—key Lean principles. However, it doesn’t directly reference Lean terminology or specific frameworks, focusing more on agile and general experimentation. While the depth and relevance are strong, the lack of explicit Lean context means it only partially fits the category."
  },
  "One Engineering System": {
    "resourceId": "Experimentation",
    "category": "One Engineering System",
    "calculated_at": "2025-05-06T20:57:05",
    "ai_confidence": 32.1,
    "ai_mentions": 0.2,
    "ai_alignment": 3.7,
    "ai_depth": 3.1,
    "ai_intent": 3.5,
    "ai_audience": 6.1,
    "ai_signal": 7.2,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 32.0,
    "reasoning": "The content describes hypothesis-driven experimentation within agile workflows, focusing on scientific decision-making, continuous improvement, and cultivating a learning culture within organizations. While these themes are broadly compatible with engineering best practices, the article does not mention the One Engineering System (1ES) by name or discuss its specific framework, integration of tools, or standardization across teams. It avoids off-topic digressions and targets a technical audience, but only aligns minimally with the 1ES category because it focuses on a generic agile experimentation practice rather than unified engineering systems. The lack of direct 1ES references and only tangential conceptual overlap led to low scores, especially in direct mentions, alignment, depth, and intent, while audience and signal were higher given the technical orientation and focus.",
    "level": "Ignored"
  },
  "Sensemaking": {
    "resourceId": "Experimentation",
    "category": "Sensemaking",
    "calculated_at": "2025-05-06T20:57:05",
    "ai_confidence": 56.35,
    "ai_mentions": 2.4,
    "ai_alignment": 6.8,
    "ai_depth": 6.6,
    "ai_intent": 6.4,
    "ai_audience": 6.2,
    "ai_signal": 7.2,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 56.0,
    "reasoning": "The content on 'Experimentation' discusses the importance of hypothesis-driven testing and iteratively validating assumptions within agile workflows. While these practices are related to sensemaking—particularly in their focus on navigating uncertainty and making informed decisions—the content does not explicitly reference sensemaking or its frameworks (e.g., Cynefin), nor does it deeply explore sensemaking concepts like the dynamics of complexity, leadership roles, or collective interpretation in organizational contexts. Conceptual alignment is moderate; experimentation is certainly a tool used for sensemaking, but the article frames it primarily as a means for validating ideas, fostering innovation, and supporting organizational adaptability. The depth of discussion goes beyond the surface, highlighting cultural and systemic benefits of experimentation, but it falls short of a thorough analysis from the lens of sensemaking. The intended audience (agile practitioners and organizational leaders) aligns reasonably well, and the content is focused without significant tangents or off-topic material. No penalties are necessary; there are no outdated practices or negative tone. Overall, the confidence reflects that experimentation can be a component of sensemaking, but this content does not directly or deeply anchor itself within the sensemaking discourse.",
    "level": "Tertiary"
  },
  "Team Performance": {
    "resourceId": "Experimentation",
    "category": "Team Performance",
    "calculated_at": "2025-05-06T20:57:17",
    "ai_confidence": 88.27,
    "ai_mentions": 7.4,
    "ai_alignment": 9.3,
    "ai_depth": 8.7,
    "ai_intent": 8.2,
    "ai_audience": 8.5,
    "ai_signal": 8.9,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 88.0,
    "reasoning": "The content centers on experimentation as a team practice within agile workflows, focusing on hypothesis-driven testing and its impact on team capability and delivery. It directly discusses teams ('empowers teams', 'cross-functional teams'), and frames experimentation as foundational to sustainable value delivery and continuous improvement—core themes of Team Performance. The main ideas are highly aligned: emphasis on system-level behaviours, delivery, and adaptive learning. While the term 'team performance' is not explicitly mentioned often, the concepts are deeply integrated with its meaning. The discussion is thorough, covering organisational, cultural and process aspects, but could discuss measurement and systemic metrics more directly to reach full depth. The target audience seems to be practitioners, managers, and leaders within agile teams, matching the category well. The content is focused and contains minimal off-topic material. No penalties apply, as there is no sign of outdatedness or contradictory tone. The confidence score appropriately reflects strong but not absolute alignment, especially given a lack of explicit metric references.",
    "level": "Primary",
    "reasoning_summary": "This content is a strong fit for the Team Performance category, as it explores how experimentation within agile teams drives capability, delivery, and continuous improvement. It addresses team dynamics, organisational culture, and adaptive learning, making it highly relevant for practitioners and leaders. While it could discuss measurement more, its focus on team practices and outcomes aligns closely with the category’s core themes."
  },
  "Platform Engineering": {
    "resourceId": "Experimentation",
    "category": "Platform Engineering",
    "calculated_at": "2025-05-06T20:57:05",
    "ai_confidence": 13.55,
    "ai_mentions": 0.25,
    "ai_alignment": 1.5,
    "ai_depth": 1.7,
    "ai_intent": 2.2,
    "ai_audience": 2.1,
    "ai_signal": 3.8,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 14.0,
    "reasoning": "The content does not explicitly mention Platform Engineering or any of its distinct key topics, such as Internal Developer Platforms, developer self-service, standardisation, or automation in the application lifecycle. The main focus is on experimentation within agile workflows—a concept that aligns with general software development and innovation rather than the specific domain of Platform Engineering. Depth of discussion is limited to generic, high-level benefits of experimentation in teams, with no deep ties to platform engineering principles or practices. The intent is broadly supportive of organisational learning and product development but is not targeted specifically at a platform engineering audience. The signal-to-noise ratio is relatively higher as the content is focused and coherent within its topic, but the actual relevance to Platform Engineering is very low, resulting in a low overall confidence.",
    "level": "Ignored"
  },
  "Customer Feedback Loops": {
    "resourceId": "Experimentation",
    "category": "Customer Feedback Loops",
    "calculated_at": "2025-05-06T20:57:05",
    "ai_confidence": 66.82,
    "ai_mentions": 2.6,
    "ai_alignment": 7.7,
    "ai_depth": 7.5,
    "ai_intent": 7.1,
    "ai_audience": 9.0,
    "ai_signal": 8.4,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 67.0,
    "reasoning": "The content on 'Experimentation' discusses hypothesis-driven approaches and iterative testing within agile workflows, frequently referencing feedback loops as a mechanism for learning and continuous improvement. Although 'Customer Feedback Loops' are not named directly, the description includes relevant concepts such as learning from feedback, aligning with customer needs, and the cyclical refinement of products and processes. Depth is strong due to detailed exploration of how experimentation impacts organizational culture and product development, though explicit techniques for customer feedback collection are not described. The intent aligns well, emphasizing learning and adaptation based on evidence—which includes, though does not exclusively focus on, customer input. Audience is highly relevant; the content targets agile practitioners and teams working on iterative product development. The signal-to-noise ratio remains high, as most content relates directly to practical aspects of experimentation and feedback. No penalties apply, as the content is current and adopts an informative tone. The calculated confidence score reflects moderate-to-strong evidence that this content fits within the 'Customer Feedback Loops' category, though it could be higher if customer feedback mechanisms were central rather than implied.",
    "level": "Secondary"
  },
  "Estimation": {
    "resourceId": "Experimentation",
    "category": "Estimation",
    "calculated_at": "2025-05-06T20:57:06",
    "ai_confidence": 21.05,
    "ai_mentions": 0.1,
    "ai_alignment": 2.2,
    "ai_depth": 1.6,
    "ai_intent": 3.0,
    "ai_audience": 7.2,
    "ai_signal": 5.0,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 21.0,
    "reasoning": "The content focuses entirely on experimentation in agile workflows, emphasizing hypothesis-driven validation, empirical evidence, and cultivating a learning culture. However, it does not mention estimation or directly reference any estimation practices, techniques, or concepts such as Planning Poker, T-shirt sizing, or velocity. There are indirect thematic overlaps in valuing empirical data and learning from uncertainty, but the main thrust is experimentation, not estimation. Depth and conceptual alignment are limited because estimation is not explored beyond a tangential proximity to empirical thinking. The intent is tangential, as it supports agile improvement but not estimation specifically. The audience (agile practitioners) overlaps with the estimation category, and the content is focused on agile-related value, warranting moderate signal and audience scores. No penalties are applied since the content is current and appropriately framed.",
    "level": "Ignored"
  },
  "Scaling": {
    "resourceId": "Experimentation",
    "category": "Scaling",
    "calculated_at": "2025-05-06T20:57:08",
    "ai_confidence": 36.85,
    "ai_mentions": 0.15,
    "ai_alignment": 3.9,
    "ai_depth": 4.1,
    "ai_intent": 4.0,
    "ai_audience": 6.25,
    "ai_signal": 6.8,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 37.0,
    "reasoning": "The content focuses on hypothesis-driven experimentation within agile workflows. There are no direct mentions of scaling, or of specific frameworks (SAFe, LeSS, Nexus), nor are the unique challenges of coordinating multiple teams or scaling agile practices addressed. Conceptually, while there is some indirect alignment—such as encouraging cross-functional collaboration and fostering organisational change—the discussion does not engage with scaling-specific methodologies, dependency management, or enterprise alignment at scale. The depth is fair for experimentation in general but does not extend into scaling topics. The intent seems more about supporting team-level agile processes, not enterprise-level scaling. The audience could have some overlap (agile practitioners who may scale up), but the content itself remains focused on team-level practice rather than strategies for large, multi-team environments. The signal-to-noise ratio is fairly high because the content is focused on its topic, but that topic is largely orthogonal to scaling. There were no outdated references or negative framings, so no penalties were applied. The resulting confidence score is proportionally low, reflecting only marginal and indirect relevance to the 'Scaling' category.",
    "level": "Ignored"
  },
  "Technical Excellence": {
    "resourceId": "Experimentation",
    "category": "Technical Excellence",
    "calculated_at": "2025-05-06T20:57:08",
    "ai_confidence": 56.85,
    "ai_mentions": 2.7,
    "ai_alignment": 6.2,
    "ai_depth": 6.6,
    "ai_intent": 5.8,
    "ai_audience": 6.9,
    "ai_signal": 5.1,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 57.0,
    "reasoning": "The content centers on experimentation as a critical practice in agile workflows, emphasizing hypothesis-driven approaches, empirical decision-making, and iterative testing. However, it does not explicitly cite or closely discuss the defining engineering practices of Technical Excellence, such as TDD, CI/CD, modular architecture, or emergent design. The discussion is conceptually adjacent to technical excellence—it values continuous improvement and scientific methodology, which can underpin technical practices—but the main thrust is fostering an experimental mindset, adaptability, and organizational learning. Direct references to 'technical excellence' or the specific engineering principles from the definition are absent, so the Direct Mentions score is low. Conceptual Alignment and Depth are moderate because there's some thematic overlap, especially regarding continuous improvement and system refinement, but key details on high-level engineering practices are missing. The target audience seems technical, but the scope is broader, encompassing team culture and organizational strategy, not just technical practitioners. Some degree of noise exists, as the content leans heavily on experimentation's impact on learning, innovation, and team motivation, which, while valuable, are not central to Technical Excellence as narrowly defined. No penalty deductions were warranted, as the tone is positive and all suggestions are contemporary.",
    "level": "Tertiary"
  },
  "Behaviour Driven Development": {
    "resourceId": "Experimentation",
    "category": "Behaviour Driven Development",
    "calculated_at": "2025-05-06T20:57:08",
    "ai_confidence": 21.43,
    "ai_mentions": 0.2,
    "ai_alignment": 2.3,
    "ai_depth": 2.0,
    "ai_intent": 2.1,
    "ai_audience": 5.1,
    "ai_signal": 3.4,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 21.0,
    "reasoning": "The content focuses entirely on experimentation and hypothesis-driven approaches within agile workflows. There are no direct or indirect mentions of Behaviour Driven Development (BDD), its terminology, principles, tools, or practices. The core concepts—such as collaboration between developers and stakeholders, user story writing, or BDD’s impact on requirements—are not discussed. The main intent is on promoting a general experimentation mindset in agile settings, which is not specific to BDD. The intended audience is generic to agile practitioners, not targeting those specifically interested in BDD methodologies. The overall signal-to-noise ratio is moderate; the content is relevant to agile and experimentation but does not intersect with BDD enough to justify a higher confidence. No penalties were applied, as the tone and context are current and neutral.",
    "level": "Ignored"
  },
  "Coaching": {
    "resourceId": "Experimentation",
    "category": "Coaching",
    "calculated_at": "2025-05-06T20:57:08",
    "ai_confidence": 61.17,
    "ai_mentions": 1.3,
    "ai_alignment": 7.1,
    "ai_depth": 6.8,
    "ai_intent": 7.3,
    "ai_audience": 7.7,
    "ai_signal": 8.2,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 61.0,
    "reasoning": "The content does not mention the term 'Coaching' directly, nor does it explicitly reference coaching techniques, frameworks, or the coach role; thus, the direct mentions score is very low. However, the core conceptual message—using experimentation, hypothesis-driven approach, fostering learning, psychological safety, and team collaboration—aligns moderately with the underlying philosophy of coaching, especially regarding fostering growth, learning, and a growth mindset. The depth of discussion is relatively strong in delineating the benefits and processes of experimentation, but it lacks depth on coaching practices specifically. The intent is mostly aligned with organizational and team learning/improvement, which overlaps with the outcomes sought in coaching, but coaching itself isn't highlighted as the mechanism. The audience appears to be agile practitioners and teams, which matches but could also be broader. The content is focused and on-topic regarding agile/learning practices, so the signal-to-noise ratio is high. No penalties are applied as the content is current and respectfully presented. The confidence score is moderate, reflecting the indirect but conceptual overlap between experimentation-as-learning and coaching, but the absence of explicit coaching focus prevents higher confidence.",
    "level": "Secondary"
  },
  "Trend Analysis": {
    "resourceId": "Experimentation",
    "category": "Trend Analysis",
    "calculated_at": "2025-05-06T20:57:09",
    "ai_confidence": 36.8,
    "ai_mentions": 1.3,
    "ai_alignment": 4.9,
    "ai_depth": 5.2,
    "ai_intent": 4.5,
    "ai_audience": 7.0,
    "ai_signal": 6.3,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 37.0,
    "reasoning": "The content focuses on experimentation within agile workflows, highlighting hypothesis-driven approaches and organisational benefits. While it discusses empirical practices influential in Agile, it does not directly mention 'trend analysis' or discuss the identification or impact of specific trends within Agile, DevOps, or business agility. Direct Mentions are very low as the term and category are never referenced. Conceptual Alignment and Depth are moderate since experimentation is a practice relevant to identifying patterns but is not explicitly tied to trend monitoring or analysis. Intent is somewhat aligned, aiming to inform teams and leaders, but its purpose is to advocate for experimentation rather than analyze trends. The audience is practitioners and leaders in Agile, which slightly overlaps with Trend Analysis targets. Signal-to-noise is fair—the content is largely relevant but not on the core Trend Analysis topic. No penalties are warranted, as the content is neither outdated nor in contradiction with the category. The overall confidence reflects the low degree of conceptual and explicit overlap with Trend Analysis.",
    "level": "Ignored"
  },
  "Agile Frameworks": {
    "resourceId": "Experimentation",
    "category": "Agile Frameworks",
    "calculated_at": "2025-05-06T20:57:11",
    "ai_confidence": 64.05,
    "ai_mentions": 3.2,
    "ai_alignment": 7.8,
    "ai_depth": 7.3,
    "ai_intent": 7.0,
    "ai_audience": 7.5,
    "ai_signal": 6.8,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 64.0,
    "reasoning": "The content focuses on experimentation as a practice within agile workflows, emphasizing hypothesis-driven approaches, empirical validation, feedback loops, and continuous improvement—all highly relevant to Agile philosophies. However, it does not directly mention any specific Agile frameworks (e.g., Scrum, Kanban, XP) or the Agile Manifesto, nor does it explicitly compare or analyze frameworks. Depth is moderate as it explains the value and organizational impact of experimentation in agile but does not deeply tie to particular frameworks. The main intent aligns with fostering agility and a learning culture, targeting an audience familiar with agile teams and practices (e.g., practitioners, leaders). There is a high degree of relevance and focus, but lack of direct framework naming and limited comparative analysis constrain the Direct Mentions and Signal-to-Noise scores. No penalties were applied as the content is current and supportive of the category framing.",
    "level": "Secondary"
  },
  "Competence": {
    "resourceId": "Experimentation",
    "category": "Competence",
    "calculated_at": "2025-05-06T20:57:01",
    "ai_confidence": 43.55,
    "ai_mentions": 1.4,
    "ai_alignment": 5.3,
    "ai_depth": 4.8,
    "ai_intent": 5.7,
    "ai_audience": 6.3,
    "ai_signal": 7.2,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 44.0,
    "reasoning": "The content focuses on hypothesis-driven experimentation in agile workflows, highlighting continuous improvement, learning, and adaptability. These are tangentially related to competence, as developing and adapting skills play a role in such contexts. However, the text centers on the process of experimentation itself, rather than directly discussing 'competence' as its main principle or theme. There are no explicit or frequent mentions of competence, nor a thorough exploration of building or measuring professional capability; the emphasis is on organizational learning, adaptation, and product development rather than skill development, mastery, or professionalism. The audience is likely practitioners and agile teams, partially overlapping with competence-focused groups, but the primary purpose is to advocate for experimentation, not competence per se. The content is highly relevant to agile but only moderately aligned with the category of competence.",
    "level": "Tertiary"
  },
  "Organisational Change": {
    "resourceId": "Experimentation",
    "category": "Organisational Change",
    "calculated_at": "2025-05-06T20:57:01",
    "ai_confidence": 85.73,
    "ai_mentions": 4.6,
    "ai_alignment": 9.0,
    "ai_depth": 8.7,
    "ai_intent": 8.3,
    "ai_audience": 8.6,
    "ai_signal": 8.7,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 86.0,
    "reasoning": "The content discusses experimentation as a practice central to agile workflows, emphasizing hypothesis-driven approaches, iterative learning, and embedding scientific methods into organisational culture. While 'organisational change' is not directly named, the piece is conceptually well-aligned: it focuses on adaptive approaches, building learning cultures, and fostering resilience—core elements of effective organisational change. The discussion goes beyond generic mentions, delving into cultural shifts, value delivery, and systemic change. While there are no explicit frameworks like ADKAR or Kotter referenced, the connection between experimentation and organisational adaptation is strongly articulated, as is the benefit for teams and organisations. The intended audience appears to be change agents, agile leaders, and transformation strategists, rather than technical practitioners only. The content is focused and has a high signal-to-noise ratio, staying on the theme throughout. The confidence score reflects the strong alignment and depth, with a small deduction in 'mentions' since explicit references to 'organisational change' are low, but the core definition is otherwise thoroughly addressed.",
    "level": "Primary",
    "reasoning_summary": "This content is a strong fit for the 'organisational change' category. It explores how experimentation underpins adaptive, learning-focused cultures—key to successful change initiatives. While it doesn’t name specific frameworks, it clearly targets change leaders and delves into cultural and systemic transformation, making its alignment with organisational change both relevant and well-supported."
  },
  "Frequent Releases": {
    "resourceId": "Experimentation",
    "category": "Frequent Releases",
    "calculated_at": "2025-05-06T20:57:12",
    "ai_confidence": 37.4,
    "ai_mentions": 0.7,
    "ai_alignment": 4.3,
    "ai_depth": 4.1,
    "ai_intent": 5.3,
    "ai_audience": 8.5,
    "ai_signal": 6.1,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 37.0,
    "reasoning": "The content centers on hypothesis-driven experimentation within agile workflows but does not reference Frequent Releases, Continuous Delivery, or related terminology directly. While some conceptual overlap exists—such as iterative improvement and feedback loops—the main ideas focus on learning, adaptability, and organisational change via scientific methods, rather than the delivery or release of software updates. The depth and intent align partially, as experimentation is a supporting practice for continuous improvement, which may underpin frequent releases, but these connections are indirect and not substantively explored. The audience appears technical and change-oriented, matching the target demographic of Frequent Releases discussions. The signal-to-noise ratio is moderately strong; most content is focused, but not on the release aspect. No penalties are applied as the content is current and maintains a positive, aligned tone. The final confidence score is low-to-moderate, reflecting the partial conceptual overlap but clear absence of direct or in-depth engagement with the Frequent Releases category.",
    "level": "Ignored"
  },
  "Modern Source Control": {
    "resourceId": "Experimentation",
    "category": "Modern Source Control",
    "calculated_at": "2025-05-06T20:57:11",
    "ai_confidence": 11.2,
    "ai_mentions": 0.2,
    "ai_alignment": 1.6,
    "ai_depth": 1.0,
    "ai_intent": 2.1,
    "ai_audience": 2.0,
    "ai_signal": 2.3,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 11.0,
    "reasoning": "The content focuses exclusively on experimentation within agile workflows, discussing hypothesis-driven approaches, cultural change, feedback loops, and empirical testing. There is no explicit mention or reference to version control systems, branching, code review, collaboration workflows, or any other core topic of Modern Source Control. The conceptual alignment is weak since experimentation in agile is at best tangential to source control practices, and the depth of discussion further confirms that the main themes remain general to software development culture, not version control. The intent and audience are somewhat relevant, as technical practitioners interested in process improvements could overlap, but the signal-to-noise ratio is low due to the absence of source control-specific content. No penalties were applied as the content is contemporary and not critical in tone, but overall, the evidence justifies a very low confidence score for this classification.",
    "level": "Ignored"
  },
  "Product Discovery": {
    "resourceId": "Experimentation",
    "category": "Product Discovery",
    "calculated_at": "2025-05-06T20:57:01",
    "ai_confidence": 86.9,
    "ai_mentions": 3.4,
    "ai_alignment": 9.0,
    "ai_depth": 9.3,
    "ai_intent": 8.9,
    "ai_audience": 8.4,
    "ai_signal": 8.7,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 87.0,
    "reasoning": "The content centers on experimentation as a hypothesis-driven approach to testing ideas and validating assumptions, directly aligned with Product Discovery's focus on validating product ideas and building an evidence-based understanding of user needs. The discussion is substantive, exploring not only the practice but also its impact on culture, learning, and cross-functional collaboration—key aspects of Product Discovery. While terms like 'Product Discovery' are not mentioned verbatim (hence a moderate mentions score), the conceptual alignment, depth of discussion, and intent are all strong, targeting practitioners interested in agile workflows and evidence-based product development. The audience fits well (teams engaged in product development and discovery), and the content maintains a high signal with nearly all discussion focused on experimentation's role in innovation and adaptation within product teams. No outdated or contradictory elements are observed, so no penalties are applied. The calculated confidence score of 86.9 reflects high but not perfect alignment due to the absence of direct category terminology and the possibility that some content might generalize experimentation beyond strictly discovery-phase activities.",
    "level": "Primary",
    "reasoning_summary": "This content is a strong fit for the Product Discovery category, as it thoroughly explores experimentation as a method for validating ideas and assumptions—core to discovery practices. While it doesn’t use the exact term, its focus on evidence-based learning, team collaboration, and agile workflows clearly targets those involved in product development and discovery. The discussion is relevant, current, and highly applicable to the intended audience."
  },
  "Organisational Psychology": {
    "resourceId": "Experimentation",
    "category": "Organisational Psychology",
    "calculated_at": "2025-05-06T20:57:01",
    "ai_confidence": 51.42,
    "ai_mentions": 1.1,
    "ai_alignment": 5.7,
    "ai_depth": 5.9,
    "ai_intent": 4.6,
    "ai_audience": 7.4,
    "ai_signal": 6.9,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 51.0,
    "reasoning": "The content focuses on 'experimenting' and 'hypothesis-driven approaches' in agile workflows, with emphasis on empirical decision-making, adaptability, and fostering a culture of learning. There are indirect nods to organisational culture, team engagement, and motivation, which overlap with organisational psychology. However, explicit references to psychological theories, leadership, group dynamics, or motivation models are absent, resulting in a low score for direct mentions. The alignment score is moderate because while the content partially overlaps with audience interests in organisational psychology, the primary frame is on workflow improvement, not psychological constructs per se. The depth is above mid-level: concepts such as psychological safety and culture are implied, but not explored in detail. Intent is somewhat weak as the main goal is to promote agile experimentation rather than to discuss psychological influences. The audience score is high due to the appeal to organisational practitioners, but it's not targeted at psychologists specifically. Signal-to-noise is solid, as the content is focused and relevant for change agents. The overall confidence reflects moderate conceptual alignment but insufficient explicit psychological depth for a high classification.",
    "level": "Tertiary"
  },
  "Personal": {
    "resourceId": "Experimentation",
    "category": "Personal",
    "calculated_at": "2025-05-06T20:57:02",
    "ai_confidence": 36.54,
    "ai_mentions": 0.25,
    "ai_alignment": 3.85,
    "ai_depth": 4.1,
    "ai_intent": 4.6,
    "ai_audience": 5.05,
    "ai_signal": 6.08,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 37.0,
    "reasoning": "The content focuses on the general concept of experimentation within agile workflows, emphasizing its importance for teams and organizations. There is no direct mention of personal experiences, anecdotes, or individual reflections, making the 'Direct Mentions' score very low. While the text is conceptually adjacent to the 'Personal' category (touching on team mindset and culture), it stays at an organizational and theoretical level rather than offering subjective or individual perspectives. The depth of discussion is moderate, but focuses on the value and mechanisms of experimentation broadly—not through a personal or anecdotal lens. The intent appears to be informative for a general agile audience (possibly practitioners or leaders), not specifically intended for personal storytelling or sharing unique insights. The content remains tightly focused (hence a higher 'signal' score), but audience and alignment scores are middling because the fit with 'Personal' is weak. No penalties are applied as there are no obsolete references or oppositional tone. The overall confidence is therefore low, proportionate to the minimal evidence this content fits the 'Personal' category.",
    "level": "Ignored"
  },
  "DevOps": {
    "resourceId": "Experimentation",
    "category": "DevOps",
    "calculated_at": "2025-05-06T20:57:05",
    "ai_confidence": 66.67,
    "ai_mentions": 1.2,
    "ai_alignment": 6.5,
    "ai_depth": 7.0,
    "ai_intent": 7.2,
    "ai_audience": 8.1,
    "ai_signal": 8.5,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 67.0,
    "reasoning": "The content describes experimentation in agile workflows and focuses on hypothesis-driven approaches, iterative testing, feedback loops, and fostering a culture of continuous improvement—all concepts that relate conceptually to DevOps. However, it never mentions 'DevOps' directly or explicitly connects its points to DevOps practices or philosophy, resulting in a very low 'Direct Mentions' score. The alignment is moderately high because continuous improvement, feedback loops, and breaking silos are key principles in DevOps, but the main framing is around agile workflows and organisational culture change. The depth is good, with substantial exploration of experimentation and its impact, but it remains agnostic of DevOps as a distinct philosophy. Intent is aligned toward supporting practices shared by a DevOps audience, but it is not explicitly DevOps-targeted. The audience match is fairly strong, as both agile/DevOps practitioners and strategists would benefit, though it's not solely technical or DevOps-centric. Lastly, the content is focused, conceptually coherent, and contains little off-topic material, yielding a high signal-to-noise score. No penalties are applied, as the content is current and its tone is supportive. Overall, the weighted confidence reflects indirect relevance to DevOps through shared principles rather than explicit treatment.",
    "level": "Secondary"
  },
  "Unrealised Value": {
    "resourceId": "Experimentation",
    "category": "Unrealised Value",
    "calculated_at": "2025-05-06T20:57:05",
    "ai_confidence": 53.37,
    "ai_mentions": 0.9,
    "ai_alignment": 6.1,
    "ai_depth": 5.7,
    "ai_intent": 6.6,
    "ai_audience": 7.2,
    "ai_signal": 7.4,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 53.0,
    "reasoning": "The content focuses on the importance of experimentation within agile workflows, emphasizing hypothesis-driven approaches, iterative testing, and a culture of continuous improvement and innovation. This aligns with the general principle of seeking improvements and innovation potential, which is conceptually adjacent to Unrealised Value as defined within Evidence-Based Management. However, the text never explicitly mentions 'Unrealised Value,' nor does it frame experimentation specifically as a means to identify or measure untapped opportunities, latent demand, or value gaps. Depth is moderate; while the content discusses how experimentation can drive value and adaptability, it does not directly address the indicators, frameworks, or strategic exploration of Unrealised Value. The main intent is to promote experimentation as a practice, not to guide decision-making around Realised versus Unrealised Value. The audience appears to be practitioners and leaders interested in agile and innovation, somewhat aligned with those concerned with Unrealised Value. The discussion is focused but somewhat broad, touching on experimentation's benefits without anchoring it to the core theme of the category. No penalties are applied because the content is current, positive in tone, and not critical or outdated. The confidence score reflects the moderate but indirect fit with the Unrealised Value classification.",
    "level": "Tertiary"
  },
  "Internal Developer Platform": {
    "resourceId": "Experimentation",
    "category": "Internal Developer Platform",
    "calculated_at": "2025-05-06T20:57:05",
    "ai_confidence": 14.1,
    "ai_mentions": 0.2,
    "ai_alignment": 2.6,
    "ai_depth": 2.4,
    "ai_intent": 2.9,
    "ai_audience": 2.8,
    "ai_signal": 2.5,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 14.0,
    "reasoning": "The content discusses experimentation within agile workflows, focusing on hypothesis-driven approaches and fostering a culture of empirical learning and innovation. However, there are no direct mentions or references to Internal Developer Platforms (IDP), nor any detailed exploration of their components, advantages, or role in software engineering. The conceptual alignment is minimal: while the content addresses iterative improvement and collaboration—concepts that IDPs support—the discussion lacks any connective tissue to IDP frameworks, automation, or streamlining processes specific to the category. Depth and intent remain generalized, primarily geared toward agile practitioners rather than those interested specifically in Internal Developer Platforms. The audience is broadly technical, but not tailored to those focused on IDPs. The signal-to-noise ratio is low: all content is relevant to experimentation in agile but entirely off-focus from the IDP category. No penalties were necessary, as the tone is positive and the material is contemporary, but the absence of substantial overlap justifies the very low confidence score.",
    "level": "Ignored"
  },
  "Shift Left Strategy": {
    "resourceId": "Experimentation",
    "category": "Shift Left Strategy",
    "calculated_at": "2025-05-06T20:57:05",
    "ai_confidence": 18.67,
    "ai_mentions": 0.2,
    "ai_alignment": 2.5,
    "ai_depth": 2.8,
    "ai_intent": 2.1,
    "ai_audience": 6.2,
    "ai_signal": 7.1,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 19.0,
    "reasoning": "The content on 'Experimentation' presents a strong focus on hypothesis-driven approaches and the value of empirical testing within agile workflows. While it discusses iterative testing, feedback loops, and continuous improvement, it does not directly reference or explicitly focus on the Shift-Left Strategy as defined—there's no mention of integrating testing, security, or compliance earlier in development or any consideration of Shift-Left methodologies or metrics. The themes generally align with a learning and improvement culture, which is synergistic but not specific to Shift-Left itself. The depth of discussion is solid regarding experimentation but only tangentially relevant to the explicit principles, tools, or techniques of Shift-Left Strategy. The primary audience (technical practitioners, agile teams) overlaps with Shift-Left's, and the focus is on actionable process improvements. However, the signal-to-noise ratio is high as the content remains focused, but is overall not targeted enough to the category. No penalties applied, as the content is current and does not contradict the framing.",
    "level": "Ignored"
  },
  "Portfolio Management": {
    "resourceId": "Experimentation",
    "category": "Portfolio Management",
    "calculated_at": "2025-05-06T20:57:05",
    "ai_confidence": 32.65,
    "ai_mentions": 0.4,
    "ai_alignment": 3.6,
    "ai_depth": 3.7,
    "ai_intent": 2.8,
    "ai_audience": 3.7,
    "ai_signal": 3.9,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 33.0,
    "reasoning": "The content focuses on hypothesis-driven experimentation as a practice within agile workflows, primarily centering on team and product/process improvement. There are no direct mentions of portfolio management or its key terms, and all references are at the level of teams, products, and iterative learning rather than portfolio-level coordination or strategic alignment. Conceptual alignment is weak: while the theme of aligning efforts with organisational goals is touched on, the discussion does not connect experimentation to prioritisation, investment decisions, value stream mapping, or any portfolio-centric concerns. The depth is moderate, providing reasons for experimentation's value but remaining at a general level and not exploring portfolio governance, metrics, or frameworks. Intent is focused on supporting agile practices at the team or cross-functional level, rather than informing portfolio management. Audience appears to be practitioners or team leads, not executives or strategists. Most content is signal rather than noise, but it's not relevant to portfolio management directly—just adjacent. No penalties were necessary, as the tone is constructive and current. The confidence score is low, proportionate to the indirect relevance and lack of portfolio management focus.",
    "level": "Ignored"
  },
  "Application Lifecycle Management": {
    "resourceId": "Experimentation",
    "category": "Application Lifecycle Management",
    "calculated_at": "2025-05-06T20:57:08",
    "ai_confidence": 38.5,
    "ai_mentions": 1.4,
    "ai_alignment": 4.9,
    "ai_depth": 4.3,
    "ai_intent": 6.1,
    "ai_audience": 6.5,
    "ai_signal": 7.2,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 38.0,
    "reasoning": "The content focuses heavily on hypothesis-driven experimentation within agile workflows. While experimentation and continuous improvement are important practices in modern software development, the content does not directly mention Application Lifecycle Management (ALM), nor does it explicitly address ALM core topics (such as governance, lifecycle stages, tools, or change management). 'Experimentation' as described pertains more to product development and agile team culture than to the processes and practices governing the full application lifecycle. The intent of the piece is to promote empirical, iterative learning, which is tangentially relevant to ALM but not a central theme. The audience (teams engaged in agile/product development) overlaps somewhat with ALM practitioners, but alignment is not precise. The discussion is in-depth about experimentation—but not about ALM. The signal-to-noise ratio remains high, as all content is on experimentation and agile, but is narrowly scoped. No penalties applied, as content is contemporary and tone is neutral. Overall, confidence is moderate but limited by the lack of direct ALM focus.",
    "level": "Ignored"
  },
  "Agile Product Management": {
    "resourceId": "Experimentation",
    "category": "Agile Product Management",
    "calculated_at": "2025-05-06T20:57:10",
    "ai_confidence": 84.05,
    "ai_mentions": 3.6,
    "ai_alignment": 8.9,
    "ai_depth": 8.7,
    "ai_intent": 8.6,
    "ai_audience": 8.3,
    "ai_signal": 8.8,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 84.0,
    "reasoning": "The content focuses on the use of hypothesis-driven experimentation within agile workflows, emphasizing the value of empirical validation, feedback loops, and alignment with customer needs—central concepts in Agile Product Management. It discusses how experimentation supports continuous improvement, cross-functional team collaboration, and the delivery of innovative products in response to market demands. While the content doesn't explicitly mention 'Agile Product Management' or specific roles (such as Product Owner), its conceptual alignment, depth of discussion, and clear purpose to inform practitioners are strong. Its language is aimed at teams and organizations involved in agile product development, matching the intended audience. Mentions are not fully direct to the category but are consistently thematically accurate. There are no signs of outdated or contradictory content, nor is the discussion diluted or off-topic. Therefore, the confidence is high, though not absolute due to lack of explicit terminology usage.",
    "level": "Primary",
    "reasoning_summary": "This content is a strong fit for the Agile Product Management category. It thoroughly explores key agile principles like experimentation, feedback, and customer focus, all of which are central to the field. While it doesn’t use explicit Agile Product Management terms, its concepts and audience alignment make it highly relevant for practitioners seeking to enhance agile product development practices."
  },
  "Product Strategy": {
    "resourceId": "Experimentation",
    "category": "Product Strategy",
    "calculated_at": "2025-05-06T20:57:09",
    "ai_confidence": 63.88,
    "ai_mentions": 1.25,
    "ai_alignment": 7.4,
    "ai_depth": 6.85,
    "ai_intent": 8.5,
    "ai_audience": 8.2,
    "ai_signal": 6.95,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 64.0,
    "reasoning": "The content does not directly mention 'product strategy' or its frameworks; its only explicit tie-in is an oblique reference to 'organisational goals' and 'product development.' The main emphasis is on hypothesis-driven experimentation within agile workflows, which is relevant for product strategy but not the explicit focus. Conceptually, the content aligns with the customer-oriented and empirical aspects of informed product strategy (alignment: 7.40). The discussion is moderately deep — it critiques the importance, outcomes, and cultural implications of experimentation (depth: 6.85). The intent (8.50) and audience (8.20) are highly relevant, targeting readers interested in strategy-driven innovation. Signal-to-noise (6.95) is slightly diluted by focus on team culture and general agile practices, which are somewhat outside strict product strategy. No outdated practices or negative tone were observed; hence, no penalties. The overall confidence is moderate, weighted by the strengths in conceptual/methodological relevance but limited by lack of explicit category mention and some content drift.",
    "level": "Secondary"
  },
  "Continuous Delivery": {
    "resourceId": "Experimentation",
    "category": "Continuous Delivery",
    "calculated_at": "2025-05-06T20:57:10",
    "ai_confidence": 43.45,
    "ai_mentions": 0.65,
    "ai_alignment": 5.4,
    "ai_depth": 6.55,
    "ai_intent": 6.8,
    "ai_audience": 7.0,
    "ai_signal": 7.75,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 43.0,
    "reasoning": "The content discusses hypothesis-driven experimentation within agile workflows, with an emphasis on learning, empirical validation, and continuous improvement. These are thematically adjacent to Continuous Delivery (CD), as both value iterative cycles, rapid feedback, and organizational learning. However, there are no direct or explicit mentions of 'Continuous Delivery' or its foundational terminology (deployment automation, release pipelines, etc.). The primary focus is on experimentation as a general practice for innovation and adaptation, rather than specifically on delivering software in short, reliable cycles or the operational mechanics that define CD. The depth of the discussion involves cultural and process elements relevant to organizations practicing CD, but it does not address CD tools, automation, deployment, or delivery pipeline concepts explicitly. The intent and audience (teams seeking to innovate with empirical methods) overlap with the CD audience but are not exclusively CD-focused. Overall, the content is relevant to organizations practicing or interested in CD but doesn't meet the strict definition or direct topicality required for high confidence under this category.",
    "level": "Tertiary"
  },
  "Large Scale Agility": {
    "resourceId": "Experimentation",
    "category": "Large Scale Agility",
    "calculated_at": "2025-05-06T20:57:09",
    "ai_confidence": 58.038,
    "ai_mentions": 2.242,
    "ai_alignment": 6.832,
    "ai_depth": 6.552,
    "ai_intent": 7.138,
    "ai_audience": 7.021,
    "ai_signal": 6.826,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 58.0,
    "reasoning": "The content focuses on experimentation and hypothesis-driven approaches within agile workflows, discussing how organisations can foster learning, innovation, and adaptability. There are some conceptual overlaps with Large Scale Agility, such as culture change, cross-functional collaboration, and organisational impact. However, it lacks explicit direct mentions of large-scale Agile, scaling frameworks (e.g., SAFe, LeSS), or enterprise transformation strategies—it remains general and applicable to both team and organisational levels. The discussion goes beyond basic explanation, addressing cultural impact and long-term systemic benefits which aligns with large-scale themes. The main intent appears relevant to Agile transformation, but not exclusively or thoroughly framed in a large-scale, enterprise context. The audience is broad: it could appeal to both team members and organisational leaders. The content is focused, largely avoiding tangents, but isn't laser-focused on large-scale scaling issues. No penalties are applied; the content is contemporary, not critical, and does not reference obsolete practice. The confidence score reflects a moderate alignment—there are signals relevant to Large Scale Agility, but not enough explicit, deep, or repeated focus to merit a high score.",
    "level": "Tertiary"
  },
  "Transparency": {
    "resourceId": "Experimentation",
    "category": "Transparency",
    "calculated_at": "2025-05-06T20:57:11",
    "ai_confidence": 27.4,
    "ai_mentions": 0.4,
    "ai_alignment": 2.1,
    "ai_depth": 2.6,
    "ai_intent": 2.7,
    "ai_audience": 8.5,
    "ai_signal": 7.7,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 27.0,
    "reasoning": "The content centers on experimentation in Agile, focusing on hypothesis-driven approaches, empirical testing, and fostering a learning culture. There is no explicit or implicit mention of transparency, visibility, or open communication—key elements of the Transparency category. The alignment and intent are both low, as the main purpose is to encourage learning and adaptability through experimentation, not specifically transparency. Audience and signal scores are higher, recognizing that Agile practitioners would find the article relevant, and it remains focused on its stated intent. There is no outdated information and no contradictory tone, so no penalties are applied. The resulting confidence score is low, which matches the lack of direct or meaningful connection to transparency.",
    "level": "Ignored"
  },
  "Experimentation": {
    "resourceId": "Experimentation",
    "category": "Experimentation",
    "calculated_at": "2025-05-06T20:57:13",
    "ai_confidence": 97.87,
    "ai_mentions": 9.6,
    "ai_alignment": 10.0,
    "ai_depth": 9.9,
    "ai_intent": 10.0,
    "ai_audience": 9.7,
    "ai_signal": 9.5,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 98.0,
    "reasoning": "The content directly and repeatedly references 'experimentation' as a core practice within Agile workflows, explicitly focusing on hypothesis-driven approaches, testing ideas, and validating assumptions. It strongly aligns with the category definition by elaborating on how experimentation empowers teams to learn from failures, use feedback loops, and drive continuous improvement. The depth is substantial, discussing strategic impact, continuous learning, and cultural integration in Agile contexts. The intent is clearly informative and supportive, targeting Agile practitioners and organizations intent on innovation. The signal-to-noise ratio is very high; nearly all content is hyper-relevant with no filler or digressions. There are no outdated or contradictory elements. Small fractional differences in scores reflect that, while mentions are very frequent, a few direct references to specific techniques (like A/B testing) or detailed case studies could further raise the score. Overall, this content is an almost perfect fit for the 'Experimentation' category.",
    "level": "Primary",
    "reasoning_summary": "This content is an excellent match for the 'Experimentation' category, as it thoroughly explores experimentation within Agile, focusing on hypothesis-driven work, learning from failure, and continuous improvement. It’s highly relevant, well-structured, and aimed at Agile professionals, though it could be enhanced with more specific techniques or case studies. Overall, it strongly supports the category’s intent and audience."
  },
  "Service Level Expectation": {
    "resourceId": "Experimentation",
    "category": "Service Level Expectation",
    "calculated_at": "2025-05-06T20:57:13",
    "ai_confidence": 7.7,
    "ai_mentions": 0.2,
    "ai_alignment": 0.7,
    "ai_depth": 0.5,
    "ai_intent": 0.4,
    "ai_audience": 2.1,
    "ai_signal": 0.5,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 8.0,
    "reasoning": "The content is focused entirely on the importance of experimentation, hypothesis-driven approaches, and continuous improvement in agile contexts. However, there is no mention—explicit or implicit—of Service Level Expectation (SLE), its definition, calculation, or application. The main themes do not directly align with SLE; while predictability and continuous improvement are discussed, they are generic Agile concepts and not specifically tied to SLE. No content addresses elapsed time ranges, probabilities, metrics, or SLE transparency. The intended audience does overlap (agile practitioners), but the discussion is not targeting SLE-specific concerns, and the signal for the 'Service Level Expectation' category is very faint. No penalties applied as the content does not criticize or misrepresent the category; it is simply off-topic.",
    "level": "Ignored"
  },
  "Definition of Ready": {
    "resourceId": "Experimentation",
    "category": "Definition of Ready",
    "calculated_at": "2025-05-06T20:57:02",
    "ai_confidence": 8.2,
    "ai_mentions": 0.1,
    "ai_alignment": 0.3,
    "ai_depth": 0.2,
    "ai_intent": 0.2,
    "ai_audience": 2.0,
    "ai_signal": 0.2,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 8.0,
    "reasoning": "The content is focused on experimentation in Agile workflows, emphasizing hypothesis-driven approaches, empirical learning, and the value of a scientific mindset. Nowhere does it mention the Definition of Ready, nor does it discuss criteria, checklists, or readiness of backlog items. Its conceptual alignment with the Definition of Ready is minimal — while experimentation can be valuable in general Agile contexts, it is not a key topic of DoR nor does it address readiness for sprint planning. The depth is correspondingly low, as there are no substantial or even surface-level links to the specified category. The content is likely aimed at Agile practitioners and teams, which brings a modest audience alignment, but the focus and intent do not match the category. Nearly all the content is off-topic from the Definition of Ready. No penalties are applied, as there is no outdated or contradicting information. The extremely low confidence reflects that the content does not pertain to the category.",
    "level": "Ignored"
  },
  "Artificial Intelligence": {
    "resourceId": "Experimentation",
    "category": "Artificial Intelligence",
    "calculated_at": "2025-05-06T20:57:15",
    "ai_confidence": 19.8,
    "ai_mentions": 0.2,
    "ai_alignment": 2.0,
    "ai_depth": 2.3,
    "ai_intent": 2.4,
    "ai_audience": 6.0,
    "ai_signal": 4.0,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 20.0,
    "reasoning": "The content focuses entirely on experimentation and hypothesis-driven approaches within agile workflows, without any direct or indirect reference to Artificial Intelligence or its application in Agile, DevOps, or software development. There are no explicit mentions (mentions: 0.2). Conceptually, while experimentation is an important practice in agile, the discussion lacks any alignment with AI's role or impact (alignment: 2.0). The discussion is moderately developed with examples and rationale for experimentation, but this depth does not extend to AI (depth: 2.3). The intent is to promote experimentation in agile, not AI in agile (intent: 2.4). The audience may overlap with practitioners interested in AI-enhanced agile, but the content is generic and does not target AI-related audiences specifically (audience: 6.0). The signal is moderately focused on experimentation in agile, but has no noise relating to AI (signal: 4.0). No penalties applied, as there is no outdated or contradictory material. Overall, the content does not fit the category of Artificial Intelligence as defined by the classification.",
    "level": "Ignored"
  },
  "Product Management": {
    "resourceId": "Experimentation",
    "category": "Product Management",
    "calculated_at": "2025-05-06T20:57:02",
    "ai_confidence": 84.9,
    "ai_mentions": 2.7,
    "ai_alignment": 8.9,
    "ai_depth": 8.3,
    "ai_intent": 8.2,
    "ai_audience": 8.7,
    "ai_signal": 8.6,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 85.0,
    "reasoning": "The content directly discusses experimentation in the context of agile workflows, focusing on hypothesis-driven approaches, empirical decision-making, and continuous improvement—all central tenets of product management strategy. It references the alignment of product development with organisational goals and customer needs, discusses feedback loops, and argues for a systemic, evidence-based culture. Although 'Product Management' is not mentioned by name, the conceptual and audience alignment is clear: product managers, agile practitioners, and strategic product teams are the intended readers. The writing is focused and relevant, with minimal off-topic content. The mention score is moderate due to a lack of explicit category naming, but the content’s in-depth treatment of relevant frameworks and systemic thinking in product development justifies high scores for alignment and depth. No penalties were applied, as the content is current, neutral in tone, and supports the foundational practices found in modern product management.",
    "level": "Primary",
    "reasoning_summary": "This content is a strong fit for the category, as it explores key product management principles like hypothesis-driven development, empirical decision-making, and continuous improvement within agile workflows. While it doesn’t explicitly name 'Product Management', its focus and depth clearly target product managers and strategic teams, making it highly relevant and well-aligned with the category’s core themes."
  },
  "Minimum Viable Product": {
    "resourceId": "Experimentation",
    "category": "Minimum Viable Product",
    "calculated_at": "2025-05-06T20:57:02",
    "ai_confidence": 54.8,
    "ai_mentions": 0.8,
    "ai_alignment": 7.2,
    "ai_depth": 6.3,
    "ai_intent": 7.7,
    "ai_audience": 7.0,
    "ai_signal": 7.2,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 55.0,
    "reasoning": "The content thoroughly discusses the importance of hypothesis-driven experimentation, which is conceptually adjacent to Minimum Viable Product practices, as both focus on validating assumptions and iterative improvement. However, it does not directly reference MVP terminology or specific MVP strategies, nor does it discuss techniques like selecting core features or using MVPs to test market reactions. The alignment is strong because MVP is one way to apply the principles outlined, but the discussion remains more general, centering on experimentation and agile culture. The depth is moderate, reflecting a thoughtful, multi-paragraph exploration of how a scientific approach can drive agile workflows, but lacking specifics on MVP application, case studies, or metrics. The audience is likely practitioners or teams interested in agile and evidence-based management, which aligns with the MVP category, though not as directly targeted as it could be. The signal-to-noise ratio is solid, with little extraneous content. No penalties are applied, as the information is current and supports evidence-based practices. The overall confidence is moderate: the connection to MVP is clear but indirect, justifying a score above 50 but below a threshold where MVP-specific discussion would appear.",
    "level": "Tertiary"
  },
  "Beta Codex": {
    "resourceId": "Experimentation",
    "category": "Beta Codex",
    "calculated_at": "2025-05-06T20:57:02",
    "ai_confidence": 56.34,
    "ai_mentions": 1.5,
    "ai_alignment": 6.8,
    "ai_depth": 6.7,
    "ai_intent": 7.1,
    "ai_audience": 7.6,
    "ai_signal": 8.3,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 56.0,
    "reasoning": "The content discusses experimentation within agile workflows, emphasizing hypothesis-driven approaches and a culture of learning, adaptation, and cross-functional collaboration. These elements are thematically related to the Beta Codex principles (e.g., decentralisation, human-centric design, adaptability), but the text does not explicitly mention 'Beta Codex' or its foundational theories. While the discussion aligns conceptually with decentralised and adaptive organisational cultures—integral to Beta Codex—it does not explore Beta Codex principles in depth nor compare them to traditional models. It is relevant for audiences interested in agile methodologies and organisational adaptability, which overlaps with the Beta Codex audience, though it lacks direct targeting. The content is focused, relevant, and offers solid depth, but the lack of direct mention and explicit anchoring in Beta Codex frameworks limits the confidence. Therefore, the weighted confidence reflects strong partial alignment but stops short of full endorsement for the Beta Codex category.",
    "level": "Tertiary"
  },
  "Lean Startup": {
    "resourceId": "Experimentation",
    "category": "Lean Startup",
    "calculated_at": "2025-05-06T20:57:02",
    "ai_confidence": 61.4,
    "ai_mentions": 2.2,
    "ai_alignment": 7.7,
    "ai_depth": 6.9,
    "ai_intent": 7.5,
    "ai_audience": 7.1,
    "ai_signal": 7.0,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 61.0,
    "reasoning": "The content describes an experimentation mindset in agile workflows, strongly overlapping with Lean Startup concepts such as hypothesis-driven testing, validation of assumptions, iterative learning, and feedback loops. However, the content never explicitly mentions 'Lean Startup,' 'MVP,' 'Build-Measure-Learn,' or other hallmark terminology central to the category. Instead, it stays broader, focusing on generic agile and experimentation principles that are foundational to (but not exclusive to) Lean Startup. Conceptual alignment is high as the practices align well with the Lean Startup philosophy, but depth is moderate since the discussion remains general, without detailed Lean Startup processes or case studies. The main intent is to advocate for experimentation in agile teams, which suits a Lean Startup audience (practitioners, innovators), though it's not exclusively focused there. The signal is decent, with little to no off-topic content, and the content is actionable for startup practitioners even if it also fits broader agile contexts. No penalty was applied as the content is timely and not contradictory. Overall confidence reflects reasonably strong relevance but is tuned down to recognize the lack of direct references and deep, category-specific exploration.",
    "level": "Secondary"
  },
  "Agile Planning Tools": {
    "resourceId": "Experimentation",
    "category": "Agile Planning Tools",
    "calculated_at": "2025-05-06T20:57:05",
    "ai_confidence": 24.722,
    "ai_mentions": 0.3,
    "ai_alignment": 2.5,
    "ai_depth": 2.2,
    "ai_intent": 2.5,
    "ai_audience": 2.1,
    "ai_signal": 2.0,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 25.0,
    "reasoning": "The content centers on the value of experimentation and hypothesis-driven approaches within agile workflows but makes no direct mention of Agile Planning Tools or specific tool names (e.g., Jira, Trello). Conceptual overlap exists in supporting agile principles and continuous improvement, but the main theme is organizational mindset and practices, not the planning tools or their application. Depth of discussion is moderate regarding experimentation as a process, not as a function enabled by planning tools. Audience could include practitioners interested in agile, but the focus is broad (organizational/strategic) rather than tool-specific. Signal-to-noise ratio is low for this particular category, as the majority of content does not address tools or methodologies for planning. No penalties were applied as the content is not outdated or critical in tone. Confidence is low, as the core requirement—discussing or meaningfully connecting to Agile Planning Tools—is not met.",
    "level": "Ignored"
  },
  "Professional Scrum": {
    "resourceId": "Experimentation",
    "category": "Professional Scrum",
    "calculated_at": "2025-05-06T20:57:05",
    "ai_confidence": 67.95,
    "ai_mentions": 1.1,
    "ai_alignment": 7.2,
    "ai_depth": 6.8,
    "ai_intent": 7.0,
    "ai_audience": 8.0,
    "ai_signal": 9.1,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 68.0,
    "reasoning": "The content centers on experimentation as a core practice in agile workflows, emphasizing empirical decision-making and iteration—both thematically aligned with the ethos of Professional Scrum. There is strong focus on empiricism, evidence-based learning, adaptation, and value delivery, all of which are foundational to Professional Scrum. However, professional Scrum itself is never directly named or referenced; terms like Scrum, Scrum values, accountability, technical excellence, or the Scrum roles are absent. Thus, the Direct Mentions score is very low. Depth is moderate: while the discussion meaningfully unpacks why experimentation matters for professional practice, it does not explicitly connect these practices to the Professional Scrum framework or its specific principles (beyond shared concepts like empiricism and adaptability). The intent and audience both fit well—targeted at cross-functional teams and leaders interested in professionalizing agile practice—but are a bit generic rather than tailored specifically for Professional Scrum practitioners. The content is tightly focused with little to no off-topic filler, giving it a high Signal-to-Noise score. With no outdated advice or negative tone, no penalties are applied. Ultimately, though the alignment is solid, the lack of direct connection to Professional Scrum (by name or explicit framework elements) lowers the overall confidence, producing a score in the upper-mid range.",
    "level": "Secondary"
  },
  "Evidence Based Leadership": {
    "resourceId": "Experimentation",
    "category": "Evidence Based Leadership",
    "calculated_at": "2025-05-06T20:57:02",
    "ai_confidence": 76.2,
    "ai_mentions": 4.0,
    "ai_alignment": 8.3,
    "ai_depth": 7.8,
    "ai_intent": 8.2,
    "ai_audience": 8.1,
    "ai_signal": 8.4,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 76.0,
    "reasoning": "The content focuses on hypothesis-driven experimentation within agile workflows, emphasizing empirical evidence, feedback loops, and a scientific approach to organizational improvement. These concepts align strongly with Evidence Based Leadership, particularly where informed (data-driven) decisions and continuous improvement are discussed. However, there is no direct mention of 'evidence based leadership' or key frameworks/authors (e.g., Ken Schwaber, Jeff Sutherland); instead, the alignment is more conceptual. The main ideas — such as using evidence rather than intuition, fostering a culture of learning, and applying feedback for improvement — closely match the category's definition. The discussion is moderately deep, articulating the value and broader impact of experimentation, but it lacks specific case studies, detailed metrics, or explicit leadership terminology/examples. The intent clearly supports the Evidence Based Leadership mindset by promoting data-driven decision-making. The intended audience seems to be organizational leaders or agile practitioners involved in change and improvement, matching the category's likely readership. Content is highly focused, with little to no off-topic material. No penalties were applied since the information is current and supportive of the category.",
    "level": "Secondary",
    "reasoning_summary": "This content aligns well with Evidence Based Leadership, as it highlights the importance of data-driven decisions, feedback loops, and continuous improvement within agile environments. While it doesn’t reference specific frameworks or authors, its focus on empirical methods and fostering a learning culture makes it highly relevant for leaders and practitioners interested in evidence-based organisational change."
  },
  "Working Agreements": {
    "resourceId": "Experimentation",
    "category": "Working Agreements",
    "calculated_at": "2025-05-06T20:57:05",
    "ai_confidence": 33.78,
    "ai_mentions": 0.2,
    "ai_alignment": 3.8,
    "ai_depth": 2.5,
    "ai_intent": 4.2,
    "ai_audience": 6.0,
    "ai_signal": 4.0,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 34.0,
    "reasoning": "The content centers around experimentation and hypothesis-driven approaches in agile workflows, with a focus on continuous improvement, empirical validation, and fostering a culture of learning. However, there are no direct mentions or explicit references to 'working agreements,' nor does the discussion provide specific team norms, collaborative principles, or structured agreements as outlined in the category definition. While the audience is agile practitioners, and there is some peripheral alignment due to the emphasis on team practices and collaboration, the main purpose is not to define, create, or adapt working agreements. The signal is somewhat diffused due to only tangential relevance. Scores remain low across most dimensions except for audience, and no penalties were required as the content is current and appropriately framed. Overall, the content does not substantially fit the 'Working Agreements' category and receives a low confidence score.",
    "level": "Ignored"
  },
  "Agile Transformation": {
    "resourceId": "Experimentation",
    "category": "Agile Transformation",
    "calculated_at": "2025-05-06T20:57:02",
    "ai_confidence": 77.45,
    "ai_mentions": 3.6,
    "ai_alignment": 8.2,
    "ai_depth": 8.7,
    "ai_intent": 7.3,
    "ai_audience": 8.1,
    "ai_signal": 7.8,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 77.0,
    "reasoning": "The content focuses heavily on the theme of experimentation and its essential role within agile workflows. There are no explicit references to 'Agile Transformation' by name, but the content strongly reflects core agile principles such as empirical feedback, continuous improvement, and cultivating an adaptive, learning-oriented culture. Conceptual alignment is high because systematic experimentation and hypothesis-driven decision-making are foundational to many successful agile transformations. The depth is substantial, exploring impacts on mindset, organisational culture, and adaptability, though it stops short of detailing transformation frameworks or change management specifics. The intent is closely relevant—guiding organisations/team leaders in fostering experimentation to drive change—making it strongly aligned with the purposeful aims of agile transformation. The target audience includes those responsible for organisational improvement and agile practices, aligning well with the category. The signal-to-noise ratio is high; all content is clearly tied to agile principles, with no filler or unrelated material. However, slightly lower scores for mentions and intent reflect that the piece never uses the term 'Agile Transformation' directly and does not discuss transformation at scale or formal change strategies. No penalties were applied; the content is current, constructive, and strongly aligned.",
    "level": "Secondary",
    "reasoning_summary": "This content is a strong fit for the category, as it thoroughly explores experimentation within agile practices, emphasising empirical feedback and adaptability—key aspects of agile transformation. While it doesn’t mention 'Agile Transformation' directly or cover formal frameworks, its focus on mindset and organisational change makes it highly relevant for those leading agile initiatives or organisational improvement."
  },
  "Test First Development": {
    "resourceId": "Experimentation",
    "category": "Test First Development",
    "calculated_at": "2025-05-06T20:57:05",
    "ai_confidence": 36.45,
    "ai_mentions": 0.2,
    "ai_alignment": 3.6,
    "ai_depth": 4.7,
    "ai_intent": 3.2,
    "ai_audience": 5.8,
    "ai_signal": 6.1,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 36.0,
    "reasoning": "The content focuses on experimentation in agile workflows using hypothesis-driven approaches and empirical validation. While there are surface similarities to Test First Development (e.g., emphasis on testing ideas, feedback loops, and empirical evidence), the article never directly mentions Test First Development, TDD, ATDD, or establishing success criteria before implementation. Its conceptual alignment is partial: hypothesis-driven experimentation is broader and not centered on the distinct practices of Test First (such as writing tests or acceptance criteria before code). The depth relates primarily to the philosophy and benefits of experimentation rather than the specifics of Test First approaches. Intent is aligned with agile and continuous improvement themes, but not strictly with Test First. The audience appears to be agile practitioners and general teams (not specifically developers or testers concerned with test-first practices). Content is generally focused, but the noise comes from broader agile experimentation themes rather than Test First Development. No penalties are applied, as the tone is neutral, up-to-date, and not misrepresenting Test First. Overall, the confidence score reflects low direct relevance and moderate thematic overlap.",
    "level": "Ignored"
  },
  "Azure DevOps": {
    "resourceId": "Experimentation",
    "category": "Azure DevOps",
    "calculated_at": "2025-05-06T20:57:05",
    "ai_confidence": 20.8,
    "ai_mentions": 0.3,
    "ai_alignment": 2.8,
    "ai_depth": 2.9,
    "ai_intent": 1.7,
    "ai_audience": 6.2,
    "ai_signal": 5.1,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 21.0,
    "reasoning": "The content does not directly mention Azure DevOps; it discusses experimentation in agile workflows in a very general manner. While some conceptual alignment exists—Azure DevOps supports agile experimentation and continuous improvement—there are no explicit references to its tools, practices, or specifics. The depth is limited to generic principles without exploring Azure DevOps functionalities. The intent is broadly informative about agile experimentation, not specifically aimed at Azure DevOps users or practitioners. The intended audience could overlap with Azure DevOps users, but the lack of specificity keeps the audience score moderate. The content is focused with little irrelevant material, yielding a moderately good signal-to-noise ratio. No penalties are applied as the content is not outdated nor critical, just overly general and lacking direct relevance.",
    "level": "Ignored"
  },
  "Definition of Done": {
    "resourceId": "Experimentation",
    "category": "Definition of Done",
    "calculated_at": "2025-05-06T20:57:02",
    "ai_confidence": 10.8,
    "ai_mentions": 0.1,
    "ai_alignment": 1.7,
    "ai_depth": 2.0,
    "ai_intent": 1.6,
    "ai_audience": 5.2,
    "ai_signal": 1.0,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 11.0,
    "reasoning": "The content exclusively discusses experimentation, hypothesis-driven approaches, and how these practices promote learning, adaptability, and innovation in agile workflows. There are no direct mentions of the Definition of Done (DoD), nor any explicit references to its concepts such as criteria for product completeness, alignment with acceptance criteria, code review, or documentation. Conceptual overlap is minimal: while experimentation may relate to overall team quality and iterative delivery, it is not intrinsically or explicitly tied to the DoD, and there is no exploration of DoD-specific themes or artefacts. The content's intent focuses entirely on promoting a culture of experimentation rather than on defining or improving DoD practices. The audience (agile practitioners, organisations) potentially overlaps with DoD interests, but the signal-to-noise ratio is low regarding the DoD topic. No penalties are applied as the content is accurate, contemporary, and neutral in tone. Overall, the confidence score is extremely low, reflecting the content’s lack of direct or meaningful alignment with the Definition of Done category.",
    "level": "Ignored"
  },
  "Pragmatic Thinking": {
    "resourceId": "Experimentation",
    "category": "Pragmatic Thinking",
    "calculated_at": "2025-05-06T20:57:02",
    "ai_confidence": 88.2,
    "ai_mentions": 3.7,
    "ai_alignment": 9.5,
    "ai_depth": 8.9,
    "ai_intent": 9.2,
    "ai_audience": 8.7,
    "ai_signal": 9.6,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 88.0,
    "reasoning": "The content directly addresses experimentation within agile workflows, a strongly pragmatic and practice-oriented topic closely related to Pragmatic Thinking. While the term 'pragmatic thinking' itself is not explicitly mentioned, the description, depth, and intent all focus on real-world application: hypothesis-driven experimentation, iterative testing, empirical learning, and adaptation—central tenets of pragmatic problem-solving in Agile/DevOps contexts. The discussion demonstrates depth, covering systemic change, feedback loops, and the cultivation of a learning culture. It targets practitioners and teams actively applying agile principles, with very little off-topic content. The only minor deduction is in 'mentions' as the category name isn't explicitly used, but conceptual and strategic alignment is extremely high. No penalties were needed, as the content is current, supportive, and frames experimentation as essential for practical problem-solving in complex environments.",
    "level": "Primary",
    "reasoning_summary": "This content is a strong fit for the category, as it thoroughly explores practical experimentation within agile workflows. It emphasises real-world application, iterative learning, and adaptation—key aspects of pragmatic thinking—even if the term itself isn’t used. The focus on actionable strategies and systemic change makes it highly relevant for practitioners seeking effective, practice-oriented solutions."
  },
  "Employee Engagement": {
    "resourceId": "Experimentation",
    "category": "Employee Engagement",
    "calculated_at": "2025-05-06T20:57:08",
    "ai_confidence": 48.3,
    "ai_mentions": 1.9,
    "ai_alignment": 5.7,
    "ai_depth": 4.9,
    "ai_intent": 5.3,
    "ai_audience": 6.0,
    "ai_signal": 7.1,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 48.0,
    "reasoning": "The content primarily focuses on the value of experimentation within agile workflows, particularly how hypothesis-driven approaches enhance team innovation, adaptability, and value delivery. There are brief allusions to team engagement and motivation—for example, references to fostering a culture where failure is a learning opportunity, promoting continuous improvement, and driving engagement and motivation as team members see the impact of their contributions. However, explicit mention of 'employee engagement' is lacking, and key category themes such as recognition, feedback, leadership styles, or specific engagement measurement strategies are not directly explored. The content aligns partially on a conceptual level but addresses experimentation as a process rather than focusing deeply on the psychological or social dynamics central to employee engagement. The signal-to-noise ratio is decent, as much of the content is relevant to agile team functioning, but it is not tightly focused on the human drivers of engagement. The intended audience seems to be agile practitioners or organisational leaders, which only partially overlaps with the category's typical audience. No penalties were applied as the tone is positive and up-to-date. The final confidence score is moderate, reflecting peripheral but not central alignment with the Employee Engagement category.",
    "level": "Tertiary"
  },
  "Leadership": {
    "resourceId": "Experimentation",
    "category": "Leadership",
    "calculated_at": "2025-05-06T20:57:02",
    "ai_confidence": 43.2,
    "ai_mentions": 0.5,
    "ai_alignment": 5.4,
    "ai_depth": 4.9,
    "ai_intent": 4.7,
    "ai_audience": 6.2,
    "ai_signal": 6.0,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 43.0,
    "reasoning": "The content focuses primarily on experimentation as a practice within agile workflows, emphasizing hypothesis-driven testing, value delivery, and fostering a culture of innovation. Direct references to leadership are virtually absent; there are no explicit mentions of leaders or leadership roles, and the leadership category is only indirectly implied through themes such as cultural change, fostering learning, and supporting systemic change. The alignment score is moderate, as experimentation can be an important aspect of leadership in fostering agility and innovation, but the central message is about team practices rather than leadership itself. The depth of discussion remains largely at the level of experimentation practice, not extending into how leaders enact, model, or strategically enable such cultures. The intent and purpose are focused on organizational improvement, offering some tangential alignment with leadership, but not as a central theme. The audience appears to be both practitioners and organizational change agents (potentially including leaders), but it is not specifically geared toward a leadership audience. The signal-to-noise ratio is reasonably strong, as the content remains on topic regarding experimentation but does not focus on leadership’s role in this process. Overall, the confidence in assigning this to the Leadership category is relatively low and reflects indirect, not direct, relevance.",
    "level": "Tertiary"
  },
  "Lean": {
    "resourceId": "Experimentation",
    "category": "Lean",
    "calculated_at": "2025-05-06T20:57:02",
    "ai_confidence": 39.2,
    "ai_mentions": 0.4,
    "ai_alignment": 3.0,
    "ai_depth": 3.6,
    "ai_intent": 4.8,
    "ai_audience": 4.5,
    "ai_signal": 4.1,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 39.0,
    "reasoning": "The content focuses on experimentation within agile workflows, emphasizing hypothesis-driven approaches, empirical validation, and continuous improvement. These concepts are partially aligned with Lean, especially the value placed on learning, iterative process improvement, and culture change. However, the content never references Lean directly by name or core Lean terms such as waste reduction, 5S, value stream mapping, or Lean tools. The main thrust is more generalized agile or scientific experimentation, not specifically Lean methodologies. Because of this, Direct Mentions is extremely low (almost none), and Conceptual Alignment, Depth, and Intent score modestly—notably because continuous improvement (Kaizen) is a Lean principle addressed indirectly, but Lean's distinct features are missing. The Audience and Signal dimensions are moderate, given the content speaks to process-oriented professionals but not exclusively Lean practitioners. No outdated references or contradicting tones are present, so no penalties apply. Overall, while experimentation and empirical cycles are compatible with Lean, the content does not substantively fit the Lean category except in a broad, tangential way.",
    "level": "Ignored"
  },
  "Psychological Safety": {
    "resourceId": "Experimentation",
    "category": "Psychological Safety",
    "calculated_at": "2025-05-06T20:57:02",
    "ai_confidence": 46.4,
    "ai_mentions": 1.2,
    "ai_alignment": 5.8,
    "ai_depth": 6.4,
    "ai_intent": 5.5,
    "ai_audience": 8.1,
    "ai_signal": 8.6,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 46.0,
    "reasoning": "The content describes the importance of experimentation in agile workflows, focusing on hypothesis-driven methods, embracing failure as learning, and fostering a culture of learning and adaptability. These themes are indirectly related to psychological safety, as a psychologically safe environment enables experimentation and risk-taking. However, the content does not directly mention psychological safety, nor does it thoroughly explore core psychological safety concepts such as risk-free communication, expressing dissent, or leader behaviors to foster safety. The main intent is on experimentation as a cultural and process lever, with psychological safety implied but not explicit or central. The discussion has moderate depth on culture and learning, and its target audience (Agile practitioners, organizations) overlaps with the category, though not uniquely so. The content is focused and mostly relevant, but the link to psychological safety is secondary—not primary—resulting in a moderate overall confidence score.",
    "level": "Tertiary"
  },
  "Technical Mastery": {
    "resourceId": "Experimentation",
    "category": "Technical Mastery",
    "calculated_at": "2025-05-06T20:57:05",
    "ai_confidence": 45.215,
    "ai_mentions": 1.7,
    "ai_alignment": 4.9,
    "ai_depth": 4.7,
    "ai_intent": 5.2,
    "ai_audience": 6.2,
    "ai_signal": 7.1,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 45.0,
    "reasoning": "The content is focused on hypothesis-driven experimentation within agile workflows, emphasizing validation, empirical evidence, learning, and adaptability. While these themes overlap with some aspects of continuous improvement and iterative feedback (adjacent to technical mastery), the discussion does not directly engage with core software engineering practices such as code quality, software design, architecture, or DevOps. There are no explicit mentions of technical mastery or in-depth treatment of tools, craftsmanship, technical debt, or engineering methodologies. The content is largely directed at agile teams, which may include technical practitioners, but its framing is broad—applicable to product management, business, and organisational contexts as well as technical ones. Signal-to-noise ratio is relatively solid, as the content stays on topic, but the signal is marginal for technical mastery specifically. No penalty points were necessary as the content is up-to-date and not satirical or contrary in tone. Hence, the confidence score is moderate, reflecting a partial but not primary alignment with the Technical Mastery category.",
    "level": "Tertiary"
  },
  "Common Goals": {
    "resourceId": "Experimentation",
    "category": "Common Goals",
    "calculated_at": "2025-05-06T20:57:08",
    "ai_confidence": 44.65,
    "ai_mentions": 1.6,
    "ai_alignment": 5.9,
    "ai_depth": 5.6,
    "ai_intent": 3.9,
    "ai_audience": 7.3,
    "ai_signal": 6.5,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 45.0,
    "reasoning": "The content focuses on experimentation as a critical practice in agile workflows, highlighting hypothesis-driven approaches and the value of empirical evidence. While it mentions 'organisational goals' and discusses alignment with customer needs and organisational objectives, these references are implicit and brief, not making Common Goals the central theme. The piece does not directly discuss the concept of Common Goals, their definition in agile or DevOps, or frameworks like OKRs; nor does it detail techniques for aligning team efforts with higher-level strategy. The main intent is to advocate for experimentation as a learning and improvement mechanism, which is related but not synonymous with Common Goals. The audience is clearly agile practitioners or teams, aligning reasonably well with the category’s intended audience. Signal is moderately high since the content is focused, but only partially relevant to the strict definition provided. No penalties were applied, as the content is not outdated and does not undermine the category, but its fit is partial—yielding a below-average confidence score.",
    "level": "Tertiary"
  },
  "Ability to Innovate": {
    "resourceId": "Experimentation",
    "category": "Ability to Innovate",
    "calculated_at": "2025-05-06T20:57:05",
    "ai_confidence": 90.0,
    "ai_mentions": 8.5,
    "ai_alignment": 9.8,
    "ai_depth": 9.2,
    "ai_intent": 9.1,
    "ai_audience": 8.8,
    "ai_signal": 8.7,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 90.0,
    "reasoning": "The content is highly aligned with the 'Ability to Innovate' category, explicitly discussing how hypothesis-driven experimentation empowers agile teams to innovate through empirical evidence and learning. The theme strongly matches the definition and key topics by centering on mechanisms (experiment-driven learning, failure as learning, feedback loops) that foster innovation. Depth is strong, covering conceptual rationale, cultural mindset, and benefits to organizational adaptation and product development. While the word 'innovation' is used a few times and not repetitively, the linkage between experimentation and enhanced innovation is clearly articulated both directly and in context. The audience is relevant for practitioners and strategists involved in Agile/DevOps, though the focus is more on operational teams than solely executives. The text is dense and focused with a high signal-to-noise ratio, sticking closely to innovation drivers in agile settings. No penalties apply; the content is current, positive, and accurately reflects EBM principles. The final confidence score of 90.0 accurately reflects the strong, comprehensive alignment with the prescribed category.",
    "level": "Primary",
    "reasoning_summary": "This content clearly fits the 'Ability to Innovate' category, as it explores how hypothesis-driven experimentation enables agile teams to learn, adapt, and drive innovation. It effectively connects experimentation, learning from failure, and feedback loops to organisational growth, making it highly relevant for practitioners aiming to enhance innovation within agile and DevOps environments."
  },
  "Working Software": {
    "resourceId": "Experimentation",
    "category": "Working Software",
    "calculated_at": "2025-05-06T20:57:02",
    "ai_confidence": 27.65,
    "ai_mentions": 0.25,
    "ai_alignment": 2.8,
    "ai_depth": 3.15,
    "ai_intent": 2.3,
    "ai_audience": 6.1,
    "ai_signal": 6.4,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 28.0,
    "reasoning": "The content centers on experimentation in agile practices, focusing on hypothesis-driven testing and validating assumptions. While it discusses iterative testing, feedback loops, and aligning development with customer needs—all tangentially related to working software—it never directly mentions 'working software' as an artifact or deliverable. The content's main idea is about a process (experimentation), not the artifact (working software) or its direct delivery. There's some conceptual overlap in discussing outcomes like innovation and alignment, but the depth is moderate since it never ties back explicitly to working software increments or quality measures. The intent is to inform about experimentation as a mindset and process, which is adjacent rather than central to the category. Audience and signal scores are relatively higher because agile practitioners are the target and the content is focused, but overall confidence is low due to lack of direct relevance.",
    "level": "Ignored"
  },
  "Agile Product Operating Model": {
    "resourceId": "Experimentation",
    "category": "Agile Product Operating Model",
    "calculated_at": "2025-05-06T20:57:08",
    "ai_confidence": 66.55,
    "ai_mentions": 0.4,
    "ai_alignment": 7.6,
    "ai_depth": 6.8,
    "ai_intent": 7.2,
    "ai_audience": 7.7,
    "ai_signal": 8.3,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 67.0,
    "reasoning": "The content focuses on experimentation within agile workflows, emphasizing hypothesis-driven approaches, empirical decision-making, iterative delivery, and continuous improvement—all closely aligned with principles of the Agile Product Operating Model (APOM). However, the APOM itself is not mentioned directly or named, and there is no explicit reference to related frameworks or concepts specific to APOM (such as product operating models, transition from project to product, or explicit governance/structure). The depth of discussion shows strong engagement with foundational agile and product mindset concepts (adaptive value delivery, learning culture), but it remains somewhat generic and does not substantively explore APOM's organizational or structural elements. The intent is aligned with value-driven product improvement, and the audience appears to be agile practitioners and organizational leaders—an appropriate fit. The content avoids off-topic information, concentrating on relevant themes, with high signal-to-noise. No penalties are applied as there are no references to outdated concepts or undermining tone. Confidence is moderate: while experimentation is a foundational practice within APOM, the absence of explicit product-operating language reduces directness and depth per the strict classification guidelines.",
    "level": "Secondary"
  },
  "Continuous Improvement": {
    "resourceId": "Experimentation",
    "category": "Continuous Improvement",
    "calculated_at": "2025-05-06T20:57:02",
    "ai_confidence": 92.7,
    "ai_mentions": 8.7,
    "ai_alignment": 9.6,
    "ai_depth": 9.2,
    "ai_intent": 9.3,
    "ai_audience": 8.6,
    "ai_signal": 9.4,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 93.0,
    "reasoning": "The content consistently emphasizes the importance of hypothesis-driven experimentation and empirical evidence, which are central tenets of Continuous Improvement. It explicitly mentions continuous improvement as a mindset enabled by experimentation and feedback loops and relates experimentation to sustained organizational change and team effectiveness. The main intent is to advocate for experimentation as a route to ongoing organizational learning, adaptability, and improved processes—directly aligned with the category. The discussion is both strategic and practical, indicating an informed audience likely interested in business agility and team effectiveness. There is minor variance in scores: 'mentions' is slightly lower because 'Continuous Improvement' is directly mentioned once, but the rest of the narrative centers on its principles. 'Audience' is high, but not a perfect 10, as the language is broadly accessible and speaks to various organizational roles rather than deeply technical or exclusively executive audiences. No outdated or contradictory content, and minimal filler—highly focused discussion. The final confidence score accurately reflects a strong, direct fit with the category, following the weighting formula.",
    "level": "Primary",
    "reasoning_summary": "This content is a strong match for the Continuous Improvement category. It highlights the value of experimentation, feedback, and ongoing learning—core principles of continuous improvement. The discussion is both strategic and practical, appealing to professionals interested in organisational agility and effectiveness. While the term is mentioned only once, the focus throughout clearly aligns with the category’s intent."
  },
  "Organisational Physics": {
    "resourceId": "Experimentation",
    "category": "Organisational Physics",
    "calculated_at": "2025-05-06T20:57:09",
    "ai_confidence": 82.0,
    "ai_mentions": 2.1,
    "ai_alignment": 8.9,
    "ai_depth": 8.7,
    "ai_intent": 8.3,
    "ai_audience": 8.5,
    "ai_signal": 8.6,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 82.0,
    "reasoning": "The content does not directly mention 'Organisational Physics' or systems thinking by name, resulting in a low score for Direct Mentions. However, it strongly aligns conceptually with the category—emphasising systemic change, feedback loops, experimentation as a mechanism for adaptive learning, and organisational behaviour. The discussion is substantive, describing not just the process but also the cultural and systemic impacts on organisations, which boosts the depth score. The purpose is educational and highly aligned with the aim of understanding and improving organisational performance through systemic perspectives. The intended audience appears to be organisational leaders, agile practitioners, and those interested in cultural change, which closely matches the likely audience for Organisational Physics topics. The content is focused, with little off-topic material—yielding a high signal-to-noise score. No penalties were necessary, as there are no outdated practices, nor does the tone contradict the framing of the category. The final confidence score is high, reflecting the strong substantive and conceptual fit, but not perfect due to the absence of explicit terminology.",
    "level": "Primary",
    "reasoning_summary": "While the content doesn’t explicitly reference ‘Organisational Physics’ or systems thinking, it thoroughly explores related concepts like systemic change, feedback loops, and organisational behaviour. Its educational focus and relevance to leaders and change agents make it a strong conceptual fit for the category, even if direct terminology is missing. The discussion is in-depth and highly aligned with the intended audience’s interests."
  },
  "Entrepreneurship": {
    "resourceId": "Experimentation",
    "category": "Entrepreneurship",
    "calculated_at": "2025-05-06T20:57:02",
    "ai_confidence": 58.55,
    "ai_mentions": 1.1,
    "ai_alignment": 7.2,
    "ai_depth": 7.5,
    "ai_intent": 6.8,
    "ai_audience": 7.1,
    "ai_signal": 8.0,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 59.0,
    "reasoning": "The content discusses experimentation within agile workflows, focusing on hypothesis-driven approaches, learning from failure, adaptation, and value delivery. These themes overlap with entrepreneurial concepts such as innovation, risk-taking, and responding to market demands. However, the content is framed generally for organizational or team contexts rather than directly referencing entrepreneurship, entrepreneurs, or startup ventures. There are no explicit mentions of 'entrepreneurship' or entrepreneurs, resulting in a low 'Direct Mentions' score. Nevertheless, the conceptual alignment is moderately strong because principles of experimentation and innovation are critical in entrepreneurship. The discussion is fairly deep, outlining why experimentation matters and detailing its impact on organizational culture, value creation, and product development. The intent is informative and supportive of innovation and adaptation, which shares audience overlap with entrepreneurship content, though the immediate target audience seems broader (including product teams, agile practitioners, or managers, not just entrepreneurs). The focus is consistently maintained on experimentation; little off-topic content exists, leading to a solid 'Signal-to-Noise Ratio'. No penalties are applied, as the content is current and maintains a neutral/positive tone. Overall, the confidence score reflects that this content is relevant to entrepreneurship as a secondary fit, but does not directly or exclusively serve an entrepreneurial audience.",
    "level": "Tertiary"
  },
  "Hybrid Agile": {
    "resourceId": "Experimentation",
    "category": "Hybrid Agile",
    "calculated_at": "2025-05-06T20:57:05",
    "ai_confidence": 14.22,
    "ai_mentions": 0.3,
    "ai_alignment": 1.7,
    "ai_depth": 2.4,
    "ai_intent": 2.5,
    "ai_audience": 3.2,
    "ai_signal": 4.1,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 14.0,
    "reasoning": "The content focuses entirely on experimentation and hypothesis-driven learning as a practice within agile workflows. There is no direct mention of 'Hybrid Agile,' nor any discussion of blending agile and traditional methodologies, associated dysfunctions, or critical analysis of hybrid approaches. The alignment is weak because while experimentation is valued in both pure agile and some hybrid contexts, the core content does not examine or critique Hybrid Agile practices. The depth of discussion is moderate but centers exclusively on experimentation as a beneficial practice, not as a response to or critique of Hybrid Agile adoption. The intent is educational about agile experimentation, not on Hybrid Agile as defined by the category. The audience appears to be agile practitioners, which could overlap with the Hybrid Agile audience, but is not specific. Signal-to-noise is moderate since the content is focused but wholly off-topic for the defined category. No penalties were applied, as the tone, time period, and content style do not contradict or undermine the category framing.",
    "level": "Ignored"
  },
  "Agnostic Agile": {
    "resourceId": "Experimentation",
    "category": "Agnostic Agile",
    "calculated_at": "2025-05-06T20:57:11",
    "ai_confidence": 53.3,
    "ai_mentions": 0.2,
    "ai_alignment": 6.6,
    "ai_depth": 6.8,
    "ai_intent": 6.1,
    "ai_audience": 6.5,
    "ai_signal": 7.2,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 53.0,
    "reasoning": "The content discusses experimentation in the context of agile workflows, emphasizing hypothesis-driven approaches, empirical decision-making, adaptation, and continuous improvement—all ideas aligned with Agnostic Agile's principles of flexibility, professional context-driven decision-making, and value delivery. However, there is no explicit or direct mention of 'Agnostic Agile' itself or specific thought leaders associated with the category, so the 'mentions' score is very low. The alignment and depth scores are moderate because, while the core ideas resonate with Agnostic Agile's philosophy (e.g., prioritizing principles over strict frameworks), the content does not make a clear or intentional connection to the category and never discusses comparative frameworks, ethical dimensions, or core figures. The intent is moderately aligned: informing agile practitioners about valuable experimentation principles, which supports Agnostic Agile, but it is not narrowly focused on the movement or its unique philosophy. Audience and signal are moderately high, as the material is suitable for agile practitioners interested in improvement and avoids significant off-topic material. No penalties are warranted since the content is current, not satirical, and not undermining the category. The overall confidence reflects strong relevant overlap but significant gaps: this is relevant to Agnostic Agile but would not suffice as a canonical example for the category without explicit contextualization.",
    "level": "Tertiary"
  },
  "Engineering Excellence": {
    "resourceId": "Experimentation",
    "category": "Engineering Excellence",
    "calculated_at": "2025-05-06T20:57:06",
    "ai_confidence": 68.58,
    "ai_mentions": 1.2,
    "ai_alignment": 7.9,
    "ai_depth": 7.3,
    "ai_intent": 7.5,
    "ai_audience": 8.1,
    "ai_signal": 8.7,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 69.0,
    "reasoning": "The content focuses on the value of experimentation in agile workflows through hypothesis-driven approaches. While there are no explicit or direct mentions of 'Engineering Excellence' or its key phrases, the conceptual alignment is relatively strong: fostering a culture of continuous improvement, iterative testing, and learning from failure are central themes. These overlap with the spirit of Engineering Excellence, such as promoting high standards and improving engineering processes. However, the discussion is general and rooted mainly in agile thinking, learning, and organisational innovation, without diving into specific engineering practices like code quality, CI/CD, code reviews, or technical metrics. The depth reflects substantial exploration of experimentation's purpose within teams, but lacks details tied directly to software craftsmanship or engineering standards. The intent fits in that it promotes disciplined, evidence-based process improvements applicable to engineering, and the audience is suitable as it speaks to practitioners and organisations interested in agile delivery. The content remains focused but edges into broader themes like cultural change, rather than tightly focusing on engineering technicalities. No penalties apply as the tone is modern and supportive of improvement. The moderately high confidence reflects strong overlap without full directness or explicit depth in engineering practice specifics.",
    "level": "Secondary"
  },
  "Deployment Strategies": {
    "resourceId": "Experimentation",
    "category": "Deployment Strategies",
    "calculated_at": "2025-05-06T20:57:05",
    "ai_confidence": 16.7,
    "ai_mentions": 0.8,
    "ai_alignment": 2.9,
    "ai_depth": 2.3,
    "ai_intent": 2.8,
    "ai_audience": 4.7,
    "ai_signal": 2.0,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 17.0,
    "reasoning": "The content focuses exclusively on experimentation within agile workflows, emphasizing hypothesis-driven development, iterative testing, and fostering a learning culture. There are no direct or indirect mentions of deployment methodologies, practices, or specific deployment strategies such as blue-green deployments, canary releases, feature toggles, or continuous deployment. Conceptual overlap is minimal: while experimentation can play a supporting role in validating deployment approaches, the discussion remains firmly in the realm of product and process innovation rather than the strategic or practical aspects of deploying software. The audience is partially aligned (technical teams in agile contexts), but the core subject matter is not about deployment strategies themselves. No evidence of obsolete practices or contradictory tone, so no penalties were applied. The final confidence is low, reflecting very weak fit with the deployment strategies category.",
    "level": "Ignored"
  },
  "Agentic Agility": {
    "resourceId": "Experimentation",
    "category": "Agentic Agility",
    "calculated_at": "2025-05-06T20:57:12",
    "ai_confidence": 61.1,
    "ai_mentions": 1.2,
    "ai_alignment": 8.8,
    "ai_depth": 7.6,
    "ai_intent": 8.1,
    "ai_audience": 8.5,
    "ai_signal": 8.9,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 61.0,
    "reasoning": "The content thoroughly discusses the practice of experimentation within agile workflows, emphasizing intentional adaptation, empirical learning, and the importance of empowering teams to innovate and respond to change. These themes demonstrate strong conceptual alignment with Agentic Agility, particularly regarding adaptability, delivering value, and fostering autonomy within teams. The piece goes beyond surface-level discussion by highlighting the shift towards a culture of learning and adaptability, which are core to agentic principles. However, direct mention of 'agency,' 'agentic agility,' or explicit references to autonomy and accountability is missing; the category is only implicitly present. Thus, the 'Direct Mentions' score is low. The rest of the scores (alignment, depth, intent, audience, signal) are high—reflecting solid relevance, purpose fit, appropriate targeting towards agile practitioners/organizations, and a high ratio of pertinent information. No penalties applied, as the content is timely, non-contradictory, and aligned in tone and framing.",
    "level": "Secondary"
  },
  "Agile Values and Principles": {
    "resourceId": "Experimentation",
    "category": "Agile Values and Principles",
    "calculated_at": "2025-05-06T20:57:06",
    "ai_confidence": 76.5,
    "ai_mentions": 3.2,
    "ai_alignment": 8.1,
    "ai_depth": 8.7,
    "ai_intent": 8.6,
    "ai_audience": 8.0,
    "ai_signal": 8.2,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 76.0,
    "reasoning": "The content centers on experimentation in Agile workflows, focusing on hypothesis-driven approaches and empirical evidence—a concept that aligns with Agile's values of adaptability, learning, and continuous improvement. The content thoroughly discusses the impact of experimentation on team culture, collaboration, and customer alignment, indicating strong conceptual alignment and depth. However, there are few direct mentions of 'Agile Values' or explicit references to the Agile Manifesto or its principles, resulting in a moderate score for direct mentions. The intent is well aligned with the category's purpose, aiming to foster an Agile mindset rather than focusing purely on technical practices. The audience (those involved in Agile workflows and organizational transformation) matches well. The content is focused and relevant, with minimal off-topic material. No penalties were required as there are no outdated references or divergences from the Agile philosophy. Although experimentation is not named in the core values, the discussion about continuous improvement, learning from failure, and adaptability is fundamentally connected to Agile Values and Principles, resulting in a strong, but not perfect, confidence score.",
    "level": "Secondary",
    "reasoning_summary": "This content is a strong fit for the category, as it explores how experimentation supports Agile values like adaptability and continuous improvement. While it doesn’t frequently reference the Agile Manifesto directly, its focus on learning, collaboration, and customer alignment clearly reflects Agile principles. The discussion is relevant and well-targeted, making it highly suitable, though not perfectly explicit, for the Agile Values category."
  }
}
