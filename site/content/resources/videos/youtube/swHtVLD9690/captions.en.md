I think the most common challenge that organisations face when trying to adopt DevOps practices is regression. I think that's probably the best way to describe it: you make two steps forward and five steps backwards. Part of that is how do we maintain our ability to control risk within this new context because the risk profile is different. 

Because the risk profile is different, we need different tools and techniques to manage that risk. In the old days—I'm calling it the old days; it might be the current days for lots of organisations—but in the old days, we would spend a bunch of time designing our product. We would design the architecture, we would decide what we're going to build, we would list out all of the features, and then we would work towards some kind of release date of our product. 

Once we got close to that release date, once we delivered a body of features that made sense, we were going to test those features. We had a different group of people who were testing and validating those features. I mean, it is still quite common to have separate test teams from engineering teams rather than a combined engineering force. Then, once they had done those tests, it was probably handed off to some kind of operations team who were going to deploy it to an environment. 

Then something like UAT was going to start, where you had some kind of additional validation of what's going on in the product. Once all of those things were successful, then it moved on to maybe deploy to production or staging, or you might have other environments. The traditional, most common model is deploy by environment, and that fundamentally doesn't work within the context of DevOps. It's going to slow you down. 

It's not going to get you the key thing that DevOps is trying to bring to your business: high quality, flexibility, adaptability, and your ability to validate assumptions. We have an assumption that this feature is going to be valuable, but in order to actually validate it, no matter how much stuff we do on paper or in labs or in studies, the only place to really validate that feature is in production. 

A great example of that is Windows 8. Microsoft spent hundreds of millions on user experience, on labs, on flying people into labs to video them using the product, performing certain tasks, and then getting feedback from them in interviews or sitting with people that are using the product. They did all of the things that you're supposed to do, and still, they ended up with a product that bounced off their consumers from a usability perspective. 

That is because of that production problem. One of my favourite people, Brian Harry, who used to run the Azure DevOps team, made this comment that I loved: "There's no place like production." No matter how much testing, no matter how much validation you do—especially if you're in the service world where you're building a service that's got thousands and thousands of customers using the same service—no matter what you do, you cannot simulate production. 

We can maybe do some stuff, but that common challenge is how do we address these things as that world is changing? We've no longer got six months or a year. The Windows team used to be on a six-year delivery; that's how long it would take them to get a new version of Windows out the door, from starting to write the code to it actually being released to production. 

They had six years to do testing. I had a colleague from Boeing, and he talked about one of the things that he saw as a problem: Boeing don't build quality in; they test quality in. You've got all of this aggressive testing that's happening as part of building something, and then you're testing it. "Oh, right, it failed." So what do we have to do differently? 

That is quite often what you have to do with some manufacturing stuff these days. You've got simulators, so you can do a lot more, but in the software world, we don't have to suffer from that problem. We don't have to spend lots of time building a rocket to put it on a rocket pad and launch it, and then it explodes, and we look at the telemetry and figure out what went wrong. 

We don't have to do that. We're building software; we can get that software out the door as quickly as humanly possible with the level of quality that we need to maintain our business brand, protect our business, protect our consumers, and protect our producers. We can get things out the door as quickly as possible, test it in the real world, test it in the market, find out how accepted that thing is in the market, whether it's increasing our market or decreasing our market, and then adapt around that. 

That's part of that DevOps story: closing those feedback loops, not just identifying those feedback loops and eliminating waste in the process to get it to go through the process as quickly as possible, but actually closing the feedback loops, not just collecting the data. 

The two big common challenges that I see are, one, closing the feedback loops, and the other one is actually getting how do we change the way we understand our risk profile and how we mitigate and organise around risk within that context. Then, how do we adapt to the things that we see in a timely manner? Those are the two most common challenges, and those are the things that we can help teams, products, and your organisation deal with and figure out how to do better.