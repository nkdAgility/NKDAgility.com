{
  "Lean": {
    "resourceId": "ffJaR9AaTl7",
    "category": "Lean",
    "calculated_at": "2025-08-07T09:25:30",
    "ai_confidence": 17.405,
    "ai_mentions": 1.2,
    "ai_alignment": 2.9,
    "ai_depth": 3.5,
    "ai_intent": 2.8,
    "ai_audience": 3.5,
    "ai_signal": 2.7,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 17.0,
    "reasoning": "The content provides a thoughtful analysis of human and AI agency in systems, emphasizing the distinction between strategic adaptation (humans) and tactical optimization (AI). While 'optimisation' and efficiency are discussed, there are no explicit or implicit references to Lean methodologies, Lean principles, or its key topics such as waste reduction, value stream mapping, Lean tools, or the Toyota Production System. The discussion centers on accountability, governance, and the division of high-level strategic vs. operational roles, but does not connect these themes back to Lean thinking or practice. The audience (leaders, strategists) loosely overlaps with typical Lean practitioners, but the topic is broader and unconcerned with Lean as a framework. Overall, this is largely unrelated content with limited conceptual overlap and no direct alignment.",
    "reasoning_summary": "The content discusses human vs. AI roles in adaptive systems, focusing on strategy and accountability. It does not reference Lean principles, tools, or methodologies; thus, the fit to 'Lean' is minimal and mostly coincidental.",
    "level": "Ignored"
  },
  "Engineering Excellence": {
    "resourceId": "ffJaR9AaTl7",
    "category": "Engineering Excellence",
    "calculated_at": "2025-05-08T08:55:02",
    "ai_confidence": 38.4,
    "ai_mentions": 1.1,
    "ai_alignment": 4.0,
    "ai_depth": 3.8,
    "ai_intent": 3.1,
    "ai_audience": 3.7,
    "ai_signal": 4.2,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 38.0,
    "reasoning": "The content primarily addresses the distinction between human and AI agency within adaptive systems, focusing on strategic vs. tactical roles. Its exploration of decision making, accountability, and the boundaries between human and machine is conceptually related to effective system design and responsible use of AI. However, it does not explicitly reference engineering excellence, software craftsmanship, or specific development practices such as coding standards, testing, CI/CD, technical debt, or engineering metrics. Most of the discussion orbits organisational adaptation and ethical considerations rather than promoting or deeply discussing best practices in software engineering or craftsmanship. The article targets a mixed audience, possibly including technical leaders, but spends most of its effort on philosophy of agency rather than concrete engineering practices. Therefore, direct mentions, alignment, depth, and intent scores are all low-to-moderate. The signal-to-noise is slightly higher because the content is tightly focused on its topic, but this topic isn't engineering excellence per se. No penalties are applied, as the content is current and does not undermine the category; its issue is one of alignment rather than contradiction.",
    "level": "Ignored"
  },
  "Technical Leadership": {
    "resourceId": "ffJaR9AaTl7",
    "category": "Technical Leadership",
    "calculated_at": "2025-05-08T08:55:02",
    "ai_confidence": 48.2,
    "ai_mentions": 2.6,
    "ai_alignment": 5.7,
    "ai_depth": 5.2,
    "ai_intent": 5.3,
    "ai_audience": 6.1,
    "ai_signal": 6.0,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 48.0,
    "reasoning": "The content focuses primarily on distinguishing human and AI agency in adaptive systems, emphasizing the critical role of human-driven strategic intent, ethical stewardship, accountability, and system direction. It draws a firm line between human leadership functions and the tactical optimization capabilities of AI. While 'leadership' is mentioned directly once (in 'abdicated leadership'), there are no explicit, repeated references to 'technical leadership,' and key terms such as 'mentoring,' 'agile,' 'DevOps,' 'agile ceremonies,' or 'team dynamics' do not appear. The discussion is conceptually adjacent to technical leadership, particularly in its treatment of accountability, strategy, and decision-making boundaries in technical systems, which partially overlaps with the evaluated category. However, it largely lacks in-depth engagement with the practices, processes, and team-oriented aspects that define technical leadership in an agile context. The primary audience appears to be technical or organizational decision-makers, which is loosely aligned with the technical leadership audience. Signal-to-noise is moderate, as the content stays thematically consistent but does not directly address technical leadership at the level of practice or team guidance. No penalties are warranted, as the content is neither outdated nor contrary in tone.",
    "level": "Tertiary"
  },
  "Leadership": {
    "resourceId": "ffJaR9AaTl7",
    "category": "Leadership",
    "calculated_at": "2025-05-08T08:55:02",
    "ai_confidence": 81.37,
    "ai_mentions": 6.6,
    "ai_alignment": 8.7,
    "ai_depth": 8.1,
    "ai_intent": 8.4,
    "ai_audience": 7.5,
    "ai_signal": 7.6,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 81.0,
    "reasoning": "The content explicitly references leadership (including one direct link to the leadership category) and strongly aligns conceptually: it argues that strategic agency, ethical stewardship, and accountability—hallmarks of leadership—are irrevocably human roles, and must not be delegated to AI. Depth of discussion is high: it deeply explores the delineation of human (leadership) vs AI (tactical) agency, discusses risks to strategic sensing, accountability, and gives practical guidance for operational boundaries. The overall intent is to warn and instruct leaders about the perils of abdicating leadership roles to AI, fitting the category well. The intended audience is executives, strategists, and decision-makers in adaptive/agile environments, closely matching the category, though it's broad enough to include technical practitioners as well. The signal-to-noise ratio is robust; content is focused, with only minimal filler. No penalties were applied: the tone is serious, not outdated, not critical of leadership as a discipline, and references are current.",
    "level": "Primary",
    "reasoning_summary": "This content is a strong fit for the leadership category, as it directly addresses leadership concepts and responsibilities. It explores the unique human aspects of leadership—such as strategic decision-making, ethics, and accountability—and cautions against delegating these to AI. The discussion is in-depth, practical, and clearly aimed at leaders and decision-makers, making it highly relevant and focused for the intended audience."
  },
  "Scrum": {
    "resourceId": "ffJaR9AaTl7",
    "category": "Scrum",
    "calculated_at": "2025-08-07T06:10:07",
    "ai_confidence": 9.42,
    "ai_mentions": 0.2,
    "ai_alignment": 1.5,
    "ai_depth": 1.6,
    "ai_intent": 1.0,
    "ai_audience": 2.0,
    "ai_signal": 2.0,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 9.0,
    "reasoning": "The content is a conceptual exploration of human vs. AI agency in adaptive systems. It focuses on the boundaries between strategic (human) and tactical (AI) roles, governance, ethical judgment, and system adaptation. However, there are no direct mentions of Scrum, nor does it reference Scrum roles, events, artifacts, or principles found in the Scrum Guide. The themes around adaptation, iterative improvement, and organisational design are tangentially related to adaptive frameworks like Scrum, but no explicit or deep connection is made to Scrum or its implementation. The audience may intersect with strategy and agile professionals but is not specifically targeted at Scrum teams or practitioners.",
    "reasoning_summary": "No direct Scrum references or specific alignment with Scrum principles/roles. Content is generally about human/AI roles in adaptation, not iterative development or empirical process control found in Scrum. Fit with the Scrum category is weak and indirect.",
    "level": "Ignored"
  },
  "Product Management": {
    "resourceId": "ffJaR9AaTl7",
    "category": "Product Management",
    "calculated_at": "2025-05-08T08:55:02",
    "ai_confidence": 67.9,
    "ai_mentions": 2.1,
    "ai_alignment": 7.3,
    "ai_depth": 7.0,
    "ai_intent": 6.1,
    "ai_audience": 7.8,
    "ai_signal": 7.2,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 68.0,
    "reasoning": "The content centers on the role of human versus AI agency in adaptive systems, emphasizing strategic intent, direction-setting, and organizational adaptation—concepts resonant with product management, particularly at the strategic level (product vision, business alignment, leadership, and ethical stewardship). There are strong conceptual overlaps: the distinction between strategy (human) and tactics (AI) mirrors the product manager's responsibility to define vision, direction, and prioritize higher-order goals while relying on other tools (potentially AI) for optimization. The table mapping layers of decision-making and the discussion of accountability reference key product management concerns. However, the article never explicitly mentions 'product management,' nor does it discuss core frameworks (Agile, Scrum, Lean), techniques for customer feedback, nor does it provide actionable product management methodologies or metrics. Mentions are limited to indirect strategic themes, not explicit category references. The depth score is moderate: ideas of adaptation, decision boundaries, and organizational consequences are discussed in detail, but from a systems/leadership perspective rather than classic product management. The intent is moderately aligned, aiming to shape thinking about strategic roles in organizational change, which is relevant to product management, though not exclusive to it. The audience appears to be senior leaders, product strategists, or system designers; this overlaps, but isn't perfectly tailored to product management roles. The signal-to-noise ratio is strong: content is dense and purposeful, though some themes could be viewed as general leadership or systems thinking rather than product management-focused. No penalties are warranted: the article is current, serious in tone, and does not contradict the category. The final confidence reflects significant conceptual alignment and relevance, but is held back by limited direct mentions and partial audience/intent fit.",
    "level": "Secondary"
  },
  "DevOps": {
    "resourceId": "ffJaR9AaTl7",
    "category": "DevOps",
    "calculated_at": "2025-05-08T08:55:02",
    "ai_confidence": 23.02,
    "ai_mentions": 0.2,
    "ai_alignment": 2.4,
    "ai_depth": 2.1,
    "ai_intent": 1.5,
    "ai_audience": 2.6,
    "ai_signal": 2.4,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 23.0,
    "reasoning": "The content focuses on the delineation between human and AI agency in adaptive systems, emphasizing strategic versus tactical decision-making and the risks of overdelegating adaptation to AI. There are no direct or indirect mentions of DevOps, nor do the main ideas clearly map to key DevOps principles such as cross-functional collaboration, automation for delivery, shared accountability between dev and ops, or continuous improvement in software delivery pipelines. While the article discusses accountability, adaptation, and operational discipline, these are approached from an AI-human governance perspective rather than the DevOps cultural and process integration context. The intended audience appears to be technology strategists or organisational leaders interested in AI governance, not DevOps practitioners. The signal-to-noise ratio is moderate; content remains on topic for its own aims, but almost none of the substance aligns with DevOps as defined. No penalties are applied as there are no outdated ideas or critical/satirical tone toward DevOps.",
    "level": "Ignored"
  },
  "Kanban": {
    "resourceId": "ffJaR9AaTl7",
    "category": "Kanban",
    "calculated_at": "2025-08-07T09:25:32",
    "ai_confidence": 2.536,
    "ai_mentions": 0.0,
    "ai_alignment": 1.2,
    "ai_depth": 2.4,
    "ai_intent": 2.1,
    "ai_audience": 1.8,
    "ai_signal": 1.7,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 3.0,
    "reasoning": "The content focuses on human vs. AI agency in adaptive systems, with strategy and optimisation as distinct roles. There are no direct mentions or discussions of Kanban, its practices, or principles. Key Kanban concepts like workflow, visual boards, WIP limits, flow management, or continuous improvement via Kanban methodology are completely absent. The intent, depth, and audience are aligned to organisational design and adaptive leadership, not Kanban practitioners or theoreticians. While generic ideas of optimisation and adaptation are present, there is no explicit or implicit link to Kanban frameworks or their application.",
    "reasoning_summary": "This content does not fit the Kanban category. It lacks any mention or discussion of Kanban methods, practices, audiences, or principles. The topic is unrelated.",
    "level": "Ignored"
  },
  "Product Development": {
    "resourceId": "ffJaR9AaTl7",
    "category": "Product Development",
    "calculated_at": "2025-05-08T08:55:02",
    "ai_confidence": 42.577,
    "ai_mentions": 0.6,
    "ai_alignment": 5.8,
    "ai_depth": 4.2,
    "ai_intent": 6.1,
    "ai_audience": 5.4,
    "ai_signal": 5.3,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 43.0,
    "reasoning": "The content primarily discusses the distinction between human and AI agency in adaptive systems, focusing on strategic intent (human) versus tactical optimisation (AI). There are indirect overlaps with product development, particularly regarding adaptation, accountability, and system governance, but these are not explicitly framed within the methodologies or practices core to product development. 'Product' or 'Product Development' is never directly mentioned (mentions: 0.6). The conceptual alignment is moderate (5.8) as the principles of strategy, adaptation, and continuous improvement echo some agile and product thinking. The depth is low to moderate (4.2) because the content is thorough in discussing agency but does not connect with product development practices or lifecycle specifics. Intent is mostly to clarify the boundary between AI and human agency for system resilience rather than to educate or guide in product development directly (6.1). The audience leans toward strategists and system designers, potentially adjacent to product leaders, but is broader than typical product development teams (5.4). Signal-to-noise is moderate (5.3) as the entire text revolves around agency models rather than methodologies or best practices for delivering valuable software products. No penalties were needed: there's no outdated info, and the tone is earnest and not critical of product methodologies. Overall, while there are some transferable principles, the content does not sufficiently align with product development to warrant a higher confidence.",
    "level": "Tertiary"
  },
  "Company as a Product": {
    "resourceId": "ffJaR9AaTl7",
    "category": "Company as a Product",
    "calculated_at": "2025-05-08T08:55:02",
    "ai_confidence": 53.43,
    "ai_mentions": 0.85,
    "ai_alignment": 5.85,
    "ai_depth": 6.4,
    "ai_intent": 5.3,
    "ai_audience": 6.05,
    "ai_signal": 7.0,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 53.0,
    "reasoning": "The content centers around the interplay of human and AI agency in adaptive systems, focusing sharply on organisational adaptation, strategy, and accountability—concepts that do intersect with the principles behind 'Company as a Product' (CaaP), especially regarding organisational evolution and governance mechanisms. There are high-depth discussions on roles and boundaries in decision-making, adaptation, and strategic versus tactical thinking. However, the text does not directly mention 'Company as a Product' or CaaP nomenclature or frameworks, and while there are tangential alignments (e.g., cross-functional system design, feedback-driven adaptation, the need for continuous evolution), there are no explicit references to treating the company as a product or to CaaP-specific implementation or culture. Audience is reasonably aligned toward strategic/leadership-level practitioners, as in CaaP. Signal is high—the content is focused and dense, with minimal filler. Scoring low on direct mentions due to the absence of the category term, moderate to strong on alignment/depth due to overlap in ideas and organisational themes, and meaningful but not perfect on intent/purpose fit since the main goal is about agency boundaries in adaptive systems rather than CaaP as such. No outdatedness or negative tone was found; therefore, no penalties were warranted. The overall confidence reflects that while there is high thematic overlap and relevant strategic/organisational insight, CaaP is neither central nor explicitly discussed in methodology or by name.",
    "level": "Tertiary"
  },
  "Agile Values and Principles": {
    "resourceId": "ffJaR9AaTl7",
    "category": "Agile Values and Principles",
    "calculated_at": "2025-05-08T08:55:02",
    "ai_confidence": 38.75,
    "ai_mentions": 0.8,
    "ai_alignment": 4.9,
    "ai_depth": 5.2,
    "ai_intent": 5.8,
    "ai_audience": 6.6,
    "ai_signal": 5.1,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 39.0,
    "reasoning": "The content focuses on the differences between human and AI agency in adaptive systems, emphasizing the importance of strategic human intent in guiding adaptation, with AI serving in a tactical optimization capacity. While there are certain conceptual parallels to Agile values—such as adaptation, accountability, sense-making, and responsiveness to change—the core Agile philosophy, terminology, and explicit principles are almost entirely absent. There are no direct mentions of 'Agile', the Agile Manifesto, its core values, or principles. Depth of discussion is moderate but remains in the domain of agency boundaries, not Agile. Intent and purpose are aligned toward leadership in complex, adaptive systems, which could resonate with Agile audiences seeking to foster adaptability, ethical stewardship, and resilience. However, the primary audience appears broader (leadership, system designers, technologists—not specifically Agile practitioners), and the framing is about AI governance rather than Agile philosophy. The signal-to-noise ratio is moderate: the piece is coherent and in-depth but the focus is largely outside Agile context. Therefore, the confidence score is low to mid-range because while some themes conceptually overlap, there is insufficient direct evidence and alignment to justify a higher association with Agile Values and Principles.",
    "level": "Ignored"
  },
  "Test Driven Development": {
    "resourceId": "ffJaR9AaTl7",
    "category": "Test Driven Development",
    "calculated_at": "2025-08-07T09:25:33",
    "ai_confidence": 6.0,
    "ai_mentions": 0.2,
    "ai_alignment": 0.1,
    "ai_depth": 0.2,
    "ai_intent": 0.1,
    "ai_audience": 0.0,
    "ai_signal": 0.0,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 6.0,
    "reasoning": "The content does not mention Test Driven Development (TDD) directly or indirectly. It focuses exclusively on the distinction between human and AI agency in adaptive systems, discussing strategy, optimisation, accountability, and collaboration boundaries. There are no references, themes, or practices related to TDD, such as automated unit testing, the Red-Green-Refactor cycle, or integration of test-first methodologies with software development. The audience is strategic/leadership and not technical practitioners looking for TDD insights. Signal-to-noise is 0 for TDD, as no part of the content fits the classification definition.",
    "reasoning_summary": "This content is unrelated to Test Driven Development; it exclusively addresses human vs. AI agency in strategy and optimisation, without any references to TDD concepts, practices, or methodologies. No fit for the assigned category.",
    "level": "Ignored"
  },
  "Market Share": {
    "resourceId": "ffJaR9AaTl7",
    "category": "Market Share",
    "calculated_at": "2025-05-08T08:55:02",
    "ai_confidence": 11.58,
    "ai_mentions": 0.3,
    "ai_alignment": 1.6,
    "ai_depth": 1.8,
    "ai_intent": 1.0,
    "ai_audience": 1.2,
    "ai_signal": 0.8,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 12.0,
    "reasoning": "The content focuses on the delineation between human and AI agency in adaptive systems, arguing for the necessity of human strategic oversight and the dangers of over-automation. Nowhere does it explicitly mention market share, strategies for expanding market presence, or competitive metrics. While there are minimal thematic links (such as strategic decision-making and organisational relevance), these are framed in terms of organisational resilience and adaptive capacity rather than approaches to increasing market share or competitive advantage. The main audience appears to be organisational leaders or designers concerned with governance and system adaptability, not specifically those seeking to learn about market share. Any possible connection to market share (e.g., the risk of obsolescence by ignoring human agency) is extremely indirect and not a primary or secondary topic. Thus, scores in all dimensions remain very low, and the total confidence rating appropriately reflects this scarce alignment.",
    "level": "Ignored"
  },
  "Scaling": {
    "resourceId": "ffJaR9AaTl7",
    "category": "Scaling",
    "calculated_at": "2025-05-08T08:55:02",
    "ai_confidence": 11.385,
    "ai_mentions": 0.5,
    "ai_alignment": 1.2,
    "ai_depth": 1.8,
    "ai_intent": 1.4,
    "ai_audience": 2.2,
    "ai_signal": 1.3,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 11.0,
    "reasoning": "The content focuses primarily on the distinction between human and AI agency in adaptive systems, particularly emphasizing the importance of human strategic intent over AI-driven tactical optimization. While it does discuss organizational issues and practices relevant to adaptation and agency, it does not directly mention or address frameworks, methodologies, or challenges specifically associated with Scaling as defined (e.g., cross-team coordination, large-scale Agile, SAFe, LeSS, enterprise-wide flow, or alignment across multiple teams). There are ideas about adaptation, leadership, and system design that have peripheral relevance to topics like governance structures and escalation frameworks, but these remain generic and not explicitly aligned to 'Scaling' as a formal practice or challenge in product and enterprise delivery. There are no direct mentions of scaling practices or frameworks, and the content is not targeting the specific audience of scaling practitioners or leaders managing multiple teams. While the article has depth concerning the agency dichotomy, it is not sufficiently relevant to scaling methodologies. Hence, the overall confidence score is very low, with minor points for some conceptual alignment and organizational-level thinking.",
    "level": "Ignored"
  },
  "Lead Time": {
    "resourceId": "ffJaR9AaTl7",
    "category": "Lead Time",
    "calculated_at": "2025-05-08T08:55:02",
    "ai_confidence": 7.2,
    "ai_mentions": 0.3,
    "ai_alignment": 1.1,
    "ai_depth": 0.7,
    "ai_intent": 1.0,
    "ai_audience": 2.2,
    "ai_signal": 1.1,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 7.0,
    "reasoning": "The content centers around the distinction and interaction between human and AI agency in adaptive systems, focusing on strategic versus tactical roles. There are no explicit or implicit mentions of Lead Time or its associated concepts, such as the measurement of work from initiation to delivery, workflow efficiency, or observability metrics. The main theme is decision-making structure, agency boundaries, risk of overdelegation, and adaptation within organizations, which do not overlap with the Lead Time category or its intended audience (teams tracking delivery efficiency or process bottlenecks). The discussion does reference optimisation and efficiency at a high level, but these references are about general operational improvement rather than time-to-delivery or process metrics. No part of the content addresses measurement techniques, lead/cycle time relationships, or delivery tracking. Thus, all dimensions score very low, with small nonzero values only to reflect minimal topical overlap via generic mentions of optimisation. The confidence score accurately reflects the near-total absence of Lead Time relevance in the material.",
    "level": "Ignored"
  },
  "Acceptance Test Driven Development": {
    "resourceId": "ffJaR9AaTl7",
    "category": "Acceptance Test Driven Development",
    "calculated_at": "2025-08-07T09:25:33",
    "ai_confidence": 0,
    "ai_mentions": 0.0,
    "ai_alignment": 0.2,
    "ai_depth": 0.2,
    "ai_intent": 0.3,
    "ai_audience": 0.6,
    "ai_signal": 0.6,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 0.0,
    "reasoning": "The content is focused on the distinction between human and AI agency in adaptive systems, strategy, and accountability, with no mention or discussion of Acceptance Test Driven Development (ATDD), acceptance criteria, collaborative practices around testing, or any related ATDD principles. No references, explicit or otherwise, to acceptance tests, ATDD roles, tools, or case studies are present. The audience may have technical or strategic overlap, but there is no topical overlap. No penalties were applied as the content does not contradict or reference outdated versions of the category; it is simply unrelated.",
    "reasoning_summary": "The content does not reference, discuss, or align with Acceptance Test Driven Development. There is no mention of acceptance tests, ATDD principles, or collaborative acceptance criteria. Fit is absent.",
    "level": "Ignored"
  },
  "Increment": {
    "resourceId": "ffJaR9AaTl7",
    "category": "Increment",
    "calculated_at": "2025-05-08T08:55:02",
    "ai_confidence": 5.63,
    "ai_mentions": 0.0,
    "ai_alignment": 1.2,
    "ai_depth": 3.0,
    "ai_intent": 1.5,
    "ai_audience": 6.1,
    "ai_signal": 8.7,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 6.0,
    "reasoning": "The content focuses on the distinction between human agency and AI agency in adaptive systems, particularly emphasizing strategy, purpose, and adaptation versus optimization. There are no direct mentions of 'Increment' or discussion related to the delivery of working software increments, Agile, or Scrum practices. Conceptual and intent alignment with the 'Increment' category is minimal, as the main themes revolve around system-level governance and the allocation of agency, rather than iteration-based software delivery. The depth score is slightly higher than alignment due to a thorough exploration of its chosen theme, but that theme is unrelated to 'Increment'. The audience score is somewhat elevated, as it potentially appeals to technical and organizational leaders (common with Increment content), and the content stays focused overall (high signal). No penalties were applied, as the content is not outdated or satirical. The resulting confidence is very low, appropriately reflecting that the material does not fit under the 'Increment' classification.",
    "level": "Ignored"
  },
  "Application Lifecycle Management": {
    "resourceId": "ffJaR9AaTl7",
    "category": "Application Lifecycle Management",
    "calculated_at": "2025-05-08T08:55:02",
    "ai_confidence": 24.225,
    "ai_mentions": 0.6,
    "ai_alignment": 2.9,
    "ai_depth": 2.8,
    "ai_intent": 1.2,
    "ai_audience": 2.3,
    "ai_signal": 2.0,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 24.0,
    "reasoning": "The content focuses on human and AI agency in adaptive systems, discussing boundaries for strategic and tactical decision-making. It primarily centers on organizational strategy, agency, and ethical considerations, not on the practices, methodologies, or tools specific to application lifecycle management (ALM). There are no direct mentions of application lifecycle, ALM stages, tools, or governance frameworks. Conceptual alignment is limited—while the topic touches on system design and governance, it does not cover the software lifecycle or associated management practices. The depth is at the level of agency separation, rather than practical or technical aspects of ALM. The intended audience seems to be strategists and organizational leaders, not specifically ALM professionals, though some overlap with technical governance exists. Signal-to-noise ratio is moderate, with most content focused on the theme but not on ALM, creating some tangential relevance. No penalties apply, as the content is current and respectful. Overall, confidence is low because the discussion is tangential to, but not meaningfully about, Application Lifecycle Management.",
    "level": "Ignored"
  },
  "Platform Engineering": {
    "resourceId": "ffJaR9AaTl7",
    "category": "Platform Engineering",
    "calculated_at": "2025-05-08T08:55:02",
    "ai_confidence": 14.8,
    "ai_mentions": 0.2,
    "ai_alignment": 1.9,
    "ai_depth": 2.3,
    "ai_intent": 2.1,
    "ai_audience": 4.9,
    "ai_signal": 5.6,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 15.0,
    "reasoning": "The content is a thought leadership piece distinguishing between human and AI agency in adaptive systems. There is no direct mention of Platform Engineering or its key concepts—nothing about Internal Developer Platforms, developer self-service, automation in the application lifecycle, or the standardisation of development tools and processes. Alignment is very low, as the content is about abstract decision-making principles, not platform construction or developer enablement. Some concepts such as 'system design' and 'governance practices' could (at a stretch) overlap with the concerns of platform engineers, but the discussion is theoretical, not technical or practical. The intent is to discuss agency and adaptation, not to inform, support, or guide practitioners in Platform Engineering. The technical sophistication and consideration of adaptive systems may resonate slightly with a technical reader, justifying mid/low audience and signal-to-noise scores, but this is peripheral. The overall confidence in this content fitting the Platform Engineering category is very low.",
    "level": "Ignored"
  },
  "Deployment Frequency": {
    "resourceId": "ffJaR9AaTl7",
    "category": "Deployment Frequency",
    "calculated_at": "2025-05-08T08:55:02",
    "ai_confidence": 8.4,
    "ai_mentions": 0.0,
    "ai_alignment": 0.7,
    "ai_depth": 0.8,
    "ai_intent": 0.6,
    "ai_audience": 1.0,
    "ai_signal": 1.2,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 8.0,
    "reasoning": "The content does not directly mention deployment frequency, nor does it discuss deployment intervals, CI/CD, Agile, or DevOps practices. Its focus is on human and AI roles in adaptive systems, emphasising agency, accountability, and strategy versus optimisation. While 'optimisation' is referenced, it is discussed at a philosophical/organizational level around decision-making and adaptation, not in the context of software deployment practices. There is no discussion of release cadence, metrics, automation, or the feedback loop central to deployment frequency. The intent, audience, and depth are not aligned with deployment frequency as a category, resulting in minimum possible scores. The signal-to-noise ratio is slightly higher to reflect that the content is on-topic for its own subject but off-topic with respect to deployment frequency. No penalties were applied, as there is no evidence of outdated information or tone issues regarding deployment frequency; it is simply unrelated. The overall confidence score is extremely low, accurately reflecting that the content does not fit the intended category in any substantive way.",
    "level": "Ignored"
  },
  "Remote Working": {
    "resourceId": "ffJaR9AaTl7",
    "category": "Remote Working",
    "calculated_at": "2025-05-08T08:55:02",
    "ai_confidence": 3.6,
    "ai_mentions": 0.3,
    "ai_alignment": 0.8,
    "ai_depth": 1.0,
    "ai_intent": 0.4,
    "ai_audience": 5.1,
    "ai_signal": 2.0,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 4.0,
    "reasoning": "The content primarily focuses on the roles of human and AI agency within adaptive systems, strategic versus tactical decision-making, and the risks of over-relying on AI for adaptation and leadership. Nowhere does it mention remote working, distributed teams, or Agile practices explicitly or even indirectly; no key terms such as remote, distributed collaboration, or team communication appear. Conceptual alignment with 'Remote Working' is negligible as the themes are about system governance and agency boundaries—not challenges or strategies related to remote Agile team practices. Depth of discussion, while significant for the topic at hand, is absent with regard to the Remote Working category; there is no effort to connect AI/human agency discussion to remote collaboration or virtual teams. The intent is off-purpose for the requested category, as the target audience seems to be strategists or leaders designing adaptive organizations—not specifically remote workers. The audience might, incidentally, overlap with people interested in modern workplace dynamics, but the focus is not on remote work. Signal-to-noise is low with respect to 'Remote Working'; the content is highly focused but not on this category. No penalty is applied, as the content is not outdated nor satirical or critical toward the Remote Working framing—it's just not connected to it. Overall, the confidence is extremely low, as the content does not address or reference the Remote Working category in any meaningful way.",
    "level": "Ignored"
  },
  "Customer Satisfaction": {
    "resourceId": "ffJaR9AaTl7",
    "category": "Customer Satisfaction",
    "calculated_at": "2025-05-08T08:55:02",
    "ai_confidence": 14.85,
    "ai_mentions": 0.2,
    "ai_alignment": 2.0,
    "ai_depth": 2.1,
    "ai_intent": 1.3,
    "ai_audience": 4.3,
    "ai_signal": 1.7,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 15.0,
    "reasoning": "The content, while nuanced and in-depth regarding the distinctions between human and AI agency in adaptive systems, does not directly mention or focus on customer satisfaction. There are no explicit references to customer experience, satisfaction metrics, customer feedback, or practices/mechanisms that connect strategy and optimization to customer happiness—the core of the Customer Satisfaction category. The concept of aligning organization adaptation with external needs could tangentially relate to product-market fit and, by weak extension, customer satisfaction, but this is neither directly stated nor explored. The depth of discussion around 'agency' is strong but not relevant to customer satisfaction principles in Agile, DevOps, or Lean contexts. The audience seems more geared toward system designers, strategists, or leadership, not necessarily those focused on customer-centric methodologies, leading to a moderately low audience alignment score. The signal-to-noise ratio is moderate, as the content remains tightly focused, but the focus is not on the category being assessed. Therefore, with these dimension scores and lack of penalizable factors, the overall confidence this fits the 'Customer Satisfaction' category is very low.",
    "level": "Ignored"
  },
  "Continuous Delivery": {
    "resourceId": "ffJaR9AaTl7",
    "category": "Continuous Delivery",
    "calculated_at": "2025-05-08T08:55:02",
    "ai_confidence": 18.55,
    "ai_mentions": 0.25,
    "ai_alignment": 2.6,
    "ai_depth": 2.75,
    "ai_intent": 2.3,
    "ai_audience": 4.15,
    "ai_signal": 2.1,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 19.0,
    "reasoning": "This content focuses on the distinction between human and AI agency within adaptive systems, emphasizing the irreplaceable role of human strategic intent and the bounded utility of AI in optimization. Nowhere does it directly mention or discuss Continuous Delivery, its principles, practices, benefits, tools, or techniques. The writing is philosophical and governance-oriented, centering on leadership, agency boundaries, and accountability. There are thematic overlaps with operational discipline and systems design; however, these are discussed only in the context of adaptive decision-making—not delivering software in reliable, incremental cycles. The closest alignment comes from the brief discussion of operationalizing boundaries, which could relate to system governance but not specifically to delivery pipelines, automation, or rapid feedback loops central to Continuous Delivery. The primary audience appears to be strategists and executives thinking about digital transformation and responsible AI integration rather than practitioners of software delivery. No references are outdated or satirical, so no penalties applied. Ultimately, the confidence is very low, as the fit is largely tangential and lacks any explicit or substantive engagement with Continuous Delivery.",
    "level": "Ignored"
  },
  "Market Adaptability": {
    "resourceId": "ffJaR9AaTl7",
    "category": "Market Adaptability",
    "calculated_at": "2025-05-08T08:55:02",
    "ai_confidence": 74.56,
    "ai_mentions": 5.2,
    "ai_alignment": 8.8,
    "ai_depth": 8.9,
    "ai_intent": 7.0,
    "ai_audience": 7.6,
    "ai_signal": 7.5,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 75.0,
    "reasoning": "The content centers on the delineation of human versus AI agency in adaptive systems, emphasizing the uniquely human capacity for strategic adaptation and purposeful reframing in complex environments. This directly aligns with the principles of market adaptability—highlighting the necessity for human-driven adaptation, strategic sensing, and ethical stewardship to keep organizations agile and resilient amidst change. The depth is strong: concrete frameworks and detailed risk discussions (e.g., collapse of strategic sensing, fragility under complexity) are provided, showing thoughtful engagement with adaptation and resilience. Direct mentions of 'market adaptability' or related jargon (agile, devops, lean) are limited (thus a middling Mentions score), but repeated reference to 'adaptive systems,' 'adaptation,' and organizational relevance anchors it conceptually. The main intent is to caution organizations against misplacing adaptive agency in AI, advocating practices that ensure continuous human-led adaptation—this is highly aligned with the category, though slightly diluted by the broader focus on agency rather than process or methodology, and with some discussion bordering on philosophical. The intended audience appears to be organizational strategists, system designers, and leaders, which aligns well with the Market Adaptability category's typical readership. While not all content is strictly procedural or method-based as per category exemplars, the overall focus remains on building organizational capability to adjust to change. No penalties are applied: the content is current, not critical of the adaptability imperative itself, and references no obsolete methods. Confidence is strong but not maximal due to limited explicit category terminology and only moderate discussion of specific methodologies (Agile, DevOps, Lean), though the broad conceptual overlap is evident and substantial.",
    "level": "Secondary",
    "reasoning_summary": "This content fits the Market Adaptability category as it explores how organisations can remain agile and resilient by emphasising human-led adaptation in complex environments. While it leans more on conceptual frameworks than specific methodologies, its focus on strategic sensing, adaptation, and organisational relevance aligns well with the category’s intent, making it valuable for leaders and strategists seeking to enhance adaptability."
  },
  "Miscellaneous": {
    "resourceId": "ffJaR9AaTl7",
    "category": "Miscellaneous",
    "calculated_at": "2025-05-08T08:55:02",
    "ai_confidence": 63.77,
    "ai_mentions": 7.6,
    "ai_alignment": 6.7,
    "ai_depth": 5.8,
    "ai_intent": 7.2,
    "ai_audience": 6.3,
    "ai_signal": 5.9,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 64.0,
    "reasoning": "The content does not reference or apply Agile, Scrum, DevOps, Lean, or Evidence-Based Management principles, thus avoiding exclusion per the strict guidelines. It deals with the philosophical and practical interplay between human and AI agency in adaptive systems. The content is predominantly general and theoretical in nature, mapping roles and boundaries but not anchoring them in any established business agility framework or author. It fits 'Miscellaneous' because it is a broad exploration disconnected from specific actionable methodologies or recognised theories, instead offering perspectives, warnings, and a conceptual framework. Mentions of the category occur implicitly through generic and boundary-spanning discussion (7.6), while alignment is moderate (6.7) as the themes fit the definition but occasionally verge into system design which could brush on broader management topics. Depth is adequate (5.8), exploring the subject beyond a superficial level via multi-layer mapping and risk breakdown, but not thoroughly embedding it in Miscellaneous themes. Intent is solidly relevant (7.2), offering discussion most useful to strategy-thinkers or leadership reflecting on human/AI balance, not technical practitioners. Audience fit is moderate (6.3) as the targets are broad (executive, strategist, thought leader), though not narrowed to technical agility professionals. Signal-to-noise is fair (5.9); while the narrative is dense and relevant, semi-redundant cautionary points and didactic repetition weaken its focus on a single topic. No penalties were necessary as content is current, not satirical or critical toward Miscellaneous framing. The scored dimensions and the confidence align: this is a reasonable, broad, somewhat theoretical fit for 'Miscellaneous', neither a perfect exemplar nor inapplicable.",
    "level": "Secondary"
  },
  "Cell Structure Design": {
    "resourceId": "ffJaR9AaTl7",
    "category": "Cell Structure Design",
    "calculated_at": "2025-05-08T08:55:02",
    "ai_confidence": 18.95,
    "ai_mentions": 0.1,
    "ai_alignment": 2.8,
    "ai_depth": 2.5,
    "ai_intent": 2.0,
    "ai_audience": 5.2,
    "ai_signal": 3.6,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 19.0,
    "reasoning": "The content does not directly mention Cell Structure Design, the Beta Codex, autonomous cells, decentralisation, or network-based structures. The central discussion is about human vs AI agency in adaptive (organizational) systems, which conceptually overlaps with the kind of distributed agency found in Cell Structure Design, but the fit is indirect. The piece deeply explores the boundaries between human and AI roles in adaptation, highlighting the necessity of human-driven strategy and purpose, but does not tie these concepts to the decentralized, cell-based organizational architecture or its principles. Intent and audience both lean toward strategy, adaptation, and organizational change, but not explicitly toward Cell Structure Design or its practitioner community. The signal is dominated by the agency and adaptation theme, with much less explicit or implicit Cell Structure Design content. There are no penalties applied as the piece is neither outdated nor critical of Cell Structure Design; rather, its relevance is tangential. Overall, the confidence score is low, reflecting the lack of direct mention or substantial alignment with the core category.",
    "level": "Ignored"
  },
  "Change Management": {
    "resourceId": "ffJaR9AaTl7",
    "category": "Change Management",
    "calculated_at": "2025-05-08T08:55:03",
    "ai_confidence": 61.21,
    "ai_mentions": 1.3,
    "ai_alignment": 6.6,
    "ai_depth": 6.7,
    "ai_intent": 6.1,
    "ai_audience": 7.4,
    "ai_signal": 8.2,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 61.0,
    "reasoning": "Direct mentions of 'change management' or directly related terms are absent; the primary explicit focus is agency differences (human vs. AI) and their roles in system adaptation. Conceptually, the piece strongly aligns with themes underlying change management—such as strategic adaptation, the centrality of human leadership, the risks of over-reliance on automation for change, and the need for accountability and stewardship—but it frames these through the lens of agency and system design rather than formal change management frameworks or methodologies. The depth of discussion is considerable, going beyond superficiality to give clear insight into decision layers, risks, and operational boundaries, which is related to organizational change and resilience. The content’s intent is partly relevant: it supports meaningful and sustainable adaptation in organisations, which is at the heart of change management, but it does not explicitly aim to provide change management practices, case studies, or tools. Its audience includes strategic leaders and system designers, overlapping with but not exclusive to the change management or Agile leadership community. The signal-to-noise ratio is high; the arguments are focused with little extraneous material, though the lens is mostly about agency, not change management directly. No penalties are applied, as the content is current and not contradictory or satirical. The overall confidence is moderate: the content provides valuable, in-depth, conceptually relevant discussion and actionable boundaries that align with the philosophies of the 'Change Management' category, but lack of explicitness and a direct focus on category-defining practices lower the final confidence.",
    "level": "Secondary"
  },
  "Coaching": {
    "resourceId": "ffJaR9AaTl7",
    "category": "Coaching",
    "calculated_at": "2025-05-08T08:55:03",
    "ai_confidence": 21.25,
    "ai_mentions": 0.2,
    "ai_alignment": 1.3,
    "ai_depth": 2.1,
    "ai_intent": 1.5,
    "ai_audience": 2.9,
    "ai_signal": 3.1,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 21.0,
    "reasoning": "The content focuses on the distinctions between human and AI agency in adaptive systems, emphasising strategic versus tactical roles, the dangers of overdelegating to AI, and the importance of human-led decision-making. There is no explicit mention of coaching or its key elements (guidance, unlocking potential, fostering team growth, or coaching practices). Conceptual alignment is minimal: while human stewardship and facilitation are discussed, they are positioned at a leadership/strategic decision-making level—not in the context of developing individuals or teams via coaching. The depth of the discussion is substantial regarding agency, but not regarding coaching; relevant coaching concepts (feedback, trust, collaboration, frameworks like GROW) are absent. The intent is broader strategy and risk, not skills development, and it targets strategists or executives more than scrum masters or coaches. Although the content is not off-topic or filler, its focus is not sufficiently aligned with the core coaching definition. Therefore, all scores are low and confidence is proportionately minimal.",
    "level": "Ignored"
  },
  "Current Value": {
    "resourceId": "ffJaR9AaTl7",
    "category": "Current Value",
    "calculated_at": "2025-05-08T08:55:03",
    "ai_confidence": 9.593,
    "ai_mentions": 0.2,
    "ai_alignment": 1.3,
    "ai_depth": 1.4,
    "ai_intent": 1.0,
    "ai_audience": 2.0,
    "ai_signal": 2.3,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 10.0,
    "reasoning": "The content does not explicitly reference Current Value or Evidence-Based Management. There are no direct mentions, nor does the focus align with Current Value's metrics, measurement, or application in Agile or DevOps. The main ideas center on distinguishing human and AI agency in adaptive systems, strategic versus tactical roles, and organisational risk when over-automating adaptation. Although tangential connections could be drawn—such as the impact of agency boundaries on organisational value—these remain theoretical and do not address practical measurement or real-time assessment, which are core to the Current Value category. The audience appears to be organisational leaders interested in AI strategy, not those directly focused on evidence-based value measurement. Signal-to-noise is low for this category: the content is focused, but not on Current Value. No penalties were applied as there are no outdated references or contradictions; the tone is contemporary and earnest. Overall, this content does not substantively or directly address Current Value, leading to an extremely low confidence score per classification guidelines.",
    "level": "Ignored"
  },
  "Organisational Change": {
    "resourceId": "ffJaR9AaTl7",
    "category": "Organisational Change",
    "calculated_at": "2025-05-08T08:55:03",
    "ai_confidence": 61.178,
    "ai_mentions": 4.2,
    "ai_alignment": 7.6,
    "ai_depth": 6.9,
    "ai_intent": 6.4,
    "ai_audience": 7.0,
    "ai_signal": 6.3,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 61.0,
    "reasoning": "The content does not directly mention 'organisational change' or classic change management frameworks, hence the low Direct Mentions score. However, Conceptual Alignment is moderately strong (7.6), as the discussion relates to governance, strategic leadership, operational discipline, and resilience in organisations—core themes of organisational change—as applied to the balance of human and AI agency. The Depth of Discussion (6.9) reflects thorough exploration of boundaries and operationalising agency, but it doesn't delve deeply into established change frameworks or Agile transformation. Intent/Purpose Fit (6.4) is solid; although the piece focuses on adaptive systems, its intent includes shaping organisational practices and cultural approaches to human-AI collaboration rather than pure theory or technicalities. The Audience score (7.0) is above average, targeting leaders, strategists, and those responsible for organisational adaptation—matching the category's typical audience. Signal-to-Noise (6.3) is slightly above average: almost all content is on-topic regarding adaptation in organisations, but part of it is conceptual and not tightly anchored to mainstream organisational change methodologies or case studies. There are no tone issues or outdated practices, so no penalties were applied. Overall, the content meaningfully overlaps with Organisational Change but from an adjacent, not direct, angle, justifying a confidence well above 50 yet not in the highest band.",
    "level": "Secondary"
  },
  "Lean Product Development": {
    "resourceId": "ffJaR9AaTl7",
    "category": "Lean Product Development",
    "calculated_at": "2025-05-08T08:55:03",
    "ai_confidence": 18.32,
    "ai_mentions": 0.7,
    "ai_alignment": 2.76,
    "ai_depth": 2.65,
    "ai_intent": 2.2,
    "ai_audience": 4.13,
    "ai_signal": 3.07,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 18.0,
    "reasoning": "The content focuses on the roles of human and AI agency in adaptive systems, exploring the boundaries between strategy (human) and optimisation (AI). While there is some discussion of efficiency, experimentation, and systemic adaptation, these are abstracted at the level of system governance rather than the concrete frameworks, principles, or waste-reduction techniques specific to Lean Product Development. There are no direct mentions of Lean, its principles, or tools such as Value Stream Mapping. Conceptual overlap is limited to the general idea of continuous improvement and purposeful adaptation, but the core themes are centered on agency and ethics, not on minimising waste or maximising learning in product creation. The audience may have mild overlap with strategic product leaders interested in Lean, but the content is not tailored to practitioners of Lean Product Development. The signal-to-noise ratio reflects that, while on topic for system design and collaboration, the material is not focused on Lean-specific topics. Therefore, the confidence score remains low, in proportion to the small degree of abstract alignment.",
    "level": "Ignored"
  },
  "Behaviour Driven Development": {
    "resourceId": "ffJaR9AaTl7",
    "category": "Behaviour Driven Development",
    "calculated_at": "2025-05-08T08:55:03",
    "ai_confidence": 7.7,
    "ai_mentions": 0.0,
    "ai_alignment": 1.4,
    "ai_depth": 1.3,
    "ai_intent": 1.0,
    "ai_audience": 2.3,
    "ai_signal": 1.1,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 8.0,
    "reasoning": "The content under review rigorously discusses the distinction between human and AI agency in adaptive systems, emphasizing strategy, purpose, and accountability. Nowhere in the title, description, or main content is Behaviour Driven Development (BDD) mentioned, referenced, or alluded to — scoring 0.0 for direct mentions. Conceptual alignment is minimal (1.4): While the content discusses system design and accountability, there is no substantive overlap with BDD's focus on aligning software requirements, writing user stories, collaborative practices, or BDD tooling. Depth (1.3) is low since the article deep-dives into organisational ethics, strategic adaptation, and AI limits, but not in ways relevant to BDD. The intent (1.0) centers on leadership, human-centric system design, and critical reflection on AI's role, not on BDD or aligning technical and business understanding of software requirements. The audience is potentially technical but more slanted to strategists and leadership (2.3), not BDD's typical developer-tester-business collaboration demographic. The signal-to-noise ratio is low for BDD purposes (1.1) since all content is off-topic relative to the category. No penalties are necessary: the tone is neutral and current. Thus, based on the explicit weighting, the overall confidence that this content is relevant to 'Behaviour Driven Development' is extremely low (7.7), reflecting the tangential or absent fit across all dimensions.",
    "level": "Ignored"
  },
  "Product Discovery": {
    "resourceId": "ffJaR9AaTl7",
    "category": "Product Discovery",
    "calculated_at": "2025-05-08T08:55:03",
    "ai_confidence": 28.65,
    "ai_mentions": 0.35,
    "ai_alignment": 3.35,
    "ai_depth": 2.75,
    "ai_intent": 2.85,
    "ai_audience": 6.2,
    "ai_signal": 6.0,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 29.0,
    "reasoning": "The content focuses on the distinction between human and AI agency in adaptive systems, exploring strategic versus tactical roles in decision-making. While strategic intent, hypothesis framing, and adaptation are somewhat adjacent to product discovery (which involves understanding user needs, defining product features, and validating ideas), there are no direct mentions or methodologies related to Product Discovery (e.g., user research, MVP, customer feedback analysis). The main emphasis is on organisational strategy, adaptation, and governance rather than explicit exploration or definition of product features or customer requirements. The content may interest advanced product leaders or strategists but is not purpose-built for the Product Discovery category. The discussion is deep and well-structured on its core topic but only tangentially overlaps with the conceptual focus of Product Discovery, resulting in a low confidence score.",
    "level": "Ignored"
  },
  "Troubleshooting": {
    "resourceId": "ffJaR9AaTl7",
    "category": "Troubleshooting",
    "calculated_at": "2025-05-08T08:55:03",
    "ai_confidence": 21.488,
    "ai_mentions": 0.5,
    "ai_alignment": 2.3,
    "ai_depth": 2.0,
    "ai_intent": 2.5,
    "ai_audience": 5.7,
    "ai_signal": 3.1,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 21.0,
    "reasoning": "The content is a strategic discussion of the distinction between human and AI agency in adaptive systems, focusing on governance, accountability, and the appropriate assignment of roles. It does not directly reference troubleshooting, nor does it provide methods or case studies for diagnosing or resolving technical issues within software, hardware, or systems. The core themes are about strategic intent and the limits of AI's role in adaptation, not systematic problem identification or resolution. Although there is some relevance for technical practitioners (audience alignment), the alignment and depth scores remain low, as there is little to no direct connection to troubleshooting or detailed exploration of diagnoses or fixes. The mention of brittleness or system failure is philosophical and not actionable troubleshooting content. No penalties were needed, as the content is not outdated or satirical. The confidence score is low, appropriately reflecting minor conceptual overlap but, fundamentally, a misalignment with the Troubleshooting category.",
    "level": "Ignored"
  },
  "Test First Development": {
    "resourceId": "ffJaR9AaTl7",
    "category": "Test First Development",
    "calculated_at": "2025-08-07T11:22:03",
    "ai_confidence": 6.23,
    "ai_mentions": 0.0,
    "ai_alignment": 1.8,
    "ai_depth": 1.4,
    "ai_intent": 1.8,
    "ai_audience": 0.9,
    "ai_signal": 0.8,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 6.0,
    "reasoning": "The content focuses on the distinction between human and AI agency in adaptive systems. There are no direct mentions or clear conceptual links to Test First Development, its principles, testing, success criteria, or related practices. The themes concern organisational strategy, decision-making layers, ethical stewardship, and the risks of overdelegating adaptation to AI; these are not related to defining or verifying success through tests before implementation. No description of TDD, ATDD, acceptance criteria, or feedback loops exists. The subject's intent, depth, and audience are not aligned with the Test First Development category. The signal is low due to irrelevance to testing methodology.",
    "reasoning_summary": "This content does not fit the Test First Development category. Its focus on agency, strategy, and AI roles lacks any conceptual, procedural, or audience overlap with test-first principles or practices.",
    "level": "Ignored"
  },
  "Windows": {
    "resourceId": "ffJaR9AaTl7",
    "category": "Windows",
    "calculated_at": "2025-08-07T11:22:04",
    "ai_confidence": 1.5,
    "ai_mentions": 0.0,
    "ai_alignment": 0.9,
    "ai_depth": 0.7,
    "ai_intent": 2.8,
    "ai_audience": 2.0,
    "ai_signal": 0.9,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 2.0,
    "reasoning": "The content focuses on human and AI agency in adaptive systems, strategic intent, and the theoretical boundary between human-led direction and AI-driven optimisation. There are zero direct mentions of Windows or any Windows-specific concepts. The alignment is extremely weak—while 'optimisation' is discussed, it's in the context of organisational and adaptive strategy, not Windows OS. Depth remains low as none of the explored themes tie into Windows installation, configuration, troubleshooting, or management. The audience appears strategic/leadership-oriented and not Windows practitioners. Intent and signal scores are minimally positive only because the optimisation theme theoretically appears (but unrelated to Windows). No penalties were needed as the content is not outdated or satirical, just off-topic.",
    "reasoning_summary": "This content is unrelated to Windows. It addresses human/AI agency and adaptation strategy with no ties to Windows OS, its management, or its ecosystem. There is no meaningful category fit.",
    "level": "Ignored"
  },
  "Large Scale Agility": {
    "resourceId": "ffJaR9AaTl7",
    "category": "Large Scale Agility",
    "calculated_at": "2025-05-08T08:55:03",
    "ai_confidence": 47.82,
    "ai_mentions": 1.1,
    "ai_alignment": 4.8,
    "ai_depth": 5.3,
    "ai_intent": 5.6,
    "ai_audience": 6.7,
    "ai_signal": 7.4,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 48.0,
    "reasoning": "The content discusses the distinction and interplay between human agency (strategy, adaptation) and AI agency (tactical optimisation) in adaptive systems. It repeatedly references organisational adaptation, accountability, system design, and governance—but does not use explicit language or frameworks central to 'Large Scale Agility' (such as SAFe, LeSS, enterprise transformation, cross-team collaboration, etc.). While the focus on organisational adaptation and human leadership has thematic overlap with values in large-scale agility (e.g., leadership stewardship, strategic pivoting, adaptive governance), it lacks direct citations of the agile scaling domain. There is some conceptual alignment in the emphasis on strategic intent, anti-fragility, responsible adaptation, and clear roles, but it stops short of connecting these specifically to enterprise-level Agile practices, transformation strategies, or scaled Agile frameworks. Its depth lies in strategic vs. tactical decision analysis, but not in large-scale Agile implementation or culture. The intent is relevant for leaders examining adaptation and accountability—one of the key audiences for large-scale agility—but remains broader, appealing to general organisational and systems thinkers rather than specifically those engaged with scaling Agile. High signal-to-noise ratio: the argument is focused and relevant, though not tailored to Agile context. No penalties apply; it is neither outdated nor oppositional. Overall confidence is moderate, as the content is tangentially relevant to topics like leadership and accountability in organisational adaptation but does not directly fit into the strict definition or key topics of 'Large Scale Agility'.",
    "level": "Tertiary"
  },
  "Agile Product Operating Model": {
    "resourceId": "ffJaR9AaTl7",
    "category": "Agile Product Operating Model",
    "calculated_at": "2025-05-08T08:55:03",
    "ai_confidence": 23.837,
    "ai_mentions": 1.2,
    "ai_alignment": 3.7,
    "ai_depth": 2.9,
    "ai_intent": 2.9,
    "ai_audience": 5.4,
    "ai_signal": 5.8,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 24.0,
    "reasoning": "The content deeply explores the distinction between human and AI agency in adaptive systems, focusing on the strategic versus tactical roles of each. While there are conceptual elements (e.g., governance, system design, operational discipline, continuous adaptation) that tangentially relate to themes within the Agile Product Operating Model (APOM), the explicit focus is not on APOM, nor on transitioning from project to product, product-oriented mindset, Scrum, or product management principles central to the category. There is no direct mention or reference to 'Agile Product Operating Model', agile methodologies, product management, or their hybridised structure in APOM. The main intent is not to discuss APOM or its audience (e.g., agile product leaders, strategists), but rather to warn organisational leaders and practitioners about over-reliance on AI for adaptation. There are minor alignments in governance and organisational design discourses, which support a moderate audience and signal score, but the overall confidence must remain low, as the themes and purpose only occasionally intersect with APOM concerns and do not offer detailed discussion or practical guidance in this domain.",
    "level": "Ignored"
  },
  "Hybrid Agile": {
    "resourceId": "ffJaR9AaTl7",
    "category": "Hybrid Agile",
    "calculated_at": "2025-05-08T08:55:03",
    "ai_confidence": 14.7,
    "ai_mentions": 0.2,
    "ai_alignment": 1.3,
    "ai_depth": 1.5,
    "ai_intent": 1.4,
    "ai_audience": 4.6,
    "ai_signal": 6.5,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 15.0,
    "reasoning": "The content focuses on the distinction between human and AI agency within adaptive systems, emphasizing the boundaries and risks associated with overdelegating adaptive work to AI. Nowhere does it mention Hybrid Agile by name or synonym, nor does it explicitly address project management, blending of agile/traditional methodologies, or the dysfunctions of Hybrid Agile. There is some conceptual overlap in highlighting risks of accountability erosion and system-level dysfunctions, which could loosely relate to critiques of compromised frameworks like Hybrid Agile, but this is indirect at best. The primary audience appears to be organizational strategists, somewhat overlapping with the intended Hybrid Agile readership. The discussion is in-depth regarding its chosen topic, but that topic is not Hybrid Agile or its core analytic concerns. There is no content that would merit penalty for being outdated or contradictory in tone—the material is relevant and clearly against certain misuses of automation, but not specifically framed against Hybrid Agile. Therefore, confidence is low and appropriately reflects the lack of direct relevance.",
    "level": "Ignored"
  },
  "Organisational Physics": {
    "resourceId": "ffJaR9AaTl7",
    "category": "Organisational Physics",
    "calculated_at": "2025-05-08T08:55:03",
    "ai_confidence": 91.8,
    "ai_mentions": 7.2,
    "ai_alignment": 9.6,
    "ai_depth": 9.4,
    "ai_intent": 9.1,
    "ai_audience": 8.7,
    "ai_signal": 9.0,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 92.0,
    "reasoning": "The content discusses the strategic role of human versus AI agency specifically within adaptive systems and organisational contexts. It maps agency to layers of organisational decision-making, references system adaptation, feedback, accountability, and complexity, all central to Organisational Physics. Systems thinking is implied through references to adaptation, feedback loops, and the interplay between strategy, decision layers, and organisational resilience. Depth is high due to nuanced differentiation between human and AI roles, examples of operationalising these boundaries, and a thorough risk discussion around overdelegating adaptation tasks. The intent is clearly aligned: the purpose is to inform, caution, and provide guidance to organisational leaders and strategists about systemic boundaries and dynamics, precisely the target audience. Audience alignment is strong, aimed at those responsible for governance, leadership, and organisational systems design. While 'Organisational Physics' is not named directly, there are multiple explicit references to core concepts (systems thinking, adaptation, dynamics). Signal is very high—minimal filler—almost all content is on-topic, only minor tangents (such as general remarks on AI) slightly reduce perfect focus. No outdated or contradictory material is present, so no penalties apply. Overall, the document demonstrates deep, practical engagement with Organisational Physics themes.",
    "level": "Primary",
    "reasoning_summary": "This content is a strong fit for the category, as it explores how human and AI agency interact within adaptive organisational systems—a core concern of Organisational Physics. It uses systems thinking concepts like feedback, adaptation, and decision layers, and targets leaders responsible for organisational design. The discussion is nuanced, practical, and highly relevant, with only minor digressions, making it well-aligned with the intended audience and topic."
  },
  "Site Reliability Engineering": {
    "resourceId": "ffJaR9AaTl7",
    "category": "Site Reliability Engineering",
    "calculated_at": "2025-05-08T08:55:03",
    "ai_confidence": 14.35,
    "ai_mentions": 0.9,
    "ai_alignment": 2.8,
    "ai_depth": 2.5,
    "ai_intent": 2.1,
    "ai_audience": 3.0,
    "ai_signal": 2.0,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 14.0,
    "reasoning": "The content extensively discusses human and AI agency in adaptive systems, focusing on the strategic-versus-tactical division of labor in complex sociotechnical environments. However, there are no direct or even implicit references to Site Reliability Engineering (SRE), SRE principles, practices, or terminology—terms like SLO/SLI/SLA, reliability, post-mortem, incident response, or system performance optimization are never used or alluded to. The main ideas revolve around leadership, governance, and accountability at a strategic and adaptation level, rather than the operational reliability, automation, or monitoring concerns at the heart of SRE. The content seems aimed more at executives or strategists thinking about AI integration, rather than SRE practitioners, though there is some technical systems framing. The signal-to-noise ratio for SRE is very low because nearly all substance is off-category. Confidence is thus minimal, reflecting only a faint conceptual overlap regarding resilient systems and operational frameworks.",
    "level": "Ignored"
  },
  "Portfolio Management": {
    "resourceId": "ffJaR9AaTl7",
    "category": "Portfolio Management",
    "calculated_at": "2025-08-07T07:05:45",
    "ai_confidence": 36.814,
    "ai_mentions": 0.2,
    "ai_alignment": 3.4,
    "ai_depth": 3.6,
    "ai_intent": 5.2,
    "ai_audience": 8.0,
    "ai_signal": 6.5,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 37.0,
    "reasoning": "The content discusses human vs. AI roles in adaptive systems, emphasizing strategic human oversight versus tactical AI optimisation. While it references organisational strategy and adaptation, it does not directly mention, frame, or apply Portfolio Management concepts, frameworks, or practices. Themes of accountability, system governance, and strategic intent are present, which partially overlap with portfolio-level concerns, but there is no exploration of portfolio alignment, investment prioritisation, value streams, or methodology. Thus, conceptual alignment and depth remain limited. The intended audience (leaders, strategists) and focus on strategic adaptation offer moderate audience/signal scores, but overall, the fit with Portfolio Management is partial and indirect.",
    "reasoning_summary": "Content focuses on human strategic agency and AI tactical optimisation in adaptive systems. While it covers strategic leadership and adaptation, it does not address portfolio-level management, prioritisation, or value stream optimisation, so fit is partial and indirect.",
    "level": "Ignored"
  },
  "Lean Startup": {
    "resourceId": "ffJaR9AaTl7",
    "category": "Lean Startup",
    "calculated_at": "2025-05-08T08:55:03",
    "ai_confidence": 23.6,
    "ai_mentions": 0.2,
    "ai_alignment": 2.9,
    "ai_depth": 3.4,
    "ai_intent": 2.5,
    "ai_audience": 4.1,
    "ai_signal": 4.2,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 24.0,
    "reasoning": "The content thoroughly explores the distinction between human and AI agency in adaptive systems, focusing on strategy versus optimization. While the notion of experimentation, adaptation, and hypothesis-driven change are discussed, there is no direct or explicit mention of Lean Startup, MVPs, Build-Measure-Learn loops, validated learning, or the Lean methodology. The alignment is weak: although the themes of adaptation, hypothesis-driven exploration, and pivoting are tangentially related to Lean Startup concepts, the core frameworks and language are absent. The depth is moderate as it substantively discusses adaptive practices but not Lean Startup itself. The purpose is broader—informing readers about effective boundaries between human strategic intent and AI optimization—not specifically supporting Lean Startup practice or its audience. The target audience (organizational leaders, strategists, system designers) could include those interested in Lean Startup but is not focused on startup practitioners or innovators using Lean methods. The signal-to-noise ratio is reasonable; most of the content is on-topic for its own theme, but none is distinctly Lean Startup. No penalties were necessary, as the material is current and the tone is not critical of Lean Startup; it simply addresses a different topic. Overall, the lack of direct conceptual overlap, terminology, and clear intent toward Lean Startup justifies a low confidence score.",
    "level": "Ignored"
  },
  "Estimation": {
    "resourceId": "ffJaR9AaTl7",
    "category": "Estimation",
    "calculated_at": "2025-05-08T08:55:03",
    "ai_confidence": 5.35,
    "ai_mentions": 0.2,
    "ai_alignment": 0.4,
    "ai_depth": 0.5,
    "ai_intent": 0.2,
    "ai_audience": 0.15,
    "ai_signal": 0.2,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 5.0,
    "reasoning": "The content focuses on the distinction between human and AI agency in adaptive systems, particularly in strategy versus tactical optimisation. Direct mentions of 'estimation' do not occur; neither does the content discuss Agile or Scrum frameworks, empirical data use, or collaborative estimation practices. There are no references to estimation techniques, velocity, retrospectives, or other Agile estimation concerns. The conceptual themes (human vs. AI agency, accountability in adaptive systems, organisational risk) are largely unrelated to estimation as defined by the category. Any inferred relation to estimation is extremely tangential, possibly surfacing via the discussion of optimisation or decision-making, but never explicitly or substantively. The audience is likely managerial, strategic, or involved in systems thinking—not specifically Agile practitioners seeking to improve estimation. The content is highly focused, but on topics outside the definition of estimation in Agile, so the signal-to-noise ratio for the 'Estimation' category is very low. No penalties were applied, as there is no tone or outdated practice that would justify deduction. The very low score reflects minimal overlap with the target category, per evidence.",
    "level": "Ignored"
  },
  "Agile Planning": {
    "resourceId": "ffJaR9AaTl7",
    "category": "Agile Planning",
    "calculated_at": "2025-05-08T08:55:03",
    "ai_confidence": 25.8,
    "ai_mentions": 0.2,
    "ai_alignment": 2.7,
    "ai_depth": 3.1,
    "ai_intent": 2.8,
    "ai_audience": 5.0,
    "ai_signal": 6.3,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 26.0,
    "reasoning": "The content directly discusses human and AI agency in adaptive systems, primarily focusing on the distinction between strategic (human) and tactical (AI) decision-making. It does not explicitly mention Agile Planning, nor does it reference Agile methodologies, practices, or key concepts like sprints, backlogs, or iteration. There is some loose conceptual connection around adaptation and strategy, which are relevant to Agile thinking, but the main discussion centers on system-level agency boundaries and governance, not on principles or practices of Agile Planning itself. The audience is partially aligned (organizational leaders and system designers, which could include agile practitioners), but the overall focus, depth, and intent are not matched to Agile Planning as strictly defined in the category guidance. Signal-to-noise is moderate, as the discussion is focused but only tangential to Agile Planning. No penalties are applied, as the content is recent, not obsolete, and does not contradict the Agile Planning framing directly. The low confidence score reflects the minimal overlap with the specific concerns and practices of Agile Planning.",
    "level": "Ignored"
  },
  "Product Backlog": {
    "resourceId": "ffJaR9AaTl7",
    "category": "Product Backlog",
    "calculated_at": "2025-05-08T08:55:02",
    "ai_confidence": 2.869,
    "ai_mentions": 0.1,
    "ai_alignment": 0.8,
    "ai_depth": 0.6,
    "ai_intent": 1.2,
    "ai_audience": 0.1,
    "ai_signal": 0.2,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 3.0,
    "reasoning": "The content does not mention or reference Product Backlog at any point. Its primary focus is on differentiating human and AI agency in adaptive systems, exploring boundaries, responsibilities, and risks of overdelegating adaptation to AI. There are no discussions of Agile, Scrum, backlog refinement, roles of Product Owner, user stories, tools for backlog management, or any practices associated with Product Backlog management. Conceptual overlap is minimal: while some organizational and strategic themes are discussed, they are independent from backlog concepts and don't address backlog prioritization, structure, or use cases. No information is present on Product Backlog audiences (Agile/Scrum teams, Product Owners). The content is coherent and in-depth regarding its actual topic, but it is almost entirely noise in relation to Product Backlog. No penalties were applied since the tone is not critical or outdated regarding Product Backlog; it is simply off-topic. The low confidence score reflects the near-total irrelevance to the Product Backlog category.",
    "level": "Ignored"
  },
  "Agile Product Management": {
    "resourceId": "ffJaR9AaTl7",
    "category": "Agile Product Management",
    "calculated_at": "2025-05-08T08:55:03",
    "ai_confidence": 24.66,
    "ai_mentions": 0.3,
    "ai_alignment": 2.8,
    "ai_depth": 3.2,
    "ai_intent": 2.5,
    "ai_audience": 4.2,
    "ai_signal": 2.1,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 25.0,
    "reasoning": "The content makes no direct mention of Agile Product Management, nor does it reference Agile, Scrum, Product Owners, backlogs, or any of the core practices or terminology exclusive to the category. The primary topic centers around the differentiation between human and AI agency within adaptive systems, focusing on strategic versus tactical decision-making—a theme that may conceptually intersect with modern product leadership but is not specifically linked to Agile frameworks or practices. There is substantial depth in discussing what constitutes human versus AI agency, including risks and operational guidelines, but these discussions are positioned at a general systems or organizational level, not in the context of maximizing product value or Agile ways of working. The intent of the content is more philosophical and strategic rather than practical or actionable within Agile Product Management. While the likely audience (executives, strategists, technologists) could overlap with some in Agile leadership, there is no targeting of product management practitioners or those invested in Agile transformation. The content is focused and coherent, but its relevance to 'Agile Product Management' is mainly tangential—any application to that category is interpretative at best and not explicit. No penalties were needed as the content is neither outdated nor satirical. Overall, the confidence score is kept low due to the absence of direct or detailed category relevance.",
    "level": "Ignored"
  },
  "Self Organisation": {
    "resourceId": "ffJaR9AaTl7",
    "category": "Self Organisation",
    "calculated_at": "2025-05-08T08:55:03",
    "ai_confidence": 31.975,
    "ai_mentions": 0.4,
    "ai_alignment": 3.6,
    "ai_depth": 3.9,
    "ai_intent": 2.1,
    "ai_audience": 4.2,
    "ai_signal": 5.3,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 32.0,
    "reasoning": "The content focuses on the division of agency between humans and AI within adaptive systems, centering on the strategic value of human decision-making over tactical AI optimisation. There are no direct or explicit mentions of 'self-organisation' or related terminology (e.g., autonomous teams, Agile, Scrum, empowerment, retrospectives), so Direct Mentions is extremely low. The main thrust is about organisational boundaries and accountability in socio-technical contexts, which has partial conceptual overlap with self-organisation (e.g., emphasis on human adaptation, accountability, autonomy at a high level), but it does not specifically address autonomous teams, collaborative practices, or frameworks that facilitate self-organisation as defined. Depth is moderate since the article explores human vs. AI roles in-depth, but only tangentially echos elements relevant to self-organisation (e.g., accountability, critical adaptation roles), and does not discuss methods or practices central to the category. The intent is not to inform about self-organisation per se, but about responsible governance of human and AI agency, so Intent/Purpose Fit is low. The probable audience (leaders and technical strategists) has some overlap with practitioners interested in self-organisation, but the framing is more about high-level system design than team empowerment, so Audience Alignment is moderate. Signal-to-Noise is above average due to focus and clarity, but not tightly tied to the category. No penalties were necessary; content tone is appropriate and up-to-date. The final confidence score reflects a weak fit—while the content has glancing conceptual resonance with the values of self-organisation, it does not meet the threshold for strong alignment with the provided definition.",
    "level": "Ignored"
  },
  "Agile Philosophy": {
    "resourceId": "ffJaR9AaTl7",
    "category": "Agile Philosophy",
    "calculated_at": "2025-05-08T08:55:03",
    "ai_confidence": 46.75,
    "ai_mentions": 0.9,
    "ai_alignment": 5.5,
    "ai_depth": 6.7,
    "ai_intent": 5.2,
    "ai_audience": 5.6,
    "ai_signal": 6.0,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 47.0,
    "reasoning": "The content deeply explores the contrast and synergy between human and AI agency in adaptive systems, emphasizing the crucial role of human strategic intent, accountability, and adaptation. While the themes of adaptability, continuous improvement, and human-centric decision-making are conceptually adjacent to Agile Philosophy, the piece does not directly mention Agile, the Agile Manifesto, or any of its 12 principles. Explicit references to Agile values, principles, teams, organizational behavior, or Agile cultural shifts are absent. The conceptual overlap lies mainly in the discussion of adaptability, sensemaking, accountability, and human leadership—all qualities foundational to the Agile philosophy, but here they are discussed in the context of AI/human roles, not directly within Agile frameworks or culture. The depth of discussion about human adaptation and purpose is notable, akin to Agile values, but the connection to Agile as a philosophy is implicit and requires interpretive inference. The intent targets organizational leaders and strategists—possibly an Agile-adjacent audience—but stops short of targeting Agile practitioners or those explicitly focused on Agile mindset. The content remains tightly focused on the core comparison of human/AI agency in adaptation, lending it moderate signal-to-noise. Final confidence reflects these conceptual overlaps but appropriately down-weights for the lack of explicit Agile philosophy reference or framing.",
    "level": "Tertiary"
  },
  "Sociotechnical Systems": {
    "resourceId": "ffJaR9AaTl7",
    "category": "Sociotechnical Systems",
    "calculated_at": "2025-05-08T08:55:03",
    "ai_confidence": 93.1,
    "ai_mentions": 7.4,
    "ai_alignment": 9.8,
    "ai_depth": 9.5,
    "ai_intent": 9.2,
    "ai_audience": 9.0,
    "ai_signal": 9.3,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 93.0,
    "reasoning": "This content thoroughly examines the interaction of human and AI agency within adaptive systems, mapping roles in decision-making, strategic vs. tactical boundaries, and accountability—all core themes in sociotechnical systems. While the term 'Sociotechnical Systems' is not used verbatim, the article explicitly discusses the integration of human (social/organisational) and AI (technical) elements in organisational contexts, notably in software adaptation, team responsibility, and governance practices. The depth includes specific risks, responsibilities, a decision flowchart, and prescriptive advice for practitioners and leaders, evidencing significant engagement with sociotechnical theory. The audience is clearly technical leaders and strategists concerned with successful, resilient organisations at the intersection of technology and organisational culture. The signal-to-noise ratio is high: nearly all content relates directly to sociotechnical themes, with only minor tangents. No outdated content or hostile tone was identified, so no penalties were applied. The confidence score reflects both comprehensive exploration and high alignment across all evaluation dimensions.",
    "level": "Primary",
    "reasoning_summary": "This content is a strong fit for the 'Sociotechnical Systems' category, as it explores how human and AI roles interact within organisations, focusing on decision-making, responsibility, and governance. It offers practical guidance and risk analysis, making it highly relevant for technical leaders interested in the integration of social and technical elements in adaptive systems."
  },
  "Internal Developer Platform": {
    "resourceId": "ffJaR9AaTl7",
    "category": "Internal Developer Platform",
    "calculated_at": "2025-08-07T11:22:03",
    "ai_confidence": 5.4,
    "ai_mentions": 0.2,
    "ai_alignment": 1.4,
    "ai_depth": 1.7,
    "ai_intent": 0.9,
    "ai_audience": 0.6,
    "ai_signal": 0.6,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 5.0,
    "reasoning": "The content focuses exclusively on the distinction between human and AI roles in adaptive systems—particularly strategic intent vs. tactical optimisation. It does not reference Internal Developer Platforms (IDPs), their architecture, benefits, DevOps, CI/CD, or platform engineering. While some notions of optimisation and system boundaries echo very distantly to automation themes sometimes seen in platform discussions, this is generic and not substantively related to IDPs. The audience and framing are about high-level organisational leadership and responsible AI, not technical engineers building or running IDPs. Virtually all substance is elsewhere, with no concrete or implied fit to the IDP category.",
    "reasoning_summary": "Content is unrelated to Internal Developer Platforms. Discussion centers on human vs. AI agency in adaptive systems, strategy, and accountability—no mention or thematic connection to IDPs, platform engineering, or related best practices.",
    "level": "Ignored"
  },
  "GitHub": {
    "resourceId": "ffJaR9AaTl7",
    "category": "GitHub",
    "calculated_at": "2025-08-07T11:22:06",
    "ai_confidence": 1.9,
    "ai_mentions": 0.1,
    "ai_alignment": 0.5,
    "ai_depth": 0.5,
    "ai_intent": 0.2,
    "ai_audience": 0.3,
    "ai_signal": 0.2,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 2.0,
    "reasoning": "The content offers an in-depth discussion of human versus AI agency in adaptive systems but does not mention GitHub, its tools, practices, or methodologies. No references, even implicit, to GitHub or version control, repositories, CI/CD, or any related aspect are present. The audience could include GitHub practitioners, but the thematic focus is too broad and abstract for a GitHub-specific classification. The signal is low as nothing references key GitHub functionalities or best practices.",
    "reasoning_summary": "No mention or focus on GitHub, its methodologies, or tools. Content is about human and AI agency in adaptive systems, with no relevance or alignment to the GitHub category.",
    "level": "Ignored"
  },
  "Value Stream Management": {
    "resourceId": "ffJaR9AaTl7",
    "category": "Value Stream Management",
    "calculated_at": "2025-05-08T08:55:03",
    "ai_confidence": 17.55,
    "ai_mentions": 0.25,
    "ai_alignment": 2.5,
    "ai_depth": 2.0,
    "ai_intent": 1.7,
    "ai_audience": 5.4,
    "ai_signal": 4.2,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 18.0,
    "reasoning": "The content focuses on the distinction between human and AI agency in adaptive systems, emphasizing the need for strategy-setting and accountability to remain with humans while AI executes tactical optimisation. This is a foundational organisational design discussion, centering on adaptation, accountability, and effective decision-making, but it does not address Value Stream Management directly or by analogy: there are no explicit mentions of value stream mapping, process optimisation against customer value, waste identification, or aligning flow with business outcomes. The ideas about boundaries and optimisation could tangentially relate to process efficiency, but the text completely lacks reference to value streams or typical VSM techniques. The closest parallel is an abstract argument about the locus of adaptation versus optimisation, but this is not explicitly or implicitly mapped to value flows or streams. The audience (organisational leaders, strategists, or technologists) may overlap with VSM, but the content’s primary focus, terminology, and depth are not aligned. Signal-to-noise is moderate, as the text is focused but not relevant to VSM per se. There is no out-of-dateness, undermining, or satirical tone, so no penalties apply. The low scores in all direct-fit dimensions and moderate audience/signal yield a low overall confidence score, as per weighting.",
    "level": "Ignored"
  },
  "Definition of Done": {
    "resourceId": "ffJaR9AaTl7",
    "category": "Definition of Done",
    "calculated_at": "2025-05-08T08:55:02",
    "ai_confidence": 2.9,
    "ai_mentions": 0.2,
    "ai_alignment": 0.6,
    "ai_depth": 0.5,
    "ai_intent": 1.0,
    "ai_audience": 4.3,
    "ai_signal": 2.8,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 3.0,
    "reasoning": "The content does not reference or discuss the Definition of Done (DoD) or its associated Agile/Scrum practices in any form. 'Definition of Done' and related terminology (e.g., acceptance criteria, product increment, backlog) are never mentioned. The conceptual focus is solely on delineating roles and boundaries between human and AI agency in adaptive systems, emphasizing strategy, purpose, and accountability—none of which are tied to DoD or Agile completion criteria. While the intended audience may slightly overlap with technical or managerial practitioners (hence the modest audience score), the depth is minimal regarding DoD, with all substantive discussion dedicated to system adaptation, ethics, and strategic leadership. The signal-to-noise ratio is low for DoD relevance; almost none of the content pertains to the category. No penalties apply as the content is not outdated or oppositional—just off-topic. Overall, this piece merits an extremely low confidence for inclusion under 'Definition of Done.'",
    "level": "Ignored"
  },
  "Azure DevOps": {
    "resourceId": "ffJaR9AaTl7",
    "category": "Azure DevOps",
    "calculated_at": "2025-09-05T03:28:25",
    "ai_confidence": 7.9,
    "ai_mentions": 0.0,
    "ai_alignment": 1.4,
    "ai_depth": 1.0,
    "ai_intent": 0.6,
    "ai_audience": 2.2,
    "ai_signal": 1.2,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 8.0,
    "reasoning": "The content does not mention Azure DevOps at all, nor any of its tools or practices. It broadly discusses human and AI roles in adaptive systems, strategy, and accountability. While some concepts (optimisation, team decision-making) may touch tangentially on themes applicable to DevOps in general, there are no references to CI/CD, agile management, or Microsoft Azure DevOps services. The audience could include practitioners in DevOps-adjacent fields, but the focus and detail are not on Azure DevOps. Content is primarily conceptual and strategic, not technical or platform-specific.",
    "reasoning_summary": "The content does not mention Azure DevOps or address any of its tools or practices. Thematically, it is only tangentially related to DevOps concepts and does not fit the category definition.",
    "level": "Ignored"
  },
  "Competence": {
    "resourceId": "ffJaR9AaTl7",
    "category": "Competence",
    "calculated_at": "2025-08-07T07:05:49",
    "ai_confidence": 44.35,
    "ai_mentions": 1.7,
    "ai_alignment": 4.4,
    "ai_depth": 5.6,
    "ai_intent": 5.0,
    "ai_audience": 5.0,
    "ai_signal": 6.2,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 44.0,
    "reasoning": "The content explores the distinction between human and AI agency in adaptive systems, focusing on strategy, accountability, and operational boundaries. While it discusses professionalism, ethical stewardship, and human responsibility, it lacks direct, sustained focus on competence and skill development per the category definition. Mentions of stewardship, discernment, and accountability overlap conceptually with competence but the main discussion is on agency, not explicit competence-building, capability development, or ongoing improvement. Limited references to learning, mastery, or skill inspection/adaptation. The audience is broad but includes practitioners and strategists relevant to competence, though intent is mainly to delineate roles/responsibilities rather than develop competence frameworks.",
    "reasoning_summary": "Content focuses on agency and role clarity in adaptive systems, only partially overlapping with competence via mentions of human accountability and stewardship. Fit with 'Competence' is tangential and not a main theme.",
    "level": "Tertiary"
  },
  "Team Motivation": {
    "resourceId": "ffJaR9AaTl7",
    "category": "Team Motivation",
    "calculated_at": "2025-05-08T08:55:03",
    "ai_confidence": 26.3,
    "ai_mentions": 0.6,
    "ai_alignment": 3.2,
    "ai_depth": 2.8,
    "ai_intent": 2.6,
    "ai_audience": 4.2,
    "ai_signal": 3.0,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 26.0,
    "reasoning": "The content focuses on delineating human and AI roles in decision-making within adaptive systems, emphasising the necessity of human agency for strategic intent and ethical stewardship. There is no explicit mention of 'team motivation' or direct references to techniques, strategies, or social dynamics that enhance engagement within agile teams. Conceptually, while the material touches on leadership, accountability, and adaptation, these are addressed in the context of organisational system design and strategy, not in relation to motivating teams or fostering team dynamics. Depth is moderate in its primary topic but remains peripheral to team motivation. The main intent is to guide strategy and agency boundaries, not to provide motivational practices or frameworks for teams. The target audience skews towards organisational leaders and system designers rather than team facilitators or agile practitioners. The content is focused and coherent, though its relevance to 'Team Motivation' is quite tangential overall. Thus, the confidence in this category fit is low and proportionate to the very limited overlap.",
    "level": "Ignored"
  },
  "Daily Scrum": {
    "resourceId": "ffJaR9AaTl7",
    "category": "Daily Scrum",
    "calculated_at": "2025-09-17T23:12:32",
    "ai_confidence": 2.4,
    "ai_mentions": 0.0,
    "ai_alignment": 0.6,
    "ai_depth": 1.2,
    "ai_intent": 2.1,
    "ai_audience": 4.7,
    "ai_signal": 1.3,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 2.0,
    "reasoning": "The content focuses entirely on human and AI agency in adaptive systems, strategic intent, and operational boundaries. There are no mentions of Scrum, Daily Scrum, or agile event practice. While practitioners interested in adaptive systems might overlap with Scrum audiences, there is no direct or indirect fit with the specified Daily Scrum category topics, intents, or structural expectations. No dimension deserved a penalty as the content isn't outdated or satirical; it simply doesn't relate.",
    "reasoning_summary": "No reference to Daily Scrum or Scrum practices. Content is unrelated to category themes, focusing exclusively on AI vs. human roles in adaptation and strategy. Fit is not present.",
    "level": "Ignored"
  },
  "Agentic Agility": {
    "resourceId": "ffJaR9AaTl7",
    "category": "Agentic Agility",
    "calculated_at": "2025-05-08T08:55:03",
    "ai_confidence": 94.36,
    "ai_mentions": 8.4,
    "ai_alignment": 9.7,
    "ai_depth": 9.4,
    "ai_intent": 9.0,
    "ai_audience": 9.2,
    "ai_signal": 9.4,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 94.0,
    "reasoning": "The content is explicitly and deeply focused on the distinctions and interplay between human and AI agency in adaptive (socio-technical) systems, directly referencing key terms and nuances of agentic agility. The term 'agency' is repeatedly and centrally discussed, highlighting both human and AI forms—their roles, boundaries, and strategic vs. tactical functions. The alignment with the Agentic Agility definition is exceptionally strong, especially regarding intentionality, adaptive action, accountability, and outcome alignment. Depth is demonstrated through thorough exploration of agency layers, the risks of overdelegating to AI, and practical strategies for operationalising agency boundaries. The content's primary intent is to inform and guide practitioners—in Agile, DevOps, and adaptive system contexts—about maintaining agentic boundaries and the crucial role of human agency, matching the intended audience of the category. Signal-to-noise ratio is very high; nearly every section elaborates directly on the relevant themes, with minimal tangents or filler. There are no outdated or contradictory references; the tone is urgent but constructive. No penalties were warranted. The final confidence score reflects the exceptional directness, conceptual match, and depth, just shy of perfection in mentions (as 'Agentic Agility' as a phrase is somewhat implicit rather than cited verbatim).",
    "level": "Primary",
    "reasoning_summary": "This content is an excellent fit for the Agentic Agility category. It thoroughly explores the roles and boundaries of human and AI agency in adaptive systems, using relevant terminology and practical examples. The focus on intentionality, accountability, and operational strategies aligns closely with the category’s aims, making it highly valuable for practitioners seeking to understand and apply agentic agility principles."
  },
  "Collaboration Tools": {
    "resourceId": "ffJaR9AaTl7",
    "category": "Collaboration Tools",
    "calculated_at": "2025-08-07T07:05:53",
    "ai_confidence": 8.4,
    "ai_mentions": 0.2,
    "ai_alignment": 1.1,
    "ai_depth": 1.3,
    "ai_intent": 1.5,
    "ai_audience": 1.6,
    "ai_signal": 0.9,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 8.0,
    "reasoning": "There are no explicit references to collaboration tools, platforms, or features. The content is focused on the distinction between human and AI agency in adaptive systems, with emphasis on strategy, accountability, and decision boundaries. While 'collaboration' is implied in the context of human-AI working together, this context falls outside team-oriented Agile collaboration tools. The target audience includes professionals interested in system design or governance, not specifically users of collaboration tools for Agile teams. No discussion of group coordination, tool implementation, or Agile frameworks is present. Therefore, fit to the 'Collaboration Tools' category is extremely weak, with nominal conceptual overlap in operational collaboration but not the category's intent.",
    "reasoning_summary": "The content discusses human and AI roles in adaptive systems but does not address platforms or practices for team collaboration in Agile. No mentions or exploration of collaboration tools; category fit is negligible.",
    "level": "Ignored"
  },
  "Frequent Releases": {
    "resourceId": "ffJaR9AaTl7",
    "category": "Frequent Releases",
    "calculated_at": "2025-10-01T16:42:29",
    "ai_confidence": 12.4,
    "ai_mentions": 0.5,
    "ai_alignment": 1.8,
    "ai_depth": 2.1,
    "ai_intent": 1.2,
    "ai_audience": 4.3,
    "ai_signal": 2.6,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 12.0,
    "reasoning": "The content thoroughly explores human and AI agency in adaptive systems, with extensive discussion of strategy, accountability, adaptation, and tactical optimisation. However, it does not directly discuss frequent releases, continuous delivery, CI/CD, or software release cycles. There are no explicit or significant indirect references to practices, principles, tooling, or metrics related to frequent software delivery. While terms like 'optimisation' and 'experimentation' sometimes appear, they refer to organisational or systemic adaptation, not to software release cadence or deployment processes. The target audience (leaders and system designers in adaptive organisations) is partially overlapping with the 'Frequent Releases' audience, but the thematic focus is out of scope. No penalties applied, as the content is current and not critical; it simply does not fit the category.",
    "reasoning_summary": "This content centers on human vs AI roles in adaptation and optimisation, not on software release frequency. It doesn’t mention or align with frequent releases, continuous delivery, or related practices. The thematic fit is minimal.",
    "level": "Ignored"
  },
  "Sensemaking": {
    "resourceId": "ffJaR9AaTl7",
    "category": "Sensemaking",
    "calculated_at": "2025-08-07T07:05:57",
    "ai_confidence": 93.47,
    "ai_mentions": 8.8,
    "ai_alignment": 9.6,
    "ai_depth": 9.3,
    "ai_intent": 9.1,
    "ai_audience": 8.7,
    "ai_signal": 8.9,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 93.0,
    "reasoning": "The content explicitly references sensemaking as a core activity in adaptive systems and distinguishes its crucial role in complex, uncertain environments. It aligns closely with the category by discussing frameworks for agency and the importance of human-led interpretation and adaptive decision-making amidst complexity. The section 'Collapse of Strategic Sensing' directly discusses sensemaking concepts. The exploration of agency boundaries, accountability, and adaptation thoroughly supports the category's principles and focus. The intended audience of organisational leaders and strategists matches well. While the discussion incorporates related topics such as AI optimisation, these are consistently positioned in relation to—and subordinate to—the sensemaking domain, maintaining a strong signal-to-noise ratio and thematic relevance. No outdated practices or framing contradictions were observed.",
    "reasoning_summary": "Content directly, deeply, and repeatedly discusses sensemaking in organisations’ adaptive responses to complexity. Main intent, conceptual framing, and audience all match the category strongly. Tightly focused on sensemaking’s principles and risks.",
    "level": "Primary"
  },
  "Social Technologies": {
    "resourceId": "ffJaR9AaTl7",
    "category": "Social Technologies",
    "calculated_at": "2025-10-01T16:57:02",
    "ai_confidence": 67.1,
    "ai_mentions": 2.2,
    "ai_alignment": 7.8,
    "ai_depth": 6.7,
    "ai_intent": 7.0,
    "ai_audience": 8.4,
    "ai_signal": 8.8,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 67.0,
    "reasoning": "The content explores agency boundaries between humans and AI in adaptive systems, emphasizing roles, governance, strategy, and accountability. It aligns with social technology themes of self-organisation, responsibility, and decision-making but does not directly reference collaborative frameworks, specific methods, or Agile/DevOps. Depth is moderate, and intent aligns with discourse on organisational adaptability and leadership. Audience is professional/organisational, fitting the category. However, explicit direct mentions of 'social technologies' or frameworks are missing, and discussion of tools/practices is implicit rather than named, keeping direct mentions and depth below maximum.",
    "reasoning_summary": "The content aligns with Social Technologies by discussing roles in collaboration, adaptation, and decision-making but does not reference social tech frameworks directly. Fit is moderate; it covers principles but little on specific practices or methodologies.",
    "level": "Secondary"
  },
  "Product Validation": {
    "resourceId": "ffJaR9AaTl7",
    "category": "Product Validation",
    "calculated_at": "2025-10-01T16:57:02",
    "ai_confidence": 22.8,
    "ai_mentions": 0.5,
    "ai_alignment": 3.2,
    "ai_depth": 2.7,
    "ai_intent": 2.0,
    "ai_audience": 4.2,
    "ai_signal": 3.7,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 23.0,
    "reasoning": "The content focuses on delineating human versus AI agency in adaptive systems, with an emphasis on strategy, purpose, and accountability. Product Validation, per the provided definition, centers on validating product ideas with real users through testing and feedback loops. While the text discusses experimentation and mentions hypotheses and adaptation, it does so in the service of governance, leadership, and strategic alignment, not in validating product ideas with users. Techniques like user testing, prototyping, or market fit are not addressed. The audience is possibly overlapping (leaders, strategists), but the topic diverges from Product Validation's core concerns. While there is a peripheral nod to weak signal exploration and hypothesis-driven approaches, these are not explored through a product validation lens. Overall, the fit is tenuous and indirect.",
    "reasoning_summary": "Content addresses strategic vs. tactical roles of humans and AI in adaptive systems but lacks direct discussion of product validation, user testing, or feedback loops. Alignment with Product Validation category is marginal and indirect.",
    "level": "Ignored"
  },
  "Lean Principles": {
    "resourceId": "ffJaR9AaTl7",
    "category": "Lean Principles",
    "calculated_at": "2025-10-01T16:57:02",
    "ai_confidence": 29.625,
    "ai_mentions": 1.3,
    "ai_alignment": 3.5,
    "ai_depth": 3.8,
    "ai_intent": 3.0,
    "ai_audience": 4.1,
    "ai_signal": 3.1,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 30.0,
    "reasoning": "Direct references to Lean Principles, waste reduction, or Lean tools are absent. The discussion focuses on agency, adaptation, AI, and strategic intent, not on Lean thinking or process optimisation as meant by Lean. Some themes (optimisation, efficiency) tangentially overlap with Lean, but the content neither aligns conceptually nor in depth with core Lean principles such as Kaizen, value-stream mapping, or waste elimination. The intent is not to teach, advocate, or explore Lean approaches, but to delineate responsibilities between human and AI agency. The targeted audience (strategic and technical leaders) partially overlaps with Lean's, but all six scoring dimensions score low due to topic misalignment.",
    "reasoning_summary": "The content focuses on human and AI agency in adaptive systems, not on Lean Principles or waste reduction. It lacks direct conceptual, topical, and depth fit with the Lean category. Any overlap with Lean is incidental and not by design.",
    "level": "Ignored"
  },
  "Engineering Practices": {
    "resourceId": "ffJaR9AaTl7",
    "category": "Engineering Practices",
    "calculated_at": "2025-10-01T16:57:07",
    "ai_confidence": 34.53,
    "ai_mentions": 1.2,
    "ai_alignment": 3.7,
    "ai_depth": 4.3,
    "ai_intent": 2.6,
    "ai_audience": 6.1,
    "ai_signal": 6.0,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 35.0,
    "reasoning": "The content focuses on the strategic distinction between human and AI roles in adaptive systems, particularly highlighting areas of accountability, adaptation, and optimisation. While it thoughtfully examines decision-making layers and the operationalisation of human-AI agency, it does not directly address fundamental Agile engineering practices such as TDD, CI/CD, clean code, automation (in technical terms), or refactoring. The mentions of optimisation are conceptual (strategic vs tactical), not about engineering automation or tooling. The intended audience overlaps somewhat with engineering practitioners but more at the strategy/governance level than technical implementation. The signal is focused, but almost entirely outside the strict remit of the 'Engineering Practices' category as defined.",
    "reasoning_summary": "This content explores high-level strategy and decision-making boundaries between humans and AI but does not discuss core topics of Agile engineering practices like clean code, TDD, CI/CD, or automation in software development contexts.",
    "level": "Ignored"
  },
  "Liberating Structures": {
    "resourceId": "ffJaR9AaTl7",
    "category": "Liberating Structures",
    "calculated_at": "2025-10-01T16:57:07",
    "ai_confidence": 4.7,
    "ai_mentions": 0.2,
    "ai_alignment": 1.1,
    "ai_depth": 0.8,
    "ai_intent": 0.4,
    "ai_audience": 1.3,
    "ai_signal": 0.9,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 5.0,
    "reasoning": "The content makes no mention of Liberating Structures, facilitation techniques, or relevant toolkit elements. Its core focus is delineating human vs. AI agency in adaptive systems, not on team facilitation or engagement. There are no references, examples, or implications related to Liberating Structures use cases, methods, or audiences. It is aimed at thinkers on strategy, AI, and human decision-making, apart from facilitative or collaborative workshop practices. Alignment, depth, and intent scores are very low due to lack of relevant content.",
    "reasoning_summary": "No linkage to Liberating Structures: no direct mentions, methods, facilitation, or collaborative practices described. Content is about human/AI agency separation in adaptive systems, not about team facilitation or engagement.",
    "level": "Ignored"
  },
  "Systems Thinking": {
    "resourceId": "ffJaR9AaTl7",
    "category": "Systems Thinking",
    "calculated_at": "2025-10-01T16:57:07",
    "ai_confidence": 78.54,
    "ai_mentions": 2.3,
    "ai_alignment": 8.6,
    "ai_depth": 7.2,
    "ai_intent": 7.0,
    "ai_audience": 8.3,
    "ai_signal": 7.8,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 79.0,
    "reasoning": "The content thoroughly discusses the interplay between human and AI agency in adaptive systems, emphasizing holistic adaptation, boundaries, and system-level effects—concepts strongly aligned with Systems Thinking. However, it does not explicitly use phrases such as 'Systems Thinking,' system dynamics, or specific tools like causal loop diagrams, and doesn't cite the prominent frameworks directly. Still, the conceptual depth—such as feedback effects, system governance, and adaptation—matches the category's definition. The content also targets organisational leaders and system designers, appropriate for Systems Thinking, and maintains good signal-to-noise by staying focused on interdependencies and adaptive risk. Final score reflects strong alignment and depth but lower on direct mentions.",
    "reasoning_summary": "Strongly aligned with Systems Thinking principles—holistic adaptation, agency boundaries, interdependencies—though not named directly. Content fits the category by exploring system-level dynamics in human/AI roles. High fit, not explicit in terminology.",
    "level": "Secondary"
  },
  "Product Delivery": {
    "resourceId": "ffJaR9AaTl7",
    "category": "Product Delivery",
    "calculated_at": "2025-10-01T16:57:07",
    "ai_confidence": 34.14,
    "ai_mentions": 0.7,
    "ai_alignment": 4.8,
    "ai_depth": 4.3,
    "ai_intent": 3.5,
    "ai_audience": 4.1,
    "ai_signal": 4.2,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 34.0,
    "reasoning": "The content rigorously explores the distinction between human and AI agency in adaptive systems, focusing on strategy, adaptation, accountability, and optimisation. However, 'Product Delivery' is neither directly named nor substantially discussed. There is only indirect conceptual crossover with product delivery practices such as roles in decision making, operational boundaries, and system design governance, but the discussion remains high-level, theoretical, and not directly anchored in the end-to-end methodology or practices of software product delivery. Audience alignment is partial: practitioners in product delivery might relate, but the primary focus is more on agency and leadership within adaptive systems rather than delivering usable software products. There is no focus on Agile, planning, development, testing, deployment, CI/CD, release management, cross-functional team engagement, KPIs, or direct software delivery practices. The signal-to-noise is moderate: the reasoning is rigorous but only tangential for Product Delivery audiences.",
    "reasoning_summary": "While the content references system design and operationalisation of boundaries relevant to delivery, it focuses mainly on agency (human vs AI) and strategy, not the state, practices, or end-to-end process of product delivery. Fit is partial and indirect.",
    "level": "Ignored"
  },
  "Customer Feedback Loops": {
    "resourceId": "ffJaR9AaTl7",
    "category": "Customer Feedback Loops",
    "calculated_at": "2025-10-01T16:57:07",
    "ai_confidence": 11.48,
    "ai_mentions": 0.2,
    "ai_alignment": 1.0,
    "ai_depth": 1.5,
    "ai_intent": 0.5,
    "ai_audience": 4.1,
    "ai_signal": 2.7,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 11.0,
    "reasoning": "The content focuses exclusively on the distinction between human and AI agency in adaptive systems, especially around strategic intent and accountability versus tactical optimisation. There is no explicit or implicit discussion of customer feedback mechanisms, methods for collecting or utilising such feedback, or integration of user input into product development. It does address sensing and adaptation at a strategic level, but this refers to organisational-level sense-making (interpreting signals and reframing strategies) rather than establishing, closing, or operationalising customer feedback loops. The audience and domains overlap as both relate to adaptive organisations and continuous learning themes, but no practical feedback collection or loop processes are shown. Thus, fit for 'Customer Feedback Loops' is very low.",
    "reasoning_summary": "Does not fit: Content focuses on human vs. AI strategic roles, not on collecting or utilising customer feedback or feedback loop mechanisms. No feedback loop processes or integration of customer input are discussed.",
    "level": "Ignored"
  },
  "Trend Analysis": {
    "resourceId": "ffJaR9AaTl7",
    "category": "Trend Analysis",
    "calculated_at": "2025-10-01T16:57:11",
    "ai_confidence": 35.38,
    "ai_mentions": 0.6,
    "ai_alignment": 3.6,
    "ai_depth": 3.9,
    "ai_intent": 3.7,
    "ai_audience": 1.3,
    "ai_signal": 3.1,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 35.0,
    "reasoning": "The content explores distinctions between human and AI agency in adaptive systems, emphasizing strategic leadership versus tactical optimisation. While related to evolution and adaptation in organisations, it scarcely mentions or directly engages with trend analysis as outlined—there's no explicit discussion of identifying or analysing trends, patterns, or shifts within Agile, DevOps, or business agility. The focus is heavily on normative governance frameworks and delineations of responsibility. The audience may include organisational strategists, but not primarily those seeking tools or evidence-based approaches for trend identification or assessment. Depth is notable regarding system adaptation, but not on trends themselves.",
    "reasoning_summary": "Content discusses human vs. AI agency, system adaptation, and governance. No substantive references to trend identification, analysis, or impact. Fit to 'Trend Analysis' is weak and mostly indirect; match is partial and non-explicit.",
    "level": "Ignored"
  },
  "Decision Theory": {
    "resourceId": "ffJaR9AaTl7",
    "category": "Decision Theory",
    "calculated_at": "2025-10-01T16:57:11",
    "ai_confidence": 74.7,
    "ai_mentions": 2.2,
    "ai_alignment": 8.7,
    "ai_depth": 8.1,
    "ai_intent": 8.3,
    "ai_audience": 8.6,
    "ai_signal": 7.9,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 75.0,
    "reasoning": "The content robustly analyzes decision-making and agency at strategic and tactical levels, clearly distinguishing between human and AI roles in adaptive systems. It deeply explores frameworks around who should make decisions in complex, uncertain environments, aligns well with Decision Theory themes (uncertainty, adaptation, accountability, heuristics), and is skillfully targeted at professionals involved with organizational and system-level decisions. The explicit term 'Decision Theory' is absent, and direct coverage of heuristics/probability is minimal, but the core conceptual alignment and depth are strong. No penalties were required as the content is contemporary and supportive of the Decision Theory framework.",
    "reasoning_summary": "This content aligns strongly with Decision Theory by analyzing decision-making, adaptation, and agency in uncertainty. Despite no explicit mention, its focus and depth on human vs. AI decisions in adaptive systems fit the category definition well.",
    "level": "Secondary"
  },
  "Business Agility": {
    "resourceId": "ffJaR9AaTl7",
    "itemId": "ffJaR9AaTl7",
    "category": "Business Agility",
    "calculated_at": "2025-10-31T18:33:06",
    "ai_confidence": 71.53,
    "ai_mentions": 2.7,
    "ai_alignment": 8.9,
    "ai_depth": 8.3,
    "ai_intent": 7.6,
    "ai_audience": 7.1,
    "ai_signal": 8.0,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 72.0,
    "reasoning": "The content addresses adaptive systems and the delegation of decision-making across human and AI agents, emphasizing strategy and continual adaptation—core elements of business agility. It discusses risks, necessary boundaries, and governance, all highly relevant to organizational agility. However, it does not mention 'business agility' directly, nor does it offer explicit case studies or practical frameworks specific to business agility as a recognised discipline. Its concepts are tightly aligned but expressed more through the innovation/adaptation/leadership lens than business agility terminology. The intent and depth are strong and meaningful to the business agility audience, especially leaders and strategists considering AI integration. Some topics, like metrics or specific agile/lean/devops integration, are only tangentially referenced.",
    "reasoning_summary": "The content fits Business Agility through its in-depth focus on adaptive capacity, strategic leadership, and accountable innovation. Direct category mention is weak, but conceptual alignment and depth are strong. Fit is solid though not fully explicit.",
    "level": "Secondary"
  },
  "Enterprise Agility": {
    "resourceId": "ffJaR9AaTl7",
    "itemId": "ffJaR9AaTl7",
    "category": "Enterprise Agility",
    "calculated_at": "2025-10-31T18:33:05",
    "ai_confidence": 42.87,
    "ai_mentions": 2.5,
    "ai_alignment": 5.3,
    "ai_depth": 6.4,
    "ai_intent": 6.0,
    "ai_audience": 5.6,
    "ai_signal": 6.1,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 43.0,
    "reasoning": "While the article thoroughly explores the distinction between human and AI agency in adaptive systems and emphasizes the importance of human-led strategy, there are only indirect or implicit connections to Enterprise Agility. The core ideas (adaptation, continuous improvement, leadership, accountability) align conceptually with agility at an organisational level, but the piece never directly addresses organisational frameworks, change management practices, or agile scaling methods. The discussion is relevant for leaders and organisations interested in agility, but it is framed from a systems/agency viewpoint, not explicitly within an enterprise agility context. Therefore, fit is moderate and partial, with good depth but limited explicit categorical matching.",
    "reasoning_summary": "The content discusses roles of human and AI agency in adaptive systems, with themes of adaptability, leadership, and organisational relevance that partially fit Enterprise Agility. However, there are no direct references to agility frameworks or transformation.",
    "level": "Tertiary"
  },
  "Technical Debt": {
    "resourceId": "ffJaR9AaTl7",
    "itemId": "ffJaR9AaTl7",
    "category": "Technical Debt",
    "calculated_at": "2025-10-31T18:33:06",
    "ai_confidence": 8.1,
    "ai_mentions": 0.1,
    "ai_alignment": 1.6,
    "ai_depth": 2.0,
    "ai_intent": 1.8,
    "ai_audience": 1.5,
    "ai_signal": 1.0,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 8.0,
    "reasoning": "The content does not explicitly mention technical debt nor its terminology. Its core themes center on human vs. AI agency in adaptive systems, focusing on strategy versus optimisation. While there are abstract touchpoints about the risks of over-optimising and system brittleness, these do not directly frame the discussion in terms of technical debt management, accrual, or remediation. No techniques, measurement strategies, or explicit trade-offs are described as technical debt consequences. The content is aimed at practitioners and strategists who might overlap with the technical debt audience, but its main intent and framing are not on technical debt itself. Hence, the confidence in this classification is extremely low.",
    "reasoning_summary": "This content does not fit the Technical Debt category. It focuses on human and AI agency, not on managing or remediating technical debt; any overlap is at most tangential and indirect.",
    "level": "Ignored"
  },
  "Evidence Based Leadership": {
    "resourceId": "ffJaR9AaTl7",
    "itemId": "ffJaR9AaTl7",
    "category": "Evidence Based Leadership",
    "calculated_at": "2025-10-31T18:33:05",
    "ai_confidence": 53.8,
    "ai_mentions": 2.3,
    "ai_alignment": 6.6,
    "ai_depth": 6.9,
    "ai_intent": 5.5,
    "ai_audience": 5.3,
    "ai_signal": 6.0,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 54.0,
    "reasoning": "Direct mentions of 'evidence-based leadership' or related frameworks are absent. The discussion aligns with leadership and accountability, especially regarding strategic human agency vs AI in adaptive work, but only implicitly supports evidence-based principles. It stresses the importance of human governance, ethical choices, and clear boundary-setting in system design, governance, and escalation, echoing core concepts underlying evidence-based management such as accountability and system health. However, it lacks references to empirical data, metrics, or specific evidence-gathering techniques. The audience appears to include leadership and strategists, but the framing is conceptual and principled rather than explicitly empirical. Signal is decent, with most of the content serving the central thesis, but without data, KPIs, or direct evidence-based leadership tooling, the fit is moderate.",
    "reasoning_summary": "Content advocates principled leadership and strategic human agency in adaptive systems but lacks direct reference to evidence-based approaches, data, or metrics. Partial category fit by theme, not by explicit evidence-based practice.",
    "level": "Tertiary"
  },
  "Artificial Intelligence": {
    "resourceId": "ffJaR9AaTl7",
    "itemId": "ffJaR9AaTl7",
    "category": "Artificial Intelligence",
    "calculated_at": "2025-10-31T18:33:17",
    "ai_confidence": 67.75,
    "ai_mentions": 8.4,
    "ai_alignment": 6.8,
    "ai_depth": 7.0,
    "ai_intent": 6.4,
    "ai_audience": 6.2,
    "ai_signal": 6.0,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 68.0,
    "reasoning": "The content explicitly references AI agency, its tactical role, boundaries, and risks, demonstrating strong mention and moderate depth. However, it doesn't directly tie AI use to Agile, DevOps, or software development, instead discussing adaptive systems in organizational and strategic contexts. The audience and purpose appear adjacent—targeting organizational leaders and practitioners concerned with AI's boundaries, governance, and ethics in complex systems—rather than specifically Agile/DevOps practitioners. While themes of automation, optimisation, and accountability are relevant, the fit with the stated category (AI within Agile, DevOps, software development) is partial and indirect. No obsolete references or critical tone found.",
    "reasoning_summary": "Strong focus on human-AI agency boundaries and roles in adaptive systems, but lacks direct tie to Agile, DevOps, or software development. Fit is partial; relevant for strategic AI but not strictly within the intended category definition.",
    "level": "Secondary"
  },
  "Empirical Process Control": {
    "resourceId": "ffJaR9AaTl7",
    "itemId": "ffJaR9AaTl7",
    "category": "Empirical Process Control",
    "calculated_at": "2025-10-31T18:33:20",
    "ai_confidence": 42.35,
    "ai_mentions": 1.8,
    "ai_alignment": 5.7,
    "ai_depth": 4.3,
    "ai_intent": 4.5,
    "ai_audience": 4.4,
    "ai_signal": 5.2,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 42.0,
    "reasoning": "The content explores strategic vs tactical agency in adaptive systems, emphasizing human-led adaptation and ethical stewardship over AI-driven optimisation. It implicitly touches upon adaptation and direct accountability (concepts core to empirical process control) but does not mention or directly situate itself within agile, Scrum, or transparent/inspect/adapt terminology. There's discussion around decision-making based on real-world signals and the dangers of relying purely on optimisation, but these do not explicitly reference the empirical framework, nor do they detail tools, practices, or the triad of transparency-inspection-adaptation as per the category definition. While professionals engaged in empirical process control might find it conceptually adjacent—especially regarding adaptation and feedback—the content remains mostly at a philosophical/strategic level about agency boundaries, with few if any direct ties to the practical theories, methods, or historical foundations (e.g., Ken Schwaber, Jeff Sutherland) of empirical process control in Agile or Scrum. Thus, the fit is partial and mostly implicit.",
    "reasoning_summary": "The content partially fits the category through its emphasis on adaptation and feedback but lacks explicit coverage of empirical process control principles, Agile/Scrum context, or terminology. Fit is indirect, mostly via overlapping ideas of adaptation.",
    "level": "Tertiary"
  },
  "Ability to Innovate": {
    "resourceId": "ffJaR9AaTl7",
    "itemId": "ffJaR9AaTl7",
    "category": "Ability to Innovate",
    "calculated_at": "2025-10-31T18:33:24",
    "ai_confidence": 42.53,
    "ai_mentions": 1.2,
    "ai_alignment": 4.9,
    "ai_depth": 5.3,
    "ai_intent": 4.3,
    "ai_audience": 4.5,
    "ai_signal": 4.1,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 43.0,
    "reasoning": "Direct mentions of innovation or its metrics are mostly absent, with the content focusing on human vs. AI agency within adaptive systems. The content indirectly discusses the human ability to adapt and redefine systems—conceptually adjacent to innovation—but does not address innovation practices, metrics, or enhancement strategies explicitly. It aligns partly with the audience (organisational leaders), and there is meaningful depth on agency and adaptation. The signal-to-noise ratio is moderate since much content is relevant to agency, not innovation. No explicit penalties are warranted, as the content is not outdated or antagonistic toward the category.",
    "reasoning_summary": "Content explores agency in adaptation, a concept related but not equivalent to innovation. Does not explicitly address innovation metrics, mechanisms, or practices. Fit is partial and conceptual, not direct. Focus rests on strategic agency over innovativeness.",
    "level": "Tertiary"
  },
  "One Engineering System": {
    "resourceId": "ffJaR9AaTl7",
    "itemId": "ffJaR9AaTl7",
    "category": "One Engineering System",
    "calculated_at": "2025-10-31T18:33:29",
    "ai_confidence": 14.47,
    "ai_mentions": 0.0,
    "ai_alignment": 2.2,
    "ai_depth": 2.8,
    "ai_intent": 1.4,
    "ai_audience": 4.7,
    "ai_signal": 2.7,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 14.0,
    "reasoning": "The content does not mention 'One Engineering System' or 1ES directly or indirectly, nor does it discuss tool/process integration, standardisation, or engineering system frameworks. Its core focus is on human vs AI agency in adaptive systems—distinct from unifying engineering systems or related methodologies. Audience partially overlaps (technical/organizational focus), but no significant topic connection or implementation discussion with 1ES principles is present.",
    "reasoning_summary": "Content focuses on human vs AI agency and adaptive systems, not on unified engineering frameworks or 1ES topics. No alignment on principles, tools, or system integration; confidence is very low for this category.",
    "level": "Ignored"
  },
  "Agile Planning Tools": {
    "resourceId": "ffJaR9AaTl7",
    "itemId": "ffJaR9AaTl7",
    "category": "Agile Planning Tools",
    "calculated_at": "2025-10-31T18:33:30",
    "ai_confidence": 7.7,
    "ai_mentions": 0.2,
    "ai_alignment": 1.2,
    "ai_depth": 1.6,
    "ai_intent": 0.8,
    "ai_audience": 1.0,
    "ai_signal": 1.3,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 8.0,
    "reasoning": "The content focuses on human vs. AI agency in adaptive systems, strategic intent, and tactical optimization. It does not reference Agile Planning Tools, their purposes, features, or best practices. No tools like Jira, backlog management, sprint or release planning, or any Agile-specific toolsets are discussed. While there are references to system optimization and roles in decision-making, these are generalized digital/organizational concepts, not Agile planning processes or tools. The audience and depth are also focused on strategy and operational philosophy—not Agile planning practitioners or tool users.",
    "reasoning_summary": "This content centers on human vs. AI agency in adaptive systems, not Agile Planning Tools. There is almost no direct or indirect alignment with Agile planning, toolsets, or workflows. Fit is extremely weak and mostly unrelated.",
    "level": "Ignored"
  },
  "Evidence Based Management": {
    "resourceId": "ffJaR9AaTl7",
    "itemId": "ffJaR9AaTl7",
    "category": "Evidence Based Management",
    "calculated_at": "2025-10-31T18:33:31",
    "ai_confidence": 44.3,
    "ai_mentions": 1.6,
    "ai_alignment": 4.9,
    "ai_depth": 5.2,
    "ai_intent": 4.5,
    "ai_audience": 6.2,
    "ai_signal": 5.5,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 44.0,
    "reasoning": "The content discusses the distinction between human and AI agency in adaptive systems, emphasizing strategic intent, accountability, and boundaries. However, it does not explicitly mention Evidence Based Management or its core metrics (Current Value, Time to Market, etc.), nor does it reference empirical data-driven decision making or outcome-based management frameworks. Key themes like agency, adaptation, and strategic vs tactical roles resonate with some EBM concepts such as human-led empirical sense-making and governance, but there is no direct focus on empirical evidence, measurement, or the structured improvement of value delivery as EBM requires. The discussion remains largely conceptual rather than operational or metric-driven. The intended audience overlaps with leadership and strategic decision makers (common to EBM), and the signal is moderately relevant but not specifically tailored to EBM principles. No explicit or implicit penalties are warranted.",
    "reasoning_summary": "The piece critiques AI overreach in adaptive systems and stresses human strategic agency, echoing some EBM values. However, it lacks direct focus on metrics, data-driven management, or explicit EBM themes, so fit is partial and indirect.",
    "level": "Tertiary"
  },
  "Metrics and Learning": {
    "resourceId": "ffJaR9AaTl7",
    "itemId": "ffJaR9AaTl7",
    "category": "Metrics and Learning",
    "calculated_at": "2025-11-10T14:43:36",
    "ai_confidence": 33.35,
    "ai_mentions": 1.2,
    "ai_alignment": 3.6,
    "ai_depth": 4.2,
    "ai_intent": 3.9,
    "ai_audience": 3.0,
    "ai_signal": 2.2,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 33.0,
    "reasoning": "Direct mentions of metrics, data, measurement, feedback loops, or continuous improvement (as defined in the category) are almost entirely absent. The content primarily contrasts human and AI agency in adaptive systems, focusing on strategic versus tactical roles. While it refers to 'optimisation', 'experimentation', 'sense-making', and 'patterns', these are not elaborated in the context of evidence-based metrics, data-driven feedback, or measurement frameworks. The main intent is philosophical/strategic (who should adapt vs. optimise), not how to use metrics or data for learning. The target audience is more organisational/design/leadership than practitioners of metrics in Agile/DevOps, and little content is devoted to actionable metrics, measurement techniques, tools, or frameworks. There are only passing allusions to feedback ('strategic sensing', 'patterns'), but these are high-level and not grounded in practical measurement. There is significant focus on system boundaries, accountability, and adaptation, but not on metrics and learning per se. No mention of case studies, measurement technologies, or continuous improvement frameworks, and only indirect relevance to evidence-based management. Thus, overall fit for the category is tenuous and partial at best.",
    "reasoning_summary": "Content primarily explores human vs. AI roles in adaptive systems, not metrics, feedback, or evidence-driven learning. Mentions of optimisation are not grounded in measurement frameworks, so alignment with the 'Metrics and Learning' category is weak and mostly indirect.",
    "level": "Ignored"
  },
  "Product Strategy": {
    "resourceId": "ffJaR9AaTl7",
    "itemId": "ffJaR9AaTl7",
    "category": "Product Strategy",
    "calculated_at": "2025-11-10T14:43:00",
    "ai_confidence": 79.14,
    "ai_mentions": 4.6,
    "ai_alignment": 8.3,
    "ai_depth": 7.8,
    "ai_intent": 8.7,
    "ai_audience": 7.2,
    "ai_signal": 8.4,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 79.0,
    "reasoning": "The content rigorously differentiates human and AI agency, repeatedly referencing 'strategy,' 'strategic intent,' 'purpose setting,' and the criticality of human-led strategic direction. It frames these in the context of adaptive systems, accountability, and system definition—strongly aligning with vision, adaptation, and roadmapping concerns of Product Strategy. The discussion is in-depth and operational, addresses the risk of misplacing strategic work, and directly guides organizational behavior—consistent with Product Strategy interests, especially in AI-centric products. However, there is less explicit discussion of frameworks, market analysis, or classic product roadmapping. The intended audience is primarily strategists and executives, though system designers may also benefit. The content is focused, with minimal tangential material, but is slightly more about the meta-structure of strategic agency rather than tool-based strategy. No penalties were necessary.",
    "reasoning_summary": "The content closely aligns with Product Strategy by defining the unique human role in setting vision, strategy, and purpose, especially in AI-driven adaptive systems. It is strategic and in-depth, though less focused on traditional frameworks or metrics.",
    "level": "Secondary"
  },
  "Scrum Master": {
    "resourceId": "ffJaR9AaTl7",
    "itemId": "ffJaR9AaTl7",
    "category": "Scrum Master",
    "calculated_at": "2025-11-10T14:43:42",
    "ai_confidence": 6.328,
    "ai_mentions": 0.2,
    "ai_alignment": 0.8,
    "ai_depth": 1.0,
    "ai_intent": 0.7,
    "ai_audience": 1.2,
    "ai_signal": 1.0,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 6.0,
    "reasoning": "The content never mentions 'Scrum Master' or Scrum principles directly, nor does it engage with the role or its accountabilities. While it addresses human agency and accountability in strategic settings, these are general leadership concepts not focused on Scrum, the Scrum Master, or their formal responsibilities. There is no discussion of Scrum roles, enabling empiricism, or systemic improvement in a Scrum context. The audience is generic, aimed at professionals considering human vs. AI agency, not Scrum practitioners. No signal for the category beyond a generic mention of accountability. No penalties as the tone is neutral and not outdated.",
    "reasoning_summary": "This content does not fit the Scrum Master category. There are no explicit or implicit connections to Scrum, its roles, or the Scrum Master’s accountabilities. Focus is strictly on general human vs. AI agency in adaptive systems.",
    "level": "Ignored"
  },
  "Strategic Goals": {
    "resourceId": "ffJaR9AaTl7",
    "itemId": "ffJaR9AaTl7",
    "category": "Strategic Goals",
    "calculated_at": "2025-11-10T14:43:41",
    "ai_confidence": 87.7,
    "ai_mentions": 6.8,
    "ai_alignment": 9.2,
    "ai_depth": 8.7,
    "ai_intent": 8.1,
    "ai_audience": 8.9,
    "ai_signal": 8.7,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 88.0,
    "reasoning": "The content deeply explores the distinction between human and AI agency, arguing that humans must own strategy, purpose-setting, adaptation, and accountability—core areas of strategic goals. Strategic intent, reframing goals, pivoting strategies, and ethical stewardship are directly and repeatedly referenced as exclusive human responsibilities, aligning strongly with business agility in adaptive systems. Though the terms 'strategic goals' and 'objectives' are not frequently literal, the concepts are persistently and substantively examined, especially through practical mappings, risks of abdicating strategy, and operational guidelines for boundary setting. The piece targets executives, strategists, and practitioners designing adaptive, agile systems, and maintains a consistently high signal-to-noise ratio, only momentarily referencing operational or tactical concerns to clarify the strategic context. No penalties apply; there is no outdated or contradictory material.",
    "reasoning_summary": "The content strongly fits Strategic Goals, focusing on human-led strategy, purpose-setting, adaptive reframing, and accountability in agile, adaptive systems. Its core argument and elaborations align fully with the category; fit is high but not absolute literal.",
    "level": "Primary"
  },
  "Minimum Viable Product": {
    "resourceId": "ffJaR9AaTl7",
    "itemId": "ffJaR9AaTl7",
    "category": "Minimum Viable Product",
    "calculated_at": "2025-11-10T14:43:05",
    "ai_confidence": 19.45,
    "ai_mentions": 0.2,
    "ai_alignment": 2.6,
    "ai_depth": 2.75,
    "ai_intent": 2.1,
    "ai_audience": 6.0,
    "ai_signal": 2.7,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 19.0,
    "reasoning": "There are no direct mentions or in-depth discussions of Minimum Viable Product (MVP), Lean Startup, or related core MVP themes. The content focuses on the distinction between human and AI agency in adaptive systems, emphasizing strategy, accountability, and the risks of overdelegating adaptation to AI. While it briefly touches on experimentation and hypothesis-driven exploration, these are discussed within the broader context of strategic sense-making and organizational adaptation, not MVP development or market validation. There is no coverage of MVP techniques, minimal feature sets, rapid iteration, Lean/Agile principles specific to product development, or feedback loops for validating business assumptions. The audience (likely technical leaders or strategists in adaptive systems) could overlap with MVP practitioners, but the signal-to-noise ratio for MVP relevance is low.",
    "reasoning_summary": "The content doesn't discuss MVP. While overlapping with themes like experimentation and adaptation, its focus is on human vs. AI agency in strategic systems, not MVP development or validation. Very weak fit for the Minimum Viable Product category.",
    "level": "Ignored"
  },
  "Agile Frameworks": {
    "resourceId": "ffJaR9AaTl7",
    "itemId": "ffJaR9AaTl7",
    "category": "Agile Frameworks",
    "calculated_at": "2025-11-10T14:43:46",
    "ai_confidence": 13.64,
    "ai_mentions": 0.3,
    "ai_alignment": 1.2,
    "ai_depth": 2.1,
    "ai_intent": 2.0,
    "ai_audience": 3.3,
    "ai_signal": 2.4,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 14.0,
    "reasoning": "The content centers on differentiating human and AI agency in adaptive systems, strategy, and accountability, emphasizing the risks of over-delegating to AI. There are no direct mentions of Agile frameworks, their principles, or application. The focus is high-level and philosophical, not referencing or discussing Scrum, Kanban, or any Agile framework in intent, terminology, or suggested practice. The conceptual relation to adaptation, experimentation, and organisational learning bears faint thematic similarity to Agile values, but no substantial alignment with the core definition of Agile Frameworks as outlined in the classification criteria. The target audience appears to be organisational leaders or strategists considering AI adoption, not practitioners of Agile frameworks.",
    "reasoning_summary": "The content discusses human and AI roles in adaptive systems, with no explicit or meaningful reference to Agile frameworks. Any connection to Agile thinking is minimal and indirect. Content does not fit the Agile Frameworks category.",
    "level": "Ignored"
  },
  "Digital Transformation": {
    "resourceId": "ffJaR9AaTl7",
    "itemId": "ffJaR9AaTl7",
    "category": "Digital Transformation",
    "calculated_at": "2025-11-10T14:43:46",
    "ai_confidence": 66.825,
    "ai_mentions": 2.4,
    "ai_alignment": 7.8,
    "ai_depth": 7.6,
    "ai_intent": 7.2,
    "ai_audience": 7.3,
    "ai_signal": 8.1,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 67.0,
    "reasoning": "Direct mentions of 'digital transformation' are absent, resulting in a low mentions score. However, the content thoroughly explores human vs. AI agency in the context of organisational strategy, adaptation, and operational systems—core elements of digital transformation. The depth and conceptual alignment are robust: discussions about system governance, strategy, accountability, and optimisation all relate to the digital transformation theme of instituting modern digital technology (AI) to influence organisational structure and behaviour. Though intent and audience generally match (strategic, professional), the focus is on agency boundaries in adaptive systems rather than explicitly on digital transformation initiatives, frameworks, or case studies. Much of the argument’s relevance to digital transformation is implicit. No evidence of penalties is found; content is current, serious, and focused.",
    "reasoning_summary": "The content strongly aligns with digital transformation concepts (AI in organisational adaptation), but does not directly mention or focus explicitly on digital transformation strategy or initiatives. Fit is partial but substantiated.",
    "level": "Secondary"
  },
  "Lean Thinking": {
    "resourceId": "ffJaR9AaTl7",
    "itemId": "ffJaR9AaTl7",
    "category": "Lean Thinking",
    "calculated_at": "2025-11-10T14:43:08",
    "ai_confidence": 16.59,
    "ai_mentions": 0.25,
    "ai_alignment": 1.8,
    "ai_depth": 1.6,
    "ai_intent": 2.4,
    "ai_audience": 3.1,
    "ai_signal": 2.25,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 17.0,
    "reasoning": "The content focuses almost exclusively on the respective roles of human and AI agency in adaptive systems, strategy, and accountability. There is no mention of Lean Thinking, its principles (value, waste, flow, etc.), or its practices. The core framing concerns organisational adaptation, agency boundaries, and decision-making between humans and AI, rather than minimising waste, value stream mapping, continuous improvement, or Lean leadership. There are partial conceptual echoes (optimisation, efficiency), but they are not situated within Lean context or terminology. The intent, audience (organisational and technical leaders), and focus are only tangentially related through the loose theme of optimization; signal-to-noise is moderate with highly focused but off-category content. No penalties were necessary.",
    "reasoning_summary": "This content does not fit the 'Lean Thinking' category. It explores human vs. AI agency in adaptive systems, but does not mention Lean principles or focus on Lean's topics (waste, value streams, etc.). Any connection is tangential and coincidental.",
    "level": "Ignored"
  },
  "Operational Practices": {
    "resourceId": "ffJaR9AaTl7",
    "itemId": "ffJaR9AaTl7",
    "category": "Operational Practices",
    "calculated_at": "2025-11-10T14:43:50",
    "ai_confidence": 68.75,
    "ai_mentions": 2.2,
    "ai_alignment": 7.8,
    "ai_depth": 6.6,
    "ai_intent": 6.2,
    "ai_audience": 7.7,
    "ai_signal": 6.5,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 69.0,
    "reasoning": "The content robustly distinguishes between human strategic agency and AI's role in tactical optimisation, noting AI's function in process efficiency and scope-bounded improvements—key signals for Operational Practices. However, the primary focus is on the philosophy and governance of adaptive systems, not hands-on optimisation practices (e.g., Kanban or workflow metrics). There are some explicit operational recommendations (e.g., 'operationalise clear agency boundaries') and implications for system governance, linking abstract principles to the embedding of operational disciplines. Still, practical application examples and concrete techniques are less developed, and much of the text is devoted to conceptual boundaries and cautionary argument rather than how-to improvement at the process level. The audience seems to include operational and governance practitioners. Mention of optimisation and operational discipline justifies moderate alignment, depth, and signal-to-noise scores.",
    "reasoning_summary": "The content links AI to operational optimisation and calls for operationalised agency boundaries, but focuses more on governance philosophy than hands-on process improvement. It partially fits Operational Practices; practical details are limited.",
    "level": "Secondary"
  },
  "Sprint Review": {
    "resourceId": "ffJaR9AaTl7",
    "category": "Sprint Review",
    "calculated_at": "2025-05-13T13:53:01",
    "ai_confidence": 1.7,
    "ai_mentions": 0.0,
    "ai_alignment": 0.5,
    "ai_depth": 0.7,
    "ai_intent": 1.0,
    "ai_audience": 2.0,
    "ai_signal": 1.0,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 2.0,
    "reasoning": "The content does not mention Sprint Review or any Scrum event, nor does it discuss any of the key topics outlined for the category. Its focus is on the distinction between human and AI agency in adaptive systems, strategy, and accountability, with no reference to Scrum, sprints, reviews, or related practices. The audience, intent, and depth are unrelated to Sprint Review, and there is no signal relevant to the category. All scores are minimal, reflecting a complete lack of fit.",
    "reasoning_summary": "This content is entirely unrelated to Sprint Review, focusing instead on human versus AI agency in adaptive systems. There are no direct or indirect references to Scrum, sprints, or reviews, resulting in a very low confidence score for this category.",
    "level": "Ignored"
  },
  "Cross Functional Teams": {
    "resourceId": "ffJaR9AaTl7",
    "itemId": "ffJaR9AaTl7",
    "category": "Cross Functional Teams",
    "calculated_at": "2025-11-10T14:43:50",
    "ai_confidence": 6.229,
    "ai_mentions": 0.2,
    "ai_alignment": 1.1,
    "ai_depth": 0.7,
    "ai_intent": 1.9,
    "ai_audience": 0.8,
    "ai_signal": 0.7,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 6.0,
    "reasoning": "The content centers on the distribution of human and AI agency in adaptive systems, exploring strategy vs. optimisation. It does not mention or discuss cross-functional teams, team composition, collaboration, or related Agile topics. No overlap with the category's core themes; no discussion of team structures, dynamics, or practices relevant to cross-functional teams. Audience and intent are not aligned, as the focus is on AI governance, not on Agile team constructs or collaboration.",
    "reasoning_summary": "Content focuses on the distinction between human and AI agency in adaptive systems, with no discussion of cross-functional teams, team structures, or related Agile topics. The topic, intent, and themes do not fit the assigned category.",
    "level": "Ignored"
  },
  "Scrum Values": {
    "resourceId": "ffJaR9AaTl7",
    "itemId": "ffJaR9AaTl7",
    "category": "Scrum Values",
    "calculated_at": "2025-11-24T18:57:18",
    "ai_confidence": 11.03,
    "ai_mentions": 0.3,
    "ai_alignment": 1.2,
    "ai_depth": 1.7,
    "ai_intent": 1.1,
    "ai_audience": 2.9,
    "ai_signal": 2.3,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 11.0,
    "reasoning": "The content does not mention Scrum Values, nor does it discuss them conceptually. Instead, it focuses on the delineation between human and AI agency in adaptive systems, strategy, and accountability. While the article addresses topics like accountability, leadership, and adaptation, these are treated in the context of AI-human systems, not Scrum. There is no exploration of the Scrum framework, its team values, or their significance, thus failing to align with the definition. The intended audience appears to be organisational leaders and strategists, not Scrum practitioners. Any incidental overlap—such as discussions of accountability—is too generic and unrelated to Scrum Values per se.",
    "reasoning_summary": "Content is unrelated to Scrum Values, focusing instead on human vs. AI agency in adaptive systems. There are no direct or conceptual links to the Scrum Values or framework; any overlap with 'accountability' is incidental and not Scrum-specific.",
    "level": "Ignored"
  },
  "Deployment Strategies": {
    "resourceId": "ffJaR9AaTl7",
    "itemId": "ffJaR9AaTl7",
    "category": "Deployment Strategies",
    "calculated_at": "2025-11-24T18:57:21",
    "ai_confidence": 13.1,
    "ai_mentions": 0.7,
    "ai_alignment": 1.3,
    "ai_depth": 2.9,
    "ai_intent": 1.2,
    "ai_audience": 3.6,
    "ai_signal": 1.1,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 13.0,
    "reasoning": "The content focuses on the distinction between human and AI agency in adaptive systems, exploring themes of strategy and accountability versus tactical AI-driven optimisation. It does not mention, discuss, or advise on deployment methodologies, practices, or strategies. There are no direct or implied references to blue-green deployments, canary releases, rolling updates, feature toggles, infrastructure as code, or any form of deployment-related practice. Although there is some alignment in discussing professional practices and operational disciplines, these are about agency boundaries and not software deployment. The technical audience partially overlaps, but the topic is unrelated to deployment strategies; thus, confidence in category fit is very low.",
    "reasoning_summary": "Content does not discuss deployment strategies or related practices; it focuses on human vs. AI agency in adaptive systems, making it a poor fit for this category.",
    "level": "Ignored"
  },
  "Organisational Psychology": {
    "resourceId": "ffJaR9AaTl7",
    "itemId": "ffJaR9AaTl7",
    "category": "Organisational Psychology",
    "calculated_at": "2025-11-24T18:56:16",
    "ai_confidence": 61.145,
    "ai_mentions": 2.4,
    "ai_alignment": 7.7,
    "ai_depth": 7.2,
    "ai_intent": 6.4,
    "ai_audience": 7.1,
    "ai_signal": 8.1,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 61.0,
    "reasoning": "The content discusses the distinct roles of human and AI agency in adaptive systems, focusing heavily on human responsibility for direction, ethics, and accountability—concepts relevant to organisational psychology, particularly leadership and decision-making. However, it centers primarily on governance and the structure of strategic vs. tactical roles, rather than directly applying or referencing psychological principles, theories, or research. There are no explicit mentions of classic organisational psychology concepts such as motivation theories, group behaviour, or team dynamics. The discussion is relevant to leaders or decision-makers navigating technological change, but the treatment of psychological topics is implicit (accountability, stewardship, ethical leadership) rather than explicit. The article’s intent is mostly to delineate function and responsibility between humans and AI at a system design level, intersecting with, but not grounded in, organisational psychology as defined. Minimal direct terminology or theory from the discipline is used. No penalties applied, as content is current and not satirical.",
    "reasoning_summary": "While concepts like leadership, accountability, and decision-making align partly with organisational psychology, the primary focus is on agency in adaptive systems, not explicit psychological theories or team dynamics. The category fit is partial, implicit, and indirect.",
    "level": "Secondary"
  },
  "Agile Transformation": {
    "resourceId": "ffJaR9AaTl7",
    "itemId": "ffJaR9AaTl7",
    "category": "Agile Transformation",
    "calculated_at": "2025-11-24T18:56:12",
    "ai_confidence": 30.5,
    "ai_mentions": 0.6,
    "ai_alignment": 2.8,
    "ai_depth": 3.2,
    "ai_intent": 2.1,
    "ai_audience": 2.8,
    "ai_signal": 3.4,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 30.0,
    "reasoning": "The content explores the roles of human and AI agency in adaptive systems, focusing on strategy, accountability, and boundaries in technology-enabled organisations. While it discusses adaptation, leadership, accountability, and the need for continuous organisational responsiveness—concepts somewhat resonant with Agile mindsets—it does not reference agile transformation, the Agile Manifesto, specific frameworks (Scrum, Kanban, Lean), or organisational-level change methodologies central to Agile Transformation. Instead, its primary concern is human/AI boundaries and risk management within adaptive/complex systems, not agile transformation strategy, process, or cultural adoption. The tone and references are methodology-neutral and don't signal Agile implementation, metrics, change management, or leadership for agile at organisational scale. Thus, only tangential conceptual alignment and depth is present, with low direct relevance.",
    "reasoning_summary": "While the content discusses adaptation and leadership, it does not address Agile Transformation, its frameworks, strategies, or methodologies. The connection to Agile Transformation is only tangential, not direct or substantive.",
    "level": "Ignored"
  },
  "Agile Leadership": {
    "resourceId": "ffJaR9AaTl7",
    "itemId": "ffJaR9AaTl7",
    "category": "Agile Leadership",
    "calculated_at": "2025-11-24T18:56:12",
    "ai_confidence": 47.49,
    "ai_mentions": 1.1,
    "ai_alignment": 5.5,
    "ai_depth": 5.2,
    "ai_intent": 5.0,
    "ai_audience": 5.9,
    "ai_signal": 4.2,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 47.0,
    "reasoning": "The content discusses human versus AI agency in adaptive systems, focusing on strategy, accountability, and ethical stewardship. While it repeatedly emphasizes the essential role of human agency, including leadership and accountability at the strategic level, it does not directly reference Agile Leadership or the Agile context. The themes of human stewardship, accountability, and resisting overreliance on AI overlap with servant leadership and empowerment, core to Agile Leadership. However, the discussion is framed much more broadly (adaptive systems, human/AI boundaries) and lacks explicit Agile terminology, audience targeting, or methods. The references to leadership are principle-based rather than directly about Agile Leadership roles, frameworks, or practices.",
    "reasoning_summary": "Content focuses on human leadership versus AI in adaptive systems, echoing some Agile Leadership values (accountability, stewardship) but lacks direct Agile context, explicit terminology, and full alignment with the category's themes.",
    "level": "Tertiary"
  },
  "Organisational Culture": {
    "resourceId": "ffJaR9AaTl7",
    "itemId": "ffJaR9AaTl7",
    "category": "Organisational Culture",
    "calculated_at": "2025-11-24T18:56:12",
    "ai_confidence": 54.85,
    "ai_mentions": 2.8,
    "ai_alignment": 6.3,
    "ai_depth": 5.8,
    "ai_intent": 6.1,
    "ai_audience": 6.7,
    "ai_signal": 6.5,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 55.0,
    "reasoning": "Direct mentions of 'culture' are absent, but the content indirectly addresses leadership, accountability, system governance, and adaptation—concepts closely tied to organisational culture. The main discussion is about the boundaries of human and AI agency in adaptive systems, focusing on the dangers of delegating strategic adaptation to AI, and emphasises the need for human-led strategy, stewardship, and ethical framing. These themes are relevant to how organisational culture shapes, and is shaped by, leaders' decisions about agency, adaptation, and risk. However, culture is never explicitly framed as the central topic. Instead, the discussion is couched in terms of agency, accountability, and strategy. The fit is partial and somewhat indirect—relevant for practitioners developing cultural boundaries for AI in organisations, but not an in-depth or central treatise on culture per se.",
    "reasoning_summary": "The content aligns with aspects of organisational culture—leadership, strategic intent, accountability—but never addresses culture directly. Fit is partial and indirect, focused on agency and system adaptation rather than culture as the primary theme.",
    "level": "Tertiary"
  },
  "Agnostic Agile": {
    "resourceId": "ffJaR9AaTl7",
    "itemId": "ffJaR9AaTl7",
    "category": "Agnostic Agile",
    "calculated_at": "2025-11-24T18:56:12",
    "ai_confidence": 44.89,
    "ai_mentions": 0.3,
    "ai_alignment": 4.2,
    "ai_depth": 4.7,
    "ai_intent": 4.1,
    "ai_audience": 7.2,
    "ai_signal": 7.8,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 45.0,
    "reasoning": "There are no direct mentions of Agnostic Agile or its core terminology; the content’s main focus is on human versus AI agency in adaptive systems, not agile methods or philosophy. However, some conceptual parallels exist: emphasis on human ethical stewardship, context-sensitive adaptation, and the call for principled decision-making. Depth is moderate—the discussion of human governance versus machine optimisation is robust, but it is not explicitly connected to the Agnostic Agile movement, frameworks, or its intellectual roots. The audience and signal-to-noise ratios are solid because the messaging targets strategic practitioners and decision-makers, though not specifically agile practitioners. No retro/outdated concepts or contradictory tone observed; therefore, no penalties were applied. The final confidence reflects minimal direct fit but reasonable partial alignment in concepts.",
    "reasoning_summary": "Content explores human vs. AI agency and ethical, strategic adaptation, which loosely aligns with Agnostic Agile principles but never references or deeply explores agile topics or its philosophy directly. Fit is partial, mostly by conceptual overlap.",
    "level": "Tertiary"
  },
  "Team Collaboration": {
    "category": "Team Collaboration",
    "calculated_at": "2025-04-29T13:04:40",
    "ai_confidence": 15.0,
    "ai_mentions": 0,
    "ai_alignment": 10.0,
    "ai_depth": 5.0,
    "non_ai_confidence": 10,
    "final_score": 15.0,
    "reasoning": "The content primarily discusses the roles of human and AI agency in adaptive systems, focusing on strategic intent and tactical optimisation. While it touches on collaboration between humans and AI, it does not explicitly address team dynamics, communication, or shared ownership within Agile, Scrum, or DevOps frameworks. The discussion lacks depth in terms of team collaboration techniques or practices, leading to a low confidence score in this category.",
    "level": "Ignored"
  },
  "Throughput": {
    "category": "Throughput",
    "calculated_at": "2025-04-29T13:04:44",
    "ai_confidence": 12.0,
    "ai_mentions": 0,
    "ai_alignment": 10.0,
    "ai_depth": 15.0,
    "non_ai_confidence": 0,
    "final_score": 12.0,
    "reasoning": "The content primarily discusses the roles of human and AI agency in adaptive systems, focusing on strategic intent and tactical optimisation. While it touches on themes of optimisation, it does not explicitly mention throughput as a delivery metric or provide any analysis, visualisation, or interpretation of throughput. The discussion lacks depth in relation to throughput metrics, making it a secondary topic rather than a primary focus.",
    "level": "Ignored"
  },
  "Technical Excellence": {
    "category": "Technical Excellence",
    "calculated_at": "2025-04-29T13:04:48",
    "ai_confidence": 32.0,
    "ai_mentions": 0,
    "ai_alignment": 25.0,
    "ai_depth": 40.0,
    "non_ai_confidence": 0,
    "final_score": 32.0,
    "reasoning": "The content discusses the roles of human and AI agency in adaptive systems, focusing on strategic intent and tactical optimisation. While it touches on the importance of human decision-making and accountability, it does not explicitly mention or explore key technical practices such as TDD, CI/CD, or modular architecture. The discussion is more philosophical and strategic rather than rooted in engineering practices that ensure high-quality software delivery. Therefore, while there is some conceptual alignment with the idea of maintaining quality through human oversight, the lack of direct mentions and specific technical practices leads to a lower confidence score in the category of Technical Excellence.",
    "level": "Ignored"
  },
  "Agile Strategy": {
    "category": "Agile Strategy",
    "calculated_at": "2025-04-29T13:04:52",
    "ai_confidence": 72.0,
    "ai_mentions": 15,
    "ai_alignment": 30,
    "ai_depth": 25,
    "non_ai_confidence": 10,
    "final_score": 72.0,
    "reasoning": "The content discusses the importance of human agency in defining strategy and purpose within adaptive systems, which aligns with the Agile Strategy category's focus on organisational vision and adaptability. It highlights the need for strategic intent and the risks of over-relying on AI for tactical optimisation, which touches on the integration of Agile principles into strategic planning. However, while it addresses strategic concepts, it does not explicitly mention Agile methodologies or practices, leading to a moderate confidence score. The depth of discussion on the interplay between human and AI roles in strategy provides valuable insights but lacks specific Agile context.",
    "level": "Secondary",
    "reasoning_summary": "This content explores how human decision-making shapes strategy in adaptive systems, emphasising the balance between human intent and AI-driven optimisation. While it aligns with Agile Strategy by focusing on vision and adaptability, it doesn’t directly reference Agile methods or practices. The discussion offers relevant insights into strategic planning but lacks explicit Agile context, making the fit partial rather than definitive."
  },
  "Team Performance": {
    "category": "Team Performance",
    "calculated_at": "2025-04-29T13:04:55",
    "ai_confidence": 12.0,
    "ai_mentions": 0,
    "ai_alignment": 15.0,
    "ai_depth": 10.0,
    "non_ai_confidence": 0,
    "final_score": 12.0,
    "reasoning": "The content primarily discusses the roles of human and AI agency in adaptive systems, focusing on strategic intent and tactical optimisation. While it touches on themes of decision-making and accountability, it does not explicitly address team performance or delivery metrics at the team level. The discussion lacks depth regarding team dynamics, systemic constraints, or delivery patterns, which are crucial for the 'Team Performance' category.",
    "level": "Ignored"
  },
  "Install and Configuration": {
    "category": "Install and Configuration",
    "calculated_at": "2025-04-29T13:05:01",
    "ai_confidence": 5.0,
    "ai_mentions": 0,
    "ai_alignment": 10.0,
    "ai_depth": 10.0,
    "non_ai_confidence": 10,
    "final_score": 5.0,
    "reasoning": "The content primarily discusses the roles of human and AI agency in adaptive systems, focusing on strategic and tactical decision-making rather than any specific installation or configuration processes. There are no direct mentions of installation procedures, configuration best practices, or troubleshooting related to Agile or DevOps tools, which are essential for this category. The discussion is theoretical and strategic, lacking the actionable instructions or insights required for effective implementation of technologies in an organisational context.",
    "level": "Ignored"
  },
  "Professional Scrum": {
    "category": "Professional Scrum",
    "calculated_at": "2025-04-29T13:05:05",
    "ai_confidence": 32.0,
    "ai_mentions": 0,
    "ai_alignment": 40.0,
    "ai_depth": 20.0,
    "non_ai_confidence": 10,
    "final_score": 32.0,
    "reasoning": "The content discusses the roles of human and AI agency in adaptive systems, focusing on strategic intent and accountability. However, it does not explicitly mention Scrum or its principles, which limits its direct relevance to the Professional Scrum category. While there are elements of accountability and the importance of human decision-making that align with Scrum values, the depth of discussion on Scrum-specific practices is minimal.",
    "level": "Ignored"
  },
  "Transparency": {
    "category": "Transparency",
    "calculated_at": "2025-04-29T13:05:09",
    "ai_confidence": 15.0,
    "ai_mentions": 0,
    "ai_alignment": 10.0,
    "ai_depth": 5.0,
    "non_ai_confidence": 0,
    "final_score": 15.0,
    "reasoning": "The content primarily discusses the roles of human and AI agency in adaptive systems, focusing on strategic intent and tactical optimisation. While it touches on accountability, it does not explicitly address transparency in Agile processes or the importance of clear communication and visibility within teams. The discussion lacks depth in relation to transparency, making it a secondary theme rather than a primary focus.",
    "level": "Ignored"
  },
  "Automated Testing": {
    "category": "Automated Testing",
    "calculated_at": "2025-04-29T13:05:13",
    "ai_confidence": 5.0,
    "ai_mentions": 0,
    "ai_alignment": 10.0,
    "ai_depth": 10.0,
    "non_ai_confidence": 0,
    "final_score": 5.0,
    "reasoning": "The content primarily discusses the roles of human and AI agency in adaptive systems, focusing on strategic intent and tactical optimisation. It does not mention automated testing, its principles, practices, or methodologies, nor does it align with the key topics outlined for the 'Automated Testing' category. The discussion is more philosophical and strategic rather than technical or related to software testing.",
    "level": "Ignored"
  },
  "Azure Pipelines": {
    "category": "Azure Pipelines",
    "calculated_at": "2025-04-29T13:05:16",
    "ai_confidence": 5.0,
    "ai_mentions": 0,
    "ai_alignment": 10.0,
    "ai_depth": 10.0,
    "non_ai_confidence": 0,
    "final_score": 5.0,
    "reasoning": "The content discusses human and AI agency in adaptive systems, focusing on strategic and tactical roles rather than Azure Pipelines or any related CI/CD practices. There are no direct mentions of Azure Pipelines, and the themes do not align with the core topics of Azure Pipelines, such as build and release management, YAML configurations, or deployment strategies.",
    "level": "Ignored"
  },
  "Employee Engagement": {
    "category": "Employee Engagement",
    "calculated_at": "2025-04-29T13:05:20",
    "ai_confidence": 12.0,
    "ai_mentions": 0,
    "ai_alignment": 10.0,
    "ai_depth": 20.0,
    "non_ai_confidence": 0,
    "final_score": 12.0,
    "reasoning": "The content primarily discusses the roles of human and AI agency in adaptive systems, focusing on strategic intent and tactical optimisation. While it touches on themes of accountability and decision-making, it does not explicitly address employee engagement, motivation, or practices that enhance team commitment. The discussion lacks direct relevance to the core principles of employee engagement, making it a secondary topic at best.",
    "level": "Ignored"
  },
  "Psychological Safety": {
    "category": "Psychological Safety",
    "calculated_at": "2025-04-29T13:05:24",
    "ai_confidence": 12.0,
    "ai_mentions": 0,
    "ai_alignment": 10.0,
    "ai_depth": 10.0,
    "non_ai_confidence": 0,
    "final_score": 12.0,
    "reasoning": "The content primarily discusses the roles of human and AI agency in adaptive systems, focusing on strategic intent and tactical optimisation. While it touches on themes of accountability and decision-making, it does not explicitly address psychological safety or its importance in team dynamics, communication, or innovation. The discussion lacks depth in relation to psychological safety, making it a secondary topic rather than a primary focus.",
    "level": "Ignored"
  },
  "Value Stream Mapping": {
    "category": "Value Stream Mapping",
    "calculated_at": "2025-04-29T13:05:27",
    "ai_confidence": 5.0,
    "ai_mentions": 0,
    "ai_alignment": 10.0,
    "ai_depth": 10.0,
    "non_ai_confidence": 10,
    "final_score": 5.0,
    "reasoning": "The content primarily discusses the roles of human and AI agency in adaptive systems, focusing on strategic intent and tactical optimisation. It does not mention Value Stream Mapping or its principles, nor does it align with the core themes of VSM, such as workflow visualisation or waste elimination. The discussion is more about decision-making layers and the risks of overdelegating to AI, which is outside the scope of Value Stream Mapping.",
    "level": "Ignored"
  },
  "Working Agreements": {
    "category": "Working Agreements",
    "calculated_at": "2025-04-29T13:05:32",
    "ai_confidence": 12.0,
    "ai_mentions": 0,
    "ai_alignment": 10.0,
    "ai_depth": 10.0,
    "non_ai_confidence": 0,
    "final_score": 12.0,
    "reasoning": "The content primarily discusses the roles of human and AI agency in adaptive systems, focusing on strategic intent and tactical optimisation. It does not explicitly mention working agreements or their importance in team collaboration, nor does it provide techniques or examples related to working agreements. The discussion is more about decision-making layers and the risks of overdelegating to AI, which deviates from the core principles of teamwork and collaboration outlined in the category definition.",
    "level": "Ignored"
  },
  "Software Development": {
    "category": "Software Development",
    "calculated_at": "2025-04-29T13:05:36",
    "ai_confidence": 12.0,
    "ai_mentions": 0,
    "ai_alignment": 20.0,
    "ai_depth": 10.0,
    "non_ai_confidence": 0,
    "final_score": 12.0,
    "reasoning": "The content primarily discusses the roles of human and AI agency in adaptive systems, focusing on strategic intent and tactical optimisation. While it touches on decision-making and operational practices, it does not directly address software development methodologies, coding practices, or any of the key topics outlined in the Software Development category. The discussion is more philosophical and strategic rather than technical or methodological, leading to a low confidence score in alignment with the Software Development category.",
    "level": "Ignored"
  },
  "Backlog Refinement": {
    "category": "Backlog Refinement",
    "calculated_at": "2025-04-29T13:05:40",
    "ai_confidence": 5.0,
    "ai_mentions": 0,
    "ai_alignment": 10.0,
    "ai_depth": 10.0,
    "non_ai_confidence": 0,
    "final_score": 5.0,
    "reasoning": "The content primarily discusses the roles of human and AI agency in adaptive systems, focusing on strategic intent and tactical optimisation. It does not mention backlog refinement, Agile practices, or Scrum methodologies, which are essential for this category. The themes of collaboration and iterative improvement are present but are not related to backlog refinement specifically.",
    "level": "Ignored"
  },
  "Time to Market": {
    "category": "Time to Market",
    "calculated_at": "2025-04-29T13:05:43",
    "ai_confidence": 15.0,
    "ai_mentions": 0,
    "ai_alignment": 10.0,
    "ai_depth": 5.0,
    "non_ai_confidence": 10,
    "final_score": 15.0,
    "reasoning": "The content primarily discusses the roles of human and AI agency in adaptive systems, focusing on strategic intent and tactical optimisation. While it touches on themes of efficiency and adaptation, it does not explicitly mention Time to Market or related metrics. The discussion lacks depth in addressing how these concepts impact the speed of delivering value to customers, which is central to the Time to Market category.",
    "level": "Ignored"
  },
  "Continuous Integration": {
    "category": "Continuous Integration",
    "calculated_at": "2025-04-29T13:05:47",
    "ai_confidence": 5.0,
    "ai_mentions": 0,
    "ai_alignment": 10.0,
    "ai_depth": 10.0,
    "non_ai_confidence": 0,
    "final_score": 5.0,
    "reasoning": "The content primarily discusses the roles of human and AI agency in adaptive systems, focusing on strategic intent and tactical optimisation. It does not mention Continuous Integration or related practices, tools, or principles. The themes of CI, such as code integration, automated testing, and team collaboration, are absent, leading to a very low confidence score for alignment with the Continuous Integration category.",
    "level": "Ignored"
  },
  "Forecasting": {
    "category": "Forecasting",
    "calculated_at": "2025-04-29T13:05:51",
    "ai_confidence": 12.0,
    "ai_mentions": 0,
    "ai_alignment": 10.0,
    "ai_depth": 15.0,
    "non_ai_confidence": 0,
    "final_score": 12.0,
    "reasoning": "The content primarily discusses the roles of human and AI agency in adaptive systems, focusing on strategic intent and tactical optimisation. While it touches on themes of adaptation and decision-making, it does not explicitly address forecasting methods, empirical data utilisation, or Agile and Scrum practices. The discussion lacks direct relevance to the core topics of forecasting, resulting in a low confidence score.",
    "level": "Ignored"
  },
  "Personal": {
    "category": "Personal",
    "calculated_at": "2025-04-29T13:05:55",
    "ai_confidence": 25.0,
    "ai_mentions": 0,
    "ai_alignment": 30.0,
    "ai_depth": 20.0,
    "non_ai_confidence": 0,
    "final_score": 25.0,
    "reasoning": "The content discusses the roles of human and AI agency in adaptive systems, focusing on strategic intent and tactical optimisation. However, it lacks personal anecdotes or reflections that would align it with the 'Personal' category. While it touches on themes relevant to Agile and decision-making, it does not provide individual experiences or insights, making it more of a theoretical discussion rather than a personal reflection.",
    "level": "Ignored"
  },
  "Mentoring": {
    "category": "Mentoring",
    "calculated_at": "2025-04-29T13:05:58",
    "ai_confidence": 12.0,
    "ai_mentions": 0,
    "ai_alignment": 15.0,
    "ai_depth": 10.0,
    "non_ai_confidence": 0,
    "final_score": 12.0,
    "reasoning": "The content primarily discusses the roles of human and AI agency in adaptive systems, focusing on strategic intent and tactical optimisation. While it touches on themes of accountability and decision-making, it does not explicitly mention mentoring, coaching, or the development of skills and behaviours necessary for effective practice in Agile or related methodologies. The discussion lacks depth in terms of mentoring techniques or strategies for professional growth, leading to a low confidence score in the 'Mentoring' category.",
    "level": "Ignored"
  },
  "Azure Repos": {
    "category": "Azure Repos",
    "calculated_at": "2025-04-29T13:06:01",
    "ai_confidence": 0,
    "ai_mentions": 0,
    "ai_alignment": 0,
    "ai_depth": 0,
    "non_ai_confidence": 0,
    "final_score": 0.0,
    "reasoning": "The content focuses on the roles of human and AI agency in adaptive systems, discussing strategic intent and tactical optimisation. It does not mention Azure Repos or any related topics such as source control, branching strategies, or CI/CD processes, making it entirely irrelevant to the category.",
    "level": "Ignored"
  },
  "Project Management": {
    "category": "Project Management",
    "calculated_at": "2025-04-29T13:06:05",
    "ai_confidence": 25.0,
    "ai_mentions": 0,
    "ai_alignment": 30.0,
    "ai_depth": 20.0,
    "non_ai_confidence": 0,
    "final_score": 25.0,
    "reasoning": "The content discusses the roles of human and AI agency in adaptive systems, focusing on strategic intent and tactical optimisation. While it touches on decision-making and accountability, which are relevant to project management, it does not explicitly address project management methodologies, lifecycle phases, or tools. The discussion is more philosophical and theoretical rather than practical and actionable, leading to a low confidence score in the context of project management.",
    "level": "Ignored"
  },
  "System Configuration": {
    "category": "System Configuration",
    "calculated_at": "2025-04-29T13:06:09",
    "ai_confidence": 12.0,
    "ai_mentions": 0,
    "ai_alignment": 10.0,
    "ai_depth": 15.0,
    "non_ai_confidence": 10,
    "final_score": 12.0,
    "reasoning": "The content primarily discusses the roles of human and AI agency in adaptive systems, focusing on strategic intent and tactical optimisation. While it touches on concepts relevant to system dynamics, it does not explicitly address system configuration, tools, or methodologies for setup and maintenance. The discussion lacks direct mentions of system configuration practices, and the depth of exploration into system setup or integration is minimal, leading to a low confidence score in this category.",
    "level": "Ignored"
  },
  "Beta Codex": {
    "category": "Beta Codex",
    "calculated_at": "2025-04-29T13:06:13",
    "ai_confidence": 72.0,
    "ai_mentions": 3,
    "ai_alignment": 85.0,
    "ai_depth": 65.0,
    "non_ai_confidence": 0,
    "final_score": 72.0,
    "reasoning": "The content discusses the importance of human agency in adaptive systems, which aligns with the human-centric approach of BetaCodex. It highlights the need for strategic intent and ethical stewardship, resonating with the principles of decentralisation and adaptive organisational cultures. However, while it touches on relevant themes, it does not explicitly mention BetaCodex or provide detailed case studies or comparisons with traditional models, which limits its depth of discussion. Overall, the content is closely aligned with the category but lacks some explicit references and depth in certain areas.",
    "level": "Secondary",
    "reasoning_summary": "The content aligns well with the category by emphasising human agency, strategic intent, and ethical stewardship—key aspects of a human-centric, adaptive approach. However, it doesn’t directly reference BetaCodex or offer in-depth examples or comparisons, which would strengthen its fit. Overall, it’s relevant but could be more explicit and detailed to fully match the category’s requirements."
  },
  "Product Owner": {
    "category": "Product Owner",
    "calculated_at": "2025-04-29T13:06:17",
    "ai_confidence": 12.0,
    "ai_mentions": 0,
    "ai_alignment": 10.0,
    "ai_depth": 20.0,
    "non_ai_confidence": 0,
    "final_score": 12.0,
    "reasoning": "The content discusses the roles of human and AI agency in adaptive systems, focusing on strategic intent and tactical optimisation. While it touches on accountability, it does not specifically address the Product Owner's accountability within Scrum or Agile frameworks. The primary focus is on the distinction between human and AI roles rather than the Product Owner's responsibilities, leading to a low confidence score in this category.",
    "level": "Ignored"
  },
  "Definition of Ready": {
    "category": "Definition of Ready",
    "calculated_at": "2025-04-29T13:06:20",
    "ai_confidence": 5.0,
    "ai_mentions": 0,
    "ai_alignment": 10.0,
    "ai_depth": 10.0,
    "non_ai_confidence": 10,
    "final_score": 5.0,
    "reasoning": "The content primarily discusses the roles of human and AI agency in adaptive systems, focusing on strategic intent and tactical optimisation. It does not mention the Definition of Ready or its criteria, nor does it address backlog item readiness, which are central to the category. The discussion lacks any direct relevance to the DoR, making it a very low confidence score.",
    "level": "Ignored"
  },
  "Unrealised Value": {
    "category": "Unrealised Value",
    "calculated_at": "2025-04-29T13:06:25",
    "ai_confidence": 25.0,
    "ai_mentions": 0,
    "ai_alignment": 30.0,
    "ai_depth": 20.0,
    "non_ai_confidence": 0,
    "final_score": 25.0,
    "reasoning": "The content primarily discusses the roles of human and AI agency in adaptive systems, focusing on strategic intent versus tactical optimisation. While it touches on the importance of human agency in defining strategy and purpose, it does not explicitly mention 'Unrealised Value' or its indicators, nor does it explore untapped opportunities or potential improvements in a way that aligns with the category's core themes. The discussion lacks depth regarding the identification or measurement of unrealised value, leading to a low confidence score.",
    "level": "Ignored"
  },
  "Release Management": {
    "category": "Release Management",
    "calculated_at": "2025-04-29T13:06:28",
    "ai_confidence": 5.0,
    "ai_mentions": 0,
    "ai_alignment": 10.0,
    "ai_depth": 10.0,
    "non_ai_confidence": 0,
    "final_score": 5.0,
    "reasoning": "The content primarily discusses the roles of human and AI agency in adaptive systems, focusing on strategic intent and tactical optimisation. It does not mention release management, software delivery, or any related practices. The themes of release management, such as version control, risk management, or CI/CD, are absent, leading to a very low confidence score in alignment with the category.",
    "level": "Ignored"
  },
  "Cycle Time": {
    "category": "Cycle Time",
    "calculated_at": "2025-04-29T13:06:31",
    "ai_confidence": 5.0,
    "ai_mentions": 0,
    "ai_alignment": 10.0,
    "ai_depth": 10.0,
    "non_ai_confidence": 0,
    "final_score": 5.0,
    "reasoning": "The content primarily discusses the roles of human and AI agency in adaptive systems, focusing on strategic intent and tactical optimisation. It does not mention Cycle Time or its measurement, nor does it explore its implications within Agile or DevOps practices. The discussion is more about decision-making and agency rather than workflow efficiency or Cycle Time metrics.",
    "level": "Ignored"
  },
  "Technical Mastery": {
    "category": "Technical Mastery",
    "calculated_at": "2025-04-29T13:06:35",
    "ai_confidence": 25.0,
    "ai_mentions": 5,
    "ai_alignment": 15,
    "ai_depth": 10,
    "non_ai_confidence": 0,
    "final_score": 25.0,
    "reasoning": "The content discusses the roles of human and AI agency in adaptive systems, focusing on strategic intent and tactical optimisation. While it touches on the importance of human discernment and accountability, it does not delve into specific software craftsmanship practices, methodologies, or principles that are central to the 'Technical Mastery' category. The discussion is more philosophical and strategic rather than technical, leading to a low confidence score in alignment with the category.",
    "level": "Ignored"
  },
  "Continuous Improvement": {
    "category": "Continuous Improvement",
    "calculated_at": "2025-04-29T13:06:40",
    "ai_confidence": 32.0,
    "ai_mentions": 0,
    "ai_alignment": 40.0,
    "ai_depth": 20.0,
    "non_ai_confidence": 0,
    "final_score": 32.0,
    "reasoning": "The content discusses the roles of human and AI agency in adaptive systems, focusing on strategic intent and tactical optimisation. While it touches on the importance of adaptation and the risks of over-relying on AI, it does not explicitly mention Continuous Improvement or its principles. The conceptual alignment is moderate as it hints at the need for ongoing adaptation, but the depth of discussion on Continuous Improvement practices is minimal, leading to a lower confidence score.",
    "level": "Ignored"
  },
  "Complexity Thinking": {
    "category": "Complexity Thinking",
    "calculated_at": "2025-04-29T13:06:44",
    "ai_confidence": 87.0,
    "ai_mentions": 15,
    "ai_alignment": 35,
    "ai_depth": 30,
    "non_ai_confidence": 10,
    "final_score": 87.0,
    "reasoning": "The content extensively discusses the roles of human and AI agency within adaptive systems, highlighting the importance of human strategic intent in the face of complexity. It aligns well with key topics of Complexity Thinking, such as the need for human discernment in decision-making and the implications of uncertainty in management. The depth of discussion on the risks of overdelegating adaptation to AI and the necessity of maintaining clear agency boundaries further supports its relevance to the category. The content does not merely mention complexity but engages with its principles, making it a strong fit for the classification.",
    "level": "Primary",
    "reasoning_summary": "This content is a strong match for the Complexity Thinking category. It thoughtfully explores how human and AI roles interact within adaptive systems, emphasising the importance of human judgement and strategic intent. By addressing uncertainty, decision-making, and the risks of over-relying on AI, it clearly engages with core principles of complexity, rather than just referencing them superficially."
  },
  "Value Delivery": {
    "category": "Value Delivery",
    "calculated_at": "2025-04-29T13:06:48",
    "ai_confidence": 25.0,
    "ai_mentions": 5,
    "ai_alignment": 15,
    "ai_depth": 5,
    "non_ai_confidence": 0,
    "final_score": 25.0,
    "reasoning": "The content discusses the roles of human and AI agency in adaptive systems, focusing on strategic intent and tactical optimisation. While it touches on the importance of human agency in defining purpose and direction, it does not explicitly address value delivery within Agile, Scrum, or DevOps frameworks. The discussion lacks depth in terms of methodologies or practices that enhance customer satisfaction or business agility, which are central to the Value Delivery category. Therefore, while there are relevant mentions, the overall alignment and depth are limited.",
    "level": "Ignored"
  },
  "Experimentation": {
    "category": "Experimentation",
    "calculated_at": "2025-04-29T13:06:53",
    "ai_confidence": 32.0,
    "ai_mentions": 2,
    "ai_alignment": 40.0,
    "ai_depth": 25.0,
    "non_ai_confidence": 50,
    "final_score": 32.0,
    "reasoning": "The content discusses the role of AI in tactical optimisation and mentions rapid experimentation within bounded parameters, which aligns with the concept of experimentation. However, the primary focus is on the distinction between human and AI agency rather than on hypothesis-driven approaches or systematic testing of ideas. The depth of discussion on experimentation techniques is limited, and while there are mentions of experimentation, they are not the central theme of the content.",
    "level": "Ignored"
  },
  "Azure Boards": {
    "category": "Azure Boards",
    "calculated_at": "2025-04-29T13:06:56",
    "ai_confidence": 5.0,
    "ai_mentions": 0,
    "ai_alignment": 10.0,
    "ai_depth": 10.0,
    "non_ai_confidence": 0,
    "final_score": 5.0,
    "reasoning": "The content does not mention Azure Boards or any related concepts. It focuses on the roles of human and AI agency in adaptive systems, which is unrelated to Azure Boards or Agile project management. There is no discussion of work items, customisation, or any features relevant to Azure Boards, leading to a very low confidence score.",
    "level": "Ignored"
  },
  "Revenue per Employee": {
    "category": "Revenue per Employee",
    "calculated_at": "2025-04-29T13:06:59",
    "ai_confidence": 5.0,
    "ai_mentions": 0,
    "ai_alignment": 10.0,
    "ai_depth": 10.0,
    "non_ai_confidence": 0,
    "final_score": 5.0,
    "reasoning": "The content primarily discusses the roles of human and AI agency in adaptive systems, focusing on strategic intent and tactical optimisation. It does not mention Revenue per Employee or relate to workforce efficiency, organisational throughput, or financial performance metrics. The discussion is more philosophical and strategic rather than empirical or metric-based, which is essential for the category.",
    "level": "Ignored"
  },
  "Modern Source Control": {
    "category": "Modern Source Control",
    "calculated_at": "2025-04-29T13:07:03",
    "ai_confidence": 5.0,
    "ai_mentions": 0,
    "ai_alignment": 10.0,
    "ai_depth": 10.0,
    "non_ai_confidence": 0,
    "final_score": 5.0,
    "reasoning": "The content primarily discusses the roles of human and AI agency in adaptive systems, focusing on strategic intent and tactical optimisation. It does not mention version control systems, branching strategies, or any other key topics related to modern source control. The discussion is centred around decision-making and organisational dynamics rather than version control practices, leading to a very low confidence score in alignment with the category.",
    "level": "Ignored"
  },
  "Working Software": {
    "category": "Working Software",
    "calculated_at": "2025-04-29T13:07:07",
    "ai_confidence": 10.0,
    "ai_mentions": 0,
    "ai_alignment": 20.0,
    "ai_depth": 10.0,
    "non_ai_confidence": 0,
    "final_score": 10.0,
    "reasoning": "The content primarily discusses the roles of human and AI agency in adaptive systems, focusing on strategic intent and tactical optimisation. It does not directly address working software as an output artifact, nor does it engage with Agile or Scrum principles. The mention of optimisation and adaptation is more theoretical and does not connect to the delivery of tangible software products, leading to a very low confidence score in the 'Working Software' category.",
    "level": "Ignored"
  },
  "Scrum Team": {
    "category": "Scrum Team",
    "calculated_at": "2025-04-29T13:07:12",
    "ai_confidence": 5.0,
    "ai_mentions": 0,
    "ai_alignment": 10.0,
    "ai_depth": 10.0,
    "non_ai_confidence": 0,
    "final_score": 5.0,
    "reasoning": "The content primarily discusses the roles of human and AI agency in adaptive systems, focusing on strategic intent and tactical optimisation. It does not mention the Scrum Team, its structure, or its responsibilities as defined in the Scrum framework. The themes of accountability and team dynamics are not addressed in relation to Scrum, leading to a very low confidence score for alignment with the 'Scrum Team' category.",
    "level": "Ignored"
  },
  "Common Goals": {
    "category": "Common Goals",
    "calculated_at": "2025-04-29T13:07:16",
    "ai_confidence": 32.0,
    "ai_mentions": 10.0,
    "ai_alignment": 40.0,
    "ai_depth": 20.0,
    "non_ai_confidence": 10,
    "final_score": 32.0,
    "reasoning": "The content discusses the roles of human and AI agency in adaptive systems, focusing on strategic intent and tactical optimisation. While it touches on the importance of strategic direction, it does not explicitly address the concept of Common Goals as defined in Agile and DevOps frameworks. The discussion lacks a clear connection to shared objectives or alignment within teams, which are central to the Common Goals category. The depth of discussion on aligning strategy with execution is minimal, leading to a low confidence score.",
    "level": "Ignored"
  },
  "Open Space Agile": {
    "category": "Open Space Agile",
    "calculated_at": "2025-04-29T13:07:21",
    "ai_confidence": 12.0,
    "ai_mentions": 0,
    "ai_alignment": 10.0,
    "ai_depth": 10.0,
    "non_ai_confidence": 0,
    "final_score": 12.0,
    "reasoning": "The content primarily discusses the roles of human and AI agency in adaptive systems, focusing on strategic intent and tactical optimisation. It does not directly mention Open Space Agile or its principles, nor does it engage with the core themes of collective participation, psychological safety, or co-creation in Agile transformations. While there are elements of organisational agility, they are not framed within the context of Open Space Agile, leading to a low confidence score.",
    "level": "Ignored"
  },
  "Continuous Learning": {
    "category": "Continuous Learning",
    "calculated_at": "2025-04-29T13:07:25",
    "ai_confidence": 32.0,
    "ai_mentions": 10.0,
    "ai_alignment": 40.0,
    "ai_depth": 20.0,
    "non_ai_confidence": 0,
    "final_score": 32.0,
    "reasoning": "The content discusses the roles of human and AI agency in adaptive systems, focusing on strategic intent and tactical optimisation. While it touches on the importance of human discernment and adaptation, it does not explicitly address continuous learning principles or practices. The discussion lacks depth in terms of techniques for knowledge sharing, feedback loops, or creating a culture of experimentation, which are key topics in the Continuous Learning category. Therefore, while there are relevant themes, the primary focus is not on continuous learning itself.",
    "level": "Ignored"
  },
  "Asynchronous Development": {
    "category": "Asynchronous Development",
    "calculated_at": "2025-04-29T13:07:29",
    "ai_confidence": 12.0,
    "ai_mentions": 0,
    "ai_alignment": 10.0,
    "ai_depth": 15.0,
    "non_ai_confidence": 0,
    "final_score": 12.0,
    "reasoning": "The content primarily discusses the roles of human and AI agency in adaptive systems, focusing on strategic versus tactical decision-making. It does not mention asynchronous development or its principles, tools, or practices. While it touches on collaboration and decision-making, it does not align with the core themes of asynchronous development, nor does it provide any depth on the topic. Therefore, the confidence score is low, reflecting minimal relevance to the category.",
    "level": "Ignored"
  },
  "Organisational Agility": {
    "category": "Organisational Agility",
    "calculated_at": "2025-04-29T13:07:33",
    "ai_confidence": 78.0,
    "ai_mentions": 15,
    "ai_alignment": 32,
    "ai_depth": 30,
    "non_ai_confidence": 0,
    "final_score": 78.0,
    "reasoning": "The content discusses the critical role of human agency in adaptive systems, emphasising the need for strategic intent and adaptability, which aligns well with the principles of organisational agility. It highlights the importance of human decision-making in response to changing environments, a key aspect of agility. The depth of discussion on the risks of over-relying on AI for adaptation and the need for clear boundaries between human and AI roles further supports its relevance to the category. However, while it touches on agility, it does not explicitly discuss agile methodologies or frameworks, which slightly lowers the confidence score.",
    "level": "Secondary",
    "reasoning_summary": "This content is a strong fit for the organisational agility category, as it explores how human agency and adaptability drive effective responses to change—core aspects of agility. While it doesn’t directly reference agile frameworks, its focus on strategic intent, decision-making, and the balance between human and AI roles makes it highly relevant to the theme."
  },
  "Flow Efficiency": {
    "category": "Flow Efficiency",
    "calculated_at": "2025-04-29T13:07:37",
    "ai_confidence": 15.0,
    "ai_mentions": 0,
    "ai_alignment": 25.0,
    "ai_depth": 10.0,
    "non_ai_confidence": 10,
    "final_score": 15.0,
    "reasoning": "The content primarily discusses the roles of human and AI agency in adaptive systems, focusing on strategic intent and tactical optimisation. While it touches on concepts of optimisation, it does not explicitly address flow efficiency or the optimisation of work throughput across a value stream. The discussion lacks direct mentions of flow efficiency principles, metrics, or techniques for eliminating bottlenecks, which are central to the category. Overall, the content is more about the philosophical implications of agency rather than practical applications related to flow efficiency.",
    "level": "Ignored"
  },
  "Hypothesis Driven Development": {
    "category": "Hypothesis Driven Development",
    "calculated_at": "2025-04-29T13:07:41",
    "ai_confidence": 32.0,
    "ai_mentions": 2,
    "ai_alignment": 40.0,
    "ai_depth": 25.0,
    "non_ai_confidence": 0,
    "final_score": 32.0,
    "reasoning": "The content discusses the importance of human agency in defining strategy and setting hypotheses, which aligns with the conceptual framework of hypothesis-driven development. However, it primarily focuses on the distinction between human and AI roles in adaptive systems rather than detailing the process of formulating, testing, and validating hypotheses through experimentation. The mention of 'hypothesis-driven exploration' is relevant but not central to the overall discussion, leading to a lower confidence score.",
    "level": "Ignored"
  },
  "Pragmatic Thinking": {
    "category": "Pragmatic Thinking",
    "calculated_at": "2025-04-29T13:07:46",
    "ai_confidence": 82.0,
    "ai_mentions": 15,
    "ai_alignment": 32,
    "ai_depth": 35,
    "non_ai_confidence": 10,
    "final_score": 82.0,
    "reasoning": "The content discusses the critical roles of human and AI agency in adaptive systems, emphasising the importance of human strategic intent and decision-making in complex environments. It aligns well with pragmatic thinking by addressing practical problem-solving strategies, particularly in the context of AI and human collaboration. The depth of discussion is significant, providing detailed insights into the risks of overdelegating to AI and the necessity of maintaining human accountability. The content effectively illustrates real-world applications and the need for clear operational boundaries, which are key aspects of pragmatic thinking.",
    "level": "Primary",
    "reasoning_summary": "This content is a strong fit for the category, as it explores how humans and AI collaborate in adaptive systems, focusing on practical strategies and real-world challenges. It highlights the importance of human oversight and decision-making, offering detailed insights into maintaining accountability and setting clear boundaries—core elements of pragmatic thinking."
  },
  "Entrepreneurship": {
    "category": "Entrepreneurship",
    "calculated_at": "2025-04-29T13:07:50",
    "ai_confidence": 32.0,
    "ai_mentions": 10.0,
    "ai_alignment": 40.0,
    "ai_depth": 50.0,
    "non_ai_confidence": 0,
    "final_score": 32.0,
    "reasoning": "The content discusses the roles of human and AI agency in adaptive systems, focusing on strategic intent and tactical optimisation. While it touches on themes of decision-making and adaptation, which are relevant to entrepreneurship, it does not explicitly address entrepreneurship principles, innovation, or risk-taking. The discussion is more about operational dynamics rather than entrepreneurial strategies or mindsets, leading to a lower confidence score in the entrepreneurship category.",
    "level": "Ignored"
  },
  "Shift Left Strategy": {
    "category": "Shift Left Strategy",
    "calculated_at": "2025-04-29T13:07:54",
    "ai_confidence": 12.0,
    "ai_mentions": 0,
    "ai_alignment": 10.0,
    "ai_depth": 15.0,
    "non_ai_confidence": 10,
    "final_score": 12.0,
    "reasoning": "The content primarily discusses the roles of human and AI agency in adaptive systems, focusing on strategic intent versus tactical optimisation. While it touches on themes of decision-making and accountability, it does not explicitly mention or align with the Shift-Left Strategy, which is centred on integrating testing, security, and compliance earlier in the software development lifecycle. The discussion lacks depth regarding the principles, techniques, or tools associated with the Shift-Left approach, leading to a low confidence score.",
    "level": "Ignored"
  },
  "Test Automation": {
    "category": "Test Automation",
    "calculated_at": "2025-04-29T13:07:58",
    "ai_confidence": 5.0,
    "ai_mentions": 0,
    "ai_alignment": 10.0,
    "ai_depth": 10.0,
    "non_ai_confidence": 0,
    "final_score": 5.0,
    "reasoning": "The content primarily discusses the roles of human and AI agency in adaptive systems, focusing on strategic intent and tactical optimisation. It does not mention test automation, frameworks, or practices related to software testing. The themes of the content do not align with the principles of test automation, and there is no depth of discussion on any relevant topics within this category.",
    "level": "Ignored"
  },
  "Decision Making": {
    "category": "Decision Making",
    "calculated_at": "2025-04-29T13:08:01",
    "ai_confidence": 82.0,
    "ai_mentions": 15,
    "ai_alignment": 32,
    "ai_depth": 28,
    "non_ai_confidence": 0,
    "final_score": 82.0,
    "reasoning": "The content discusses the critical roles of human and AI agency in decision-making within adaptive systems, focusing on strategic intent versus tactical optimisation. It explicitly mentions decision-making layers and the importance of human accountability in the process. The depth of discussion on the risks of overdelegating to AI and the need for clear agency boundaries further aligns with evidence-based decision-making principles. However, while it touches on decision-making, it does not delve deeply into structured methodologies or empirical frameworks, which slightly lowers the confidence score.",
    "level": "Primary",
    "reasoning_summary": "This content is relevant to the category as it explores how human and AI roles interact in decision-making, highlighting the balance between strategic oversight and tactical execution. It addresses key issues like accountability and the risks of excessive AI delegation, which are central to evidence-based decision-making, though it doesn’t fully detail specific methodologies or frameworks."
  },
  "Customer Retention": {
    "category": "Customer Retention",
    "calculated_at": "2025-04-29T13:08:05",
    "ai_confidence": 12.0,
    "ai_mentions": 0,
    "ai_alignment": 10.0,
    "ai_depth": 20.0,
    "non_ai_confidence": 0,
    "final_score": 12.0,
    "reasoning": "The content primarily discusses the roles of human and AI agency in adaptive systems, focusing on strategic intent and tactical optimisation. While it touches on themes of adaptation and decision-making, it does not directly address customer retention strategies, user engagement, or methodologies for minimising churn. The discussion lacks explicit references to customer satisfaction, feedback mechanisms, or personalisation, which are crucial for the Customer Retention category.",
    "level": "Ignored"
  },
  "Principle": {
    "resourceId": "ffJaR9AaTl7",
    "category": "Principle",
    "calculated_at": "2025-05-01T14:48:24",
    "ai_confidence": 78.0,
    "ai_mentions": 15,
    "ai_alignment": 32,
    "ai_depth": 28,
    "non_ai_confidence": null,
    "final_score": 78.0,
    "reasoning": "The content centers around the principles of human agency and the role of AI in adaptive systems, explicitly discussing how humans define strategy and purpose while AI focuses on tactical optimisation. There is a clear categorization of decision-making layers that aligns with key Agile principles such as accountability, adaptability, and the distinction between strategic and tactical roles. Additionally, it advocates for a principled approach to collaboration between humans and AI, making it actionable in the context of adaptive systems. Overall, the depth of discussion is substantial, covering practical implications and risks associated with misallocating agency, which further supports a high confidence in alignment with the category 'Principle'.",
    "level": "Secondary",
    "reasoning_summary": "This content is a strong fit for the 'Principle' category, as it thoughtfully explores the division of roles between humans and AI, emphasising strategic human agency and tactical AI optimisation. It aligns with Agile values like accountability and adaptability, and offers actionable guidance for collaboration, making its classification as a principle-based discussion well justified."
  },
  "Tool": {
    "resourceId": "ffJaR9AaTl7",
    "category": "Tool",
    "calculated_at": "2025-05-01T14:48:24",
    "ai_confidence": 10.0,
    "ai_mentions": 0,
    "ai_alignment": 20.0,
    "ai_depth": 10.0,
    "non_ai_confidence": null,
    "final_score": 10.0,
    "reasoning": "The content primarily discusses the roles of human and AI agency in adaptive systems, focusing on strategic vs. tactical decision-making. There are no mentions of specific tools or mechanisms that facilitate workflows, nor does it explore the implementation of any tools or their integration within Agile, Lean, or DevOps practices. Although it touches on concepts that can tangentially relate to tools (such as optimization), it does not provide sufficient depth or focus on tool utilization in a context relevant to the category.",
    "level": "Ignored"
  },
  "Accountability": {
    "resourceId": "ffJaR9AaTl7",
    "category": "Accountability",
    "calculated_at": "2025-05-01T14:48:24",
    "ai_confidence": 82.0,
    "ai_mentions": 60.0,
    "ai_alignment": 90.0,
    "ai_depth": 90.0,
    "non_ai_confidence": null,
    "final_score": 82.0,
    "reasoning": "The content discusses the differentiation between human agency and AI agency, particularly emphasizing the importance of human accountability in strategic decision-making and system adaptation. It explicitly mentions accountability in the context of human responsibility for systemic impacts and outcomes, which is a crucial aspect of the 'Accountability' category. The alignment with key themes of the category, such as the ownership of outcomes and the risks of weak accountability due to overdelegating tasks to AI, is strong. The depth of the discussion is extensive, providing detailed insights into how accountability operates within the framework of adaptive systems. This makes the content a good fit for the 'Accountability' category.",
    "level": "Primary",
    "reasoning_summary": "This content is well-suited to the 'Accountability' category, as it explores the distinction between human and AI roles, highlighting the necessity for humans to remain responsible for decisions and outcomes. It thoroughly addresses how accountability should be maintained, especially when adapting systems or delegating tasks to AI, making it highly relevant to the category’s core themes."
  },
  "Philosophy": {
    "resourceId": "ffJaR9AaTl7",
    "category": "Philosophy",
    "calculated_at": "2025-05-01T14:48:24",
    "ai_confidence": 87.0,
    "ai_mentions": 15,
    "ai_alignment": 34,
    "ai_depth": 32,
    "non_ai_confidence": null,
    "final_score": 87.0,
    "reasoning": "The content aligns strongly with the category of Philosophy as it discusses the foundational beliefs surrounding human agency and AI agency within adaptive systems. It emphasizes the importance of human strategic intent versus AI's role in tactical optimization, exploring the ethical implications and responsibilities inherent in this relationship. The depth of discussion covers various layers of decision-making and the risks associated with overdelegating to AI, framing it within a philosophical context of agency and accountability. It explicitly discusses the 'why' behind these practices and the cultural implications for organizations, meeting the essence of philosophical exploration as required by the category.",
    "level": "Primary",
    "reasoning_summary": "This content is a great fit for the Philosophy category, as it delves into fundamental questions about human and AI agency, ethical responsibilities, and the reasoning behind organisational practices. By exploring the deeper 'why' and cultural impacts, it clearly engages with philosophical themes around decision-making, accountability, and the balance between human intent and AI optimisation."
  },
  "Discipline": {
    "resourceId": "ffJaR9AaTl7",
    "category": "Discipline",
    "calculated_at": "2025-05-01T14:48:24",
    "ai_confidence": 58.0,
    "ai_mentions": 2,
    "ai_alignment": 60.0,
    "ai_depth": 55.0,
    "non_ai_confidence": null,
    "final_score": 58.0,
    "reasoning": "The content discusses the role of human agency vs. AI in adaptive systems, highlighting the importance of human accountability and strategic intent, which ties into governance structures and ethical considerations in a discipline. However, while it touches on principles of organisation and decision-making, it does not delve deeply into the implementation of Agile, DevOps, or Lean methodologies as specific disciplines. The mention of 'operational discipline' suggests some alignment, but the focus does not exclusively remain on established disciplines and their evolution, which lowers the overall confidence score.",
    "level": "Tertiary"
  },
  "Artifact": {
    "resourceId": "ffJaR9AaTl7",
    "category": "Artifact",
    "calculated_at": "2025-05-01T14:48:24",
    "ai_confidence": 12.0,
    "ai_mentions": 10.0,
    "ai_alignment": 5.0,
    "ai_depth": 3.0,
    "non_ai_confidence": null,
    "final_score": 12.0,
    "reasoning": "The content focuses primarily on the roles of human and AI agency within adaptive systems, discussing strategic and tactical dimensions rather than specific artifacts relevant to Agile or similar frameworks. While it hints at the importance of agency and decision-making processes, it does not explicitly address artifacts or their management, structure, or purpose in a way that aligns with the category. The discussion is more abstract and philosophical, without connecting to key artifact types or best practices that are central to the 'Artifact' classification.",
    "level": "Ignored"
  },
  "Observability": {
    "resourceId": "ffJaR9AaTl7",
    "category": "Observability",
    "calculated_at": "2025-05-01T14:48:24",
    "ai_confidence": 15.0,
    "ai_mentions": 0,
    "ai_alignment": 20.0,
    "ai_depth": 10.0,
    "non_ai_confidence": null,
    "final_score": 15.0,
    "reasoning": "The content primarily focuses on the relationship between human and AI agency in adaptive systems. While it mentions concepts of decision-making and adaptation, it does not discuss observability in the context of software systems, metrics, logs, traces, or tools that enhance observability. The themes presented do not align with the key topics outlined for the observability category, such as monitoring solutions or best practices in Agile and DevOps environments. Overall, the content does not meet the criteria for a robust discussion on observability.",
    "level": "Ignored"
  },
  "Practice": {
    "resourceId": "ffJaR9AaTl7",
    "category": "Practice",
    "calculated_at": "2025-05-01T14:48:24",
    "ai_confidence": 15.0,
    "ai_mentions": 0,
    "ai_alignment": 10.0,
    "ai_depth": 5.0,
    "non_ai_confidence": null,
    "final_score": 15.0,
    "reasoning": "The content primarily discusses the philosophical and theoretical aspects of human and AI agency in adaptive systems rather than presenting actionable practices or techniques for enhancing team performance. It mentions concepts that could be tangentially related to collaborative decision-making or optimization, but it does not explore specific practices that teams can implement to improve their effectiveness or collaboration. Therefore, it scores low in direct mentions, alignment with the core themes of the category, and depth of discussion related to practical techniques.",
    "level": "Ignored"
  },
  "Method": {
    "resourceId": "ffJaR9AaTl7",
    "category": "Method",
    "calculated_at": "2025-05-01T14:48:24",
    "ai_confidence": 25.0,
    "ai_mentions": 100.0,
    "ai_alignment": 35.0,
    "ai_depth": 20.0,
    "non_ai_confidence": null,
    "final_score": 25.0,
    "reasoning": "The content discusses the roles of human agency and AI agency in adaptive systems, focusing mainly on strategic versus tactical dimensions. While it briefly mentions operationalizing agency boundaries, it lacks a detailed, structured, step-by-step method specific to Agile, Lean, or DevOps practices. Instead, it presents conceptual discussions about agency in systems without offering procedural insights or practical methods for implementation.",
    "level": "Ignored"
  },
  "Strategy": {
    "resourceId": "ffJaR9AaTl7",
    "category": "Strategy",
    "calculated_at": "2025-05-01T14:48:24",
    "ai_confidence": 85.0,
    "ai_mentions": 16,
    "ai_alignment": 35,
    "ai_depth": 30,
    "non_ai_confidence": null,
    "final_score": 85.0,
    "reasoning": "The content extensively discusses the roles of human agency in defining strategy and purpose within adaptive systems, distinct from AI's tactical optimisation role. The text emphasizes strategic intent, decision-making, and the importance of human oversight in a world increasingly influenced by AI, which strongly aligns with strategic discussions. There are multiple direct mentions of strategic aspects such as 'strategic intent', 'purpose setting', and the 'collapse of strategic sensing', indicating a thorough engagement with strategic concepts. However, while the depth of discussion is rich, some areas delve more into tactical implications rather than purely strategic frameworks, slightly reducing the overall alignment score. Overall, it is a deep strategic analysis with clear references to high-level decision-making frameworks.",
    "level": "Primary",
    "reasoning_summary": "This content is a strong fit for the strategy category, as it focuses on human-driven strategic intent, purpose setting, and oversight in adaptive systems, especially in contrast to AI’s tactical roles. While some tactical elements are present, the main emphasis remains on high-level decision-making and strategic frameworks, making it highly relevant to strategic discussions."
  },
  "Model": {
    "resourceId": "ffJaR9AaTl7",
    "category": "Model",
    "calculated_at": "2025-05-01T14:48:24",
    "ai_confidence": 32.0,
    "ai_mentions": 12,
    "ai_alignment": 24,
    "ai_depth": 20,
    "non_ai_confidence": null,
    "final_score": 32.0,
    "reasoning": "The content discusses human and AI agency in adaptive systems, emphasizing strategy and purpose, but does not explicitly mention any models, frameworks, or structures that directly relate to the 'Model' category. While it touches on the conceptual roles of human and AI agency suggesting a layered approach to decision-making, it lacks the detailed discussion of specific models or frameworks aligned with Agile, DevOps, or Lean principles. Thus, while there are elements of discussion that are conceptually relevant, they do not fully align with the classification requirements.",
    "level": "Ignored"
  },
  "Framework": {
    "resourceId": "Human-AI-Agency",
    "category": "Framework",
    "calculated_at": "2025-05-01T14:48:24",
    "ai_confidence": 22.0,
    "ai_mentions": 10.0,
    "ai_alignment": 40.0,
    "ai_depth": 30.0,
    "non_ai_confidence": null,
    "final_score": 22.0,
    "reasoning": "The content primarily discusses the roles of human and AI agency within adaptive systems, emphasizing strategic vs. tactical distinctions rather than framing discussions around specific frameworks like Agile or Lean. While it touches on themes related to human decision-making and system adaptation, it lacks a clear focus on established frameworks or methodologies that would align it with the Framework category. The mention of agency does not translate into a structured framework discussion as defined in the category description, thus resulting in a low confidence score.",
    "level": "Ignored"
  },
  "Tenet": {
    "resourceId": "ffJaR9AaTl7",
    "category": "Tenet",
    "calculated_at": "2025-05-01T14:48:24",
    "ai_confidence": 23.0,
    "ai_mentions": 5,
    "ai_alignment": 15,
    "ai_depth": 30,
    "non_ai_confidence": null,
    "final_score": 23.0,
    "reasoning": "The content discusses the roles and boundaries of human and AI agency in adaptive systems, which could relate to decision-making processes. However, it does not explicitly present actionable guiding rules or doctrines that are prescriptive for organizations, which is essential for the Tenet category. While there are mentions of strategic direction and accountability, the overall focus is not on the application of specific tenets in methodologies like Agile, Lean, or DevOps.",
    "level": "Ignored"
  },
  "Capability": {
    "resourceId": "ffJaR9AaTl7",
    "category": "Capability",
    "calculated_at": "2025-05-01T14:48:24",
    "ai_confidence": 38.0,
    "ai_mentions": 5,
    "ai_alignment": 45.0,
    "ai_depth": 30.0,
    "non_ai_confidence": null,
    "final_score": 38.0,
    "reasoning": "The content primarily discusses the distinct roles of human and AI agency in adaptive systems. While there are elements that touch on capabilities, such as the need for strategic intent and the cultivation of human discernment, it largely focuses on the roles and accountabilities of humans versus AI, which falls outside the core themes of capability. Additionally, it emphasizes the risks of relying too heavily on AI without placing enough importance on the development of enduring competencies, thus leading to a lower confidence score in terms of depth and alignment with the Capability category.",
    "level": "Ignored"
  },
  "Service Level Expectation": {
    "resourceId": "ffJaR9AaTl7",
    "category": "Service Level Expectation",
    "calculated_at": "2025-05-06T20:54:05",
    "ai_confidence": 0,
    "ai_mentions": 0,
    "ai_alignment": 0,
    "ai_depth": 0,
    "ai_intent": 0,
    "ai_audience": 0.1,
    "ai_signal": 0.1,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 0.0,
    "reasoning": "The content does not mention Service Level Expectation (SLE) or reference any of its key concepts, such as cycle time ranges, probability-based forecasts, or Agile/Scrum/Kanban flow metrics. The focus is entirely on delineating human versus AI agency in adaptive systems, with no connection to SLE definition, calculation, use, or improvement in Agile contexts. There are no references to Kanban, Scrum, SLE metrics, or relevant authoritative sources. The content may tangentially relate to adaptation in systems, but this does not overlap with Service Level Expectation as defined. Accordingly, all relevant scores are near or at zero, and confidence is 0 on the required scale.",
    "level": "Ignored"
  },
  "Ethos": {
    "resourceId": "ffJaR9AaTl7",
    "category": "Ethos",
    "calculated_at": "2025-05-13T13:52:44",
    "ai_confidence": 87.7,
    "ai_mentions": 7.2,
    "ai_alignment": 9.3,
    "ai_depth": 9.1,
    "ai_intent": 8.4,
    "ai_audience": 8.7,
    "ai_signal": 8.6,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 88.0,
    "reasoning": "The content deeply explores the foundational convictions underpinning adaptive systems, especially the non-negotiable role of human agency in strategy, purpose, and accountability. It distinguishes between genuine ethos (human-led adaptation, ethical stewardship) and superficial optimisation (AI-driven tactics), mapping these to system behaviour and outcomes. The discussion is thorough, with explicit examples of how ethos sustains resilience and prevents system fragility. While 'ethos' is not named directly, the entire argument is anchored in system-level beliefs and disciplined stances, targeting leaders, coaches, and practitioners responsible for system evolution. The content is highly aligned, focused, and relevant, with minimal off-topic material. No penalties were applied as the tone, currency, and framing are consistent with the category definition.",
    "reasoning_summary": "This content robustly examines the core beliefs and disciplined boundaries that define sustainable, adaptive systems—emphasising human agency as the foundation for strategy, accountability, and ethical stewardship. Its depth and focus make it a strong fit for the Ethos category, despite not naming 'ethos' directly.",
    "level": "Primary"
  },
  "First Principal": {
    "resourceId": "ffJaR9AaTl7",
    "category": "First Principal",
    "calculated_at": "2025-05-13T13:52:50",
    "ai_confidence": 67.6,
    "ai_mentions": 2.2,
    "ai_alignment": 7.8,
    "ai_depth": 7.5,
    "ai_intent": 7.0,
    "ai_audience": 7.2,
    "ai_signal": 6.9,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 68.0,
    "reasoning": "The content does not explicitly mention 'first principles' or foundational theorists, resulting in a low Direct Mentions score. However, it strongly aligns with the concept of immutable constraints by arguing that human agency in strategy and accountability is non-negotiable in adaptive systems, and that AI's role is strictly tactical. The discussion is deep, mapping agency to system layers and warning of the consequences of violating these boundaries, which echoes the spirit of first principles. The intent is to establish foundational boundaries, and the audience is practitioners and strategists in adaptive systems, overlapping with Lean/Agile/DevOps professionals. The content is focused, but lacks explicit Lean/Agile/Scrum/DevOps context or direct first principle terminology, which limits the confidence score.",
    "reasoning_summary": "While the content doesn't directly reference first principles, it robustly argues for immutable boundaries between human and AI agency, echoing the spirit of foundational constraints. Its depth and alignment are strong, but lack of explicit first principle language and Lean/Agile context lowers overall confidence.",
    "level": "Secondary"
  },
  "Customer Focus": {
    "resourceId": "ffJaR9AaTl7",
    "category": "Customer Focus",
    "calculated_at": "2025-05-13T13:52:56",
    "ai_confidence": 23.7,
    "ai_mentions": 0.6,
    "ai_alignment": 2.2,
    "ai_depth": 2.7,
    "ai_intent": 2.1,
    "ai_audience": 7.2,
    "ai_signal": 6.8,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 24.0,
    "reasoning": "The content focuses on the distinction between human and AI agency in adaptive systems, emphasising strategic intent, accountability, and the risks of overdelegating adaptation to AI. While it discusses value, purpose, and accountability, it does not explicitly address customer outcomes, feedback loops, or the measurement of value from the customer's perspective. The main themes are about organisational resilience, ethical stewardship, and system design, not about anchoring product development in customer value. There are no direct mentions of customer focus or related practices, and the alignment is tangential at best. The audience is technical and strategic, which partially overlaps with the category, but the content's intent and depth do not sufficiently address the core of Customer Focus.",
    "reasoning_summary": "This content explores human versus AI roles in adaptive systems, focusing on strategy, accountability, and organisational resilience. It does not directly address customer outcomes or value, so its fit with the Customer Focus category is low.",
    "level": "Ignored"
  },
  "Definition of Workflow": {
    "resourceId": "ffJaR9AaTl7",
    "category": "Definition of Workflow",
    "calculated_at": "2025-05-18T20:52:47",
    "ai_confidence": 7.8,
    "ai_mentions": 0.2,
    "ai_alignment": 1.5,
    "ai_depth": 1.8,
    "ai_intent": 1.2,
    "ai_audience": 1.0,
    "ai_signal": 1.0,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 8.0,
    "reasoning": "The content focuses on the distinction between human and AI agency in adaptive systems, emphasising strategic versus tactical roles. There is no explicit mention or discussion of the Definition of Workflow as articulated in Kanban or agile contexts. The main themes are about agency, accountability, and system adaptation, not about explicit workflow agreements, WIP limits, entry/exit criteria, or Kanban practices. The audience is more general (organisational leaders, strategists) rather than Kanban or agile practitioners. There is no signal of workflow policy, process visualisation, or improvement frameworks as defined in the category. Thus, the content is only tangentially related, if at all, to the Definition of Workflow.",
    "reasoning_summary": "This content is about human and AI roles in adaptive systems, not about the Definition of Workflow in Kanban or agile. It lacks explicit references, alignment, or depth on workflow policies, so confidence in this classification is very low.",
    "level": "Ignored"
  },
  "Objective Key Results": {
    "resourceId": "ffJaR9AaTl7",
    "category": "Objective Key Results",
    "calculated_at": "2025-06-03T12:22:23",
    "ai_confidence": 20,
    "ai_mentions": 0.0,
    "ai_alignment": 2.0,
    "ai_depth": 1.5,
    "ai_intent": 1.5,
    "ai_audience": 4.0,
    "ai_signal": 3.5,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 20.0,
    "reasoning": "The content provided primarily focuses on differentiating the roles and boundaries between human agency (strategy setting, purpose, adaptation, accountability) and AI agency (tactical optimisation, efficiency). There are no direct mentions or explicit references to OKRs. Conceptual themes loosely touch upon strategic intent, adaptation, and accountability, which have some theoretical alignment to OKRs' strategic and measurable elements. However, the content does not discuss or reference the OKR framework explicitly nor extensively explore its core ideas such as measurable key results, alignment, transparency, stretch goals, or OKR implementation. Depth and intent scores are relatively low due to absence of meaningful exploration of the OKR framework. Audience relevance is somewhat moderate, as content may still be relevant to organizational strategists or leaders who could potentially be familiar with OKRs. The content overall remains off-topic from an explicit OKR perspective, hence relatively low meaningful alignment scores across dimensions.",
    "reasoning_summary": "Content explores strategy and optimisation roles for humans and AI without explicitly mentioning or deeply aligning with OKR principles or practices, making it largely unrelated to the OKR category.",
    "level": "Ignored"
  },
  "Product Developer": {
    "resourceId": "ffJaR9AaTl7",
    "category": "Product Developer",
    "calculated_at": "2025-06-23T09:01:19",
    "ai_confidence": 22.47,
    "ai_mentions": 0.3,
    "ai_alignment": 2.6,
    "ai_depth": 2.9,
    "ai_intent": 2.5,
    "ai_audience": 6.3,
    "ai_signal": 2.4,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 22.0,
    "reasoning": "The content offers a thorough discussion of human and AI agency in adaptive systems, with strong emphasis on accountability, agency boundaries, and decision layers. However, it never references Product Developers, their formal responsibilities, or core concepts from product development frameworks. The discussion of accountability and adaptation overlaps a little with Product Developer mindset, but neither the terminology nor the context fit the tag requirements. Most content is focused on general organisational strategy, AI/automation limits, and ethical stewardship, which are tangential to the core scope of Product Developer as a role and accountability per the provided definition.",
    "reasoning_summary": "The content explores agency, accountability, and organisational adaptation, but does not address Product Developer roles, responsibilities, or context. Its relevance is tangential, making it a poor fit for the Product Developer category.",
    "level": "Ignored"
  },
  "Collective Intelligence": {
    "resourceId": "ffJaR9AaTl7",
    "category": "Collective Intelligence",
    "calculated_at": "2025-06-23T09:01:13",
    "ai_confidence": 62.6,
    "ai_mentions": 3.9,
    "ai_alignment": 7.3,
    "ai_depth": 6.7,
    "ai_intent": 5.6,
    "ai_audience": 7.1,
    "ai_signal": 7.6,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 63.0,
    "reasoning": "The content deeply explores the interplay between human and AI agency but frames this as a strict separation, explicitly stating that humans govern strategic layers while AI is relegated to tactics. There is some acknowledgement of synergy but not partnership; the model is hierarchical not collaborative. Collective Intelligence, as defined, focuses on human-AI collaboration, shared understanding, and the emergence of capabilities through partnership. While the content is highly relevant to the boundaries and risks of human-AI roles, it rarely discusses collaborative patterns, reciprocal team dynamics, or shared cognition—thus, the depth and alignment scores are solid but not high. Audience and signal are strong due to professional tone and focus. Direct mentions are limited: 'collaboration' and 'partnership' are referenced as contrasts rather than embraced. Intent is informative for the correct audience but with an argument that challenges the core of 'collective intelligence' as defined.",
    "reasoning_summary": "This content thoroughly articulates distinctions between human strategic agency and AI tactical optimisation. While closely related to human-AI team dynamics, it emphasizes boundaries more than collaboration, offering limited focus on true collective intelligence as defined by shared human-AI partnership.",
    "level": "Secondary"
  },
  "Agentic Engineering": {
    "resourceId": "ffJaR9AaTl7",
    "category": "Agentic Engineering",
    "calculated_at": "2025-07-23T12:05:09",
    "ai_confidence": 87.37,
    "ai_mentions": 7.8,
    "ai_alignment": 9.6,
    "ai_depth": 9.1,
    "ai_intent": 8.5,
    "ai_audience": 8.8,
    "ai_signal": 8.9,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 87.0,
    "reasoning": "The content deeply explores the distinction and necessary balance between human and AI agency, directly aligning with Agentic Engineering's focus on designing systems that maximize both forms of agency. Discussion covers strategic intent, decision-making frameworks, ethical boundaries, and operational governance—all central to Agentic Engineering. Core topics like feedback-driven adaptation, ethical AI integration, and systemic observability are clearly addressed. There are no penalties; the content is timely, direct, and very thematically aligned with little off-topic material. Audience is practitioners and strategists interested in the philosophy and operationalisation of human-AI collaboration in adaptive systems.",
    "reasoning_summary": "This content offers a rigorous, nuanced discussion on the operational boundaries between human and AI agency in adaptive systems—a perfect thematic fit for Agentic Engineering, with strong depth, directness, and alignment to key audience concerns.",
    "level": "Primary"
  },
  "Agentic Software Delivery": {
    "resourceId": "ffJaR9AaTl7",
    "category": "Agentic Software Delivery",
    "calculated_at": "2025-08-07T06:10:04",
    "ai_confidence": 66.05,
    "ai_mentions": 1.3,
    "ai_alignment": 7.1,
    "ai_depth": 6.9,
    "ai_intent": 6.5,
    "ai_audience": 7.5,
    "ai_signal": 8.0,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 66.0,
    "reasoning": "The content offers a nuanced discussion of human and AI agency in adaptive systems and clearly maps boundaries and collaboration between them. It references key concepts like context-aware AI, human-in-the-loop processes, governance, and adaptation boundaries—anchoring on issues of accountability, outcome stewardship, and real-world operational frameworks. However, it generally speaks at a conceptual or philosophical level and only indirectly references integration into software delivery workflows. There are no direct mentions of 'Agentic Software Delivery' or concrete implementation within modern engineering practices like CI/CD or backlog triage. The fit is strong for conceptual alignment, intent, and depth relative to the agency theme, but weaker for direct references and practical delivery specifics. The intended audience is practitioners or strategists grappling with AI integration, aligning decently. The piece does not reference obsolete practices, and the tone is earnest and supportive of considered, agency-aware integration.",
    "reasoning_summary": "Content strongly explores human vs. AI agency boundaries in adaptive systems, aligning conceptually with agentic delivery themes. Lacks direct references or practical delivery focus, making the category fit partial but relevant for audience and intent.",
    "level": "Secondary"
  },
  "Product Operating Model": {
    "resourceId": "ffJaR9AaTl7",
    "itemId": "ffJaR9AaTl7",
    "category": "Product Operating Model",
    "calculated_at": "2025-11-24T18:57:07",
    "ai_confidence": 60.1,
    "ai_mentions": 1.4,
    "ai_alignment": 7.3,
    "ai_depth": 6.9,
    "ai_intent": 6.3,
    "ai_audience": 7.6,
    "ai_signal": 6.8,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 60.0,
    "reasoning": "The content focuses on delineating human and AI agency in adaptive systems, emphasizing the importance of human-led strategy versus AI-led optimization. It discusses decision-making layers, governance boundaries, accountability, and organizational risk when agency is delegated improperly—closely related to foundational elements of product operating models (especially those considering modern AI’s insertion into governance and practice). However, it does not directly mention or structure the argument specifically as 'product operating model' design or transformation, instead offering a conceptual framework that could underpin such models. It aligns with the category's themes (governance, roles, escalation frameworks, strategic vs tactical boundaries), and targets decision-makers who would influence or operate product operating models. Still, the lack of explicit operating model language and only indirect structure reduce 'mentions' and slightly 'depth'.",
    "reasoning_summary": "Content aligns with product operating model concepts—governance roles, agency boundaries, escalation practices—but is structured around human/AI agency in adaptive systems, not expressly product operating models. Fit is partial, mainly conceptual.",
    "level": "Tertiary"
  },
  "Operating Model": {
    "resourceId": "ffJaR9AaTl7",
    "itemId": "ffJaR9AaTl7",
    "category": "Operating Model",
    "calculated_at": "2025-11-24T18:57:10",
    "ai_confidence": 69.337,
    "ai_mentions": 1.7,
    "ai_alignment": 7.5,
    "ai_depth": 7.6,
    "ai_intent": 6.3,
    "ai_audience": 6.9,
    "ai_signal": 7.0,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 69.0,
    "reasoning": "The content does not explicitly mention 'operating model' but explores how organizations should structure human and AI agency within adaptive systems, including clear distinctions of layers, accountabilities, and governance. It discusses the allocation of strategic vs tactical roles and warns about the risks of misalignment, with an emphasis on embedding boundaries into system design and governance. Concepts such as decision rights, accountability, and operational discipline are core to operating model theory, and the content uses language aligned with organizational structure and value delivery. However, it does not thoroughly reference operating model frameworks, components, or transformation case studies and focuses more on agency distinctions than the broader operating model design or its evolution. Therefore, the fit is strong conceptually but partial in explicit scope and depth of operating model-focused analysis.",
    "reasoning_summary": "Strong conceptual alignment with operating model (roles, governance, accountability) but lacks explicit naming and deep exploration of operating model design or transformation. Partial but relevant fit; overlaps in intent and audience.",
    "level": "Secondary"
  },
  "AI Product Operating Model": {
    "resourceId": "ffJaR9AaTl7",
    "itemId": "ffJaR9AaTl7",
    "category": "AI Product Operating Model",
    "calculated_at": "2025-11-24T18:57:14",
    "ai_confidence": 53.55,
    "ai_mentions": 2.8,
    "ai_alignment": 5.8,
    "ai_depth": 7.1,
    "ai_intent": 4.7,
    "ai_audience": 7.4,
    "ai_signal": 9.4,
    "ai_penalties_applied": false,
    "ai_penalty_points": 0,
    "ai_penalty_details": "none",
    "final_score": 54.0,
    "reasoning": "The content provides an articulated distinction between human and AI agency in adaptive systems with implications for operational boundaries, governance, and escalation—key concerns in AI product operating models. However, it does not explicitly reference operating models, frameworks, or organizational implementation for managing the end-to-end AI product lifecycle, nor does it directly engage with AI model governance, lifecycle management, or organizational design. The themes are conceptually aligned (decision accountability, boundaries, and risk) but not operationalized with actionable AI product operating structures, frameworks, or role definitions. It targets executive/strategic audiences relevant to the category and remains highly focused, but depth is more philosophical/strategic than procedural/practical. Mentions of the category are indirect (no explicit 'AI Product Operating Model' references).",
    "reasoning_summary": "Content strongly aligns with the category's philosophical underpinnings (agency, accountability, escalation boundaries) but lacks explicit discussion of operational models, frameworks, or practical implementation for AI product lifecycle management. Fit is partial.",
    "level": "Tertiary"
  }
}