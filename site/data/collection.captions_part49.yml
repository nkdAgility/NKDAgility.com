- id: s4h2K62V_d0
  title: Code Reviews for Quality Assurance
  description: Discover how to integrate quality from the start in software development with Martin Hinshelwood. Embrace shifting left for better products and reduced technical debt!
  captions: |-
    So the concept that we talk about a lot from the perspective of modern software engineering that enables you to push quality, build quality, build security, build these things in from the beginning, build it in rather than testing it in later, is the concept of shift left. 

    Moving quality assurance earlier in the development cycle. Now shifting left is often controversial because when do you stop? How far left should you shift? As far as I'm concerned, it's all the way to the left. The people writing the code should be the ones that are writing the tests. They should be the ones that are running the tests, that are taking the fallout from the tests, that are all the way through to, "I need to support this." If I wrote the code, I need to support this in production. 

    Now usually in bigger products, that's team-based, right? So the team that's working on this feature or this capability or this part of the product, this area of the product, are the ones that also support it in production. But I don't just mean support it in production. They're dealing with the bugs, they're dealing with almost with the support requests. It's probably being, in a big organisation, it's probably being filtered through some kind of levels in a centre, but they need to handle those support requests and they need to look at the telemetry for the experiments that they're doing and figure out whether their experiments are successful, whether things are right. Do we have the right alerts to know when something's gone wrong with this feature or capability? 

    That is all shifted left to the engineering team that is building the product. They are building it, running it, supporting it, maintaining it, validating it going forward into the future, which means you need all of those skills that in the past you maybe had dispersed around the organisation. You need all of those skills on the engineering team. 

    So usually, you have a lot of people who can code, right? But you also have people who have security expertise, who have testing and validation expertise, who have user experience expertise. All of those things move further towards the source of all of our problems and all of our successes, which is building the code. 

    In the modern engineering space, in agile practices, there are a bunch of tools and techniques that have kind of grown in that space to be really popular, but also very contentious because people don't want to put in the work because they're more effort to do right. So it seems like we're going to deliver less stuff because we're spending more time on the engineering space. But because we don't have that arc, or I don't have, is probably too strong, we have less of that arc of people finding problems in production. 

    We have less of that arc of reduction in brand. Everybody's losing their mind because we're not being presented well in front of our customers. We have less of that. We have an opportunity here to do stuff, but we need to fix this stuff first because we never fixed it before. We just pushed it out and we didn't bother with it. All of those things are more effective. 

    Therefore, when we start shifting left and we're doing more things from the perspective of the engineering team, it takes longer to do each thing the first time, but each thing is done more right. Because the lack of doing things—I'm trying to avoid the term technical debt because I'm including more things in this story than just technical debt—are poor engineering practices and poor quality output, right? For whatever reason, including technical debt is an exponential problem, not a linear problem. 

    Think about if you built the first story of a building with substandard materials, and then you built the second story of a building with substandard materials. How far are you going to get before the building collapses? Right? Now we have an advantage in the software industry that we can build the first level. We can build the first level of the building quickly, and when we go to build the second level of the building, we can go back and refactor, redo any of the things that we need to enhance the support of the second story of the building, right? 

    This is a terrible analogy, but it's working for me just now. It enables us, as you don't know how many stories your building's going to have when you're doing software engineering. If you want to look up the building analogy, look up the Chrysler Building and how the Chrysler Building was built because it was effectively built in an agile manner. They didn't know how many stories they were going to put on it. They didn't have all the materials up front. They didn't have a plan up front of the building and what they were going to do. 

    It's a great example of this in that physical space, but in the virtual space, we can do test-driven development. Test-driven development is not about building better code; it's about having better architectures. We create better architectures because we only create what we need to support the things that we need it to do. 

    Doing pair programming means that we've got more eyes on the code. It feels like it's going to be less productive, but productivity is not a moment in time for the engineering team. It needs to be looked at across the whole system. We can take what seems like a hit on productivity to do pair programming because there's two people doing things. Why can't we have them both doing their own thing? But we have less problems that we need later, which are much more expensive than the pair programming is at the start, right? 

    Same with test-driven development. We build better architectures, so it's easier to add stuff going forward into the future. And when we do need to refactor our architecture, we have a test infrastructure to validate at each change as we make it to make sure that we're not breaking existing things, existing stories that are supposed to happen. 

    These tools are fundamentally part of that story of how do we build a practice of engineering excellence within our organisation and enable great things to happen. We want to build great products. We want to make great profits from the great products that we build, and the only way we're going to do that is if we do things in a way that enables us through the future rather than us getting to a point where we're drowning under the weight of all the crap that we've shipped to production over the years. 

    And if you are currently drowning under the weight of all of that, as the Azure DevOps team was when they moved from their waterfall model that they'd been doing for 200 what let C to the for five years, and then they moved to continuous delivery to production, they had to go pay back all of those loans that they'd taken out for the future, right? 

    But what they found was that in paying back those loans, they became so much more effective because of the exponential cost of taking out those loans. They became so much more effective that in actual fact, the cost of paying it back was completely irrelevant to the benefit and value that they got from it being paid back. That makes sense, right? They went from 25 features to production each year with the same number of people to nearly 50 features to production each year. 

    And even taking four years to get there, holy moly, I would rather have 800 features than 25 features, right? And yes, the features are smaller, but it's more experiment, right? It's more chances to get something right. It's more ideas to see if they connect with the customer. 

    So in the old ways, you have to stick with and focus on something even when it's not quite the most optimal thing. But if you're doing lots of little experiments, you can find, "Oh, this thing resonates really much more than these other things with the customer. Let's do more on that until it stops resonating," and then we move on to something else and move on to something else, and we build a better product with more capabilities. 

    So there are lots of tools and techniques that you can use within the context of modern software engineering practices to build quality in from the start and enabling your teams to be as effective as possible to choose the practices because they need to be able to choose the practices that work best for their context. That's something that naked agility can help you with.
- id: S4zWfPiLAmc
  title: 3 best ways to wreck Kanban. Use vanity metrics.
  description: Unlock Kanban success by ditching vanity metrics! Discover actionable insights to enhance your workflow and drive continuous improvement. Watch now!
  captions: |-
    One of the main ways to mess up the implementation of your cand strategy and not get the value that you're expecting, i.e. we're doing better than we were before, is to not look at the data. I see this in team after team. They say they're doing caman, but all they have is a board with some columns. They don't have any whip limit, and that's it. They just have a board with some columns and they call it caman. I'm sorry to say, but that is definitely not enough to be a Canan strategy. It's just not enough. 

    One of the key things that they miss out is not looking at the data or using vanity metrics. Oh my goodness me, the vanity metrics coming up my ears! Story points, velocity, burndowns—get rid of all that crap! It's terrible. It's not going to help you maximize the flow of value for your customers. It's not going to help you see what's going on enough to be able to ask more interesting questions that allow you to change your process in order to optimize it. Those metrics are absolutely not good enough. 

    In that story, there are four key metrics in cat that you should be looking at. They're all fairly straightforward, easy to collect. Most of us are using digital tools today, so you effectively only need one piece of data—well, technically two pieces of data—for each piece of work that flows through your system: the date and time when it started and the date and time when it finished. That's it! Those two pieces of data. If you're using Jira, if you're using Azure DevOps, if you're using Trello, all of that data is collected for you. It's in there, and you just need to render it in a way that makes sense. 

    So if it's started but not finished, it's part of your working process, right? That's your whip. If it's started but not finished, you can also see how old it is—how long it's been sitting there. That's work item age. If it's finished, if it's crossed that finish line, you know how many items you've delivered per unit of time. Right? So over each 30-day period, how many items do we deliver? There you go, we've got that number just by virtue of that end date. And if you have the start date and the end date, you also have the cycle time—how long it actually took to deliver each of those items. 

    Then you can do a bunch of math, which is pretty standardized. You can go or look at a bunch of pieces of information in that, but without that data, you can't see what's going on. Right? Transparency, inspection, and adaption—where's the transparency if you don't have the data or you decide to look at vanity metrics or metrics that don't actually provide you with anything actionable? That's the key. It is the data. What are you doing with the data you're looking at? How are you changing the way you work based on the outcome of that data? 

    If you're not, you're either not doing it or you're not looking at the right data because the data is not telling you something that helps you. Stop looking at back data as part of your cband strategy. Stop looking at vanity metrics and improve your process. If you're struggling to implement a cband strategy, we can help or help you find somebody who can. Click on the link below and get in touch.
- id: S7Xr1-qONmM
  title: Why do you think the PSU course has become so popular for product development?
  description: Discover why the PSU course is transforming product development by integrating UX into Scrum teams, empowering them to create valuable customer solutions.
  captions: ""
- id: Sa7uw3CX_yE
  title: The Tyranny of Taylorism and how to spot agile lies for The Future of Work in Scotland
  description: Explore the pitfalls of Taylorism and uncover the truths behind agile methodologies in Scotland's evolving work landscape. Join the conversation!
  captions: ""
- id: sAKCLQ38GzA
  title: Does 'starting with why' - Simon Sinek - really matter?
  description: Explore the essence of motivation beyond money! Discover how 'starting with why' fuels intrinsic drive and purpose in the workplace.
  captions: ""
- id: sb9RsFslUfU
  title: How did you know you were ready to transition from DevOps practitioner to DevOps Consultant?
  description: Discover Martin Hinshelwood's journey from DevOps practitioner to consultant, exploring mastery through empowering others in this insightful video.
  captions: ""
- id: sBBKKlfwlrA
  title: Professional Scrum with Nexus (SPS) with Certification - Learn skills to overcome scaling challenges
  description: Master scaling Scrum with the Scaled Professional Scrum course! Overcome challenges and enhance collaboration across teams for greater value creation.
  captions: ""
- id: sbr8NkJSLPU
  title: 3 core practices of Kanban Defining and visualizing a workflow
  description: "Unlock Kanban's potential! Learn to define and visualise workflows for enhanced efficiency, clarity, and team collaboration. Streamline your processes today! \U0001F680"
  captions: |-
    One of the core practices of Kanban is to define and visualize your workflow. This is really important because it sets the groundwork for us all, all the people that are participating in the work in the system, as all agreeing what is it that we do and how do we currently work. 

    So there are lots of pieces of information that we need to pull together to figure this out. One might be like, what goes on our board? That might be a great question to ask. What types of work go on our board? Another question might be, what are the current stages that all of our work goes through? Not a specific piece of work, but all of our work goes through that would generate the columns within your Kanban board. 

    But also asking additional questions like, why do we pick this item? How do we pick the item from this column when we go to select things? And what you'll find is that for most teams, most groups of people, they don't have any level of agreement on what that is. Why would you pick something? Lots of people will pick stuff because they think it's cool, because it fits, they think it fits their skill set. 

    But the important questions to be asking are, how do we pick this so we maximize the value of what we're delivering? Is that the right thing to select in order to do that? Is that the right business decision? Answering those questions and coming up with a set of rules for our team—right, these are our decided rules on how we're going to select things, why things move from one column to another, what are the defined states, what are the types of work, and how are we going to control the amount of work that we have in progress—are all part of defining our workflow and explicitly setting policies. 

    That's all part of this defining and visualizing your workflow. Bringing all of those things together allows us to have a way of working. Another way of saying this, another way of saying define and visualize your workflow, is what is your team's way of working? And does everybody on your team agree that that's the way that they should be working? 

    This definition is absolutely fundamental to figuring out how work flows through your system. Right, you'll probably have heard folks talk about having a stable system. Right, you need a stable system. Well, you can't have a stable system if every member of the team decides ad hoc how they're going to do all of the things within the system. We don't have a system; we've got to make it up as you go along system, and that's never going to generate stability. 

    So having a stable system means we've decided what it is, we've written down how we're going to work, we've debated as a team and decided on certain things, and then we visualize that so we can all see what's going on. If you are struggling to implement a Kanban strategy, we can help, or we can help you find somebody who can. Click on the link below.
- id: sdIPSpbeA9w
  title: The Cost Saving Power of Automation
  description: Discover how automation transforms software development by reducing errors and enhancing efficiency. Learn from Azure DevOps' rapid feedback success!
  captions: |-
    Automation is a huge part of enabling the building of software to be more effective, more standardised, more efficient. I don't like that word, but it certainly fits here. The difficulty is that anytime you've got a human doing something, you're going to have mistakes. You're going to have mistakes, you're going to have steps missed, you're going to have things go wrong. 

    Two examples: one's a commercial example and one's just an interesting one. One of the interesting ones, do that one first, is that I used to teach a bunch of testers how to use the Azure DevOps test tools when there was a separate app. It was quite some time ago, and you would think that the superpower of testers would be that they could follow a test script, right? They're going to spend a bunch of work creating the test script, but then they're going to follow that test script to run the test, running a manual test and following those steps, right? 

    So there's 10 steps in the test; they follow those 10 steps, and it either passes or it fails, right? Well, in this training, we had workshops, we had labs, and they had to follow a set of steps on the lab. I kept getting feedback from loads of people in the room, loads of instances of the training. I would get feedback all the time that the labs were wrong, that the labs didn't work, that they were following the steps and the lab doesn't work; it doesn't do what it's supposed to do. 

    This was false; they just weren't following the steps. They were either missing something or clicking the wrong button or not doing whatever it was, right? So those testers, whose job it is to create and then run sets of steps manually, perhaps to validate whether something is built correctly, were manifestly unable to successfully and consistently run through a set of steps. This is just part of being human; it's not a slight on them, it's not a tester thing, it's a human thing. 

    The commercial example is the KN Capital Group, which you might have heard of or maybe not, but they went bankrupt because somebody couldn't follow a set of steps. They had a deployment of a new version of their product; they were Stock Exchange Management trading software. So you can imagine when something goes wrong, the cost of something going wrong is pretty enormous. 

    With $400 million in the bank at the beginning of the day, a deployment happened. It went wrong because the person running the deployment deployed to eight out of the nine servers, and because not all the servers had the latest version of the code, some weird stuff started happening. It took them till the end of the day to figure out what the problem was, which was steps missed, and by that time, they had to declare bankruptcy. They were done, right? 

    The cost of these types of mistakes is enormous. That one is a very extreme example, but the cost of somebody who's running a set of steps—this could be a tester, it could be a coder, it could be UAT, right?—could be a user. They walk through a set of steps and then communicate back that it was wrong; it didn't work. That cost is in investigation from the engineers, in loss of focus on new features because they're not building new stuff for you when they're having to go look at this erroneous problem. 

    What we want instead is automation. The solution to both of these problems is automation. There is an expression I use: if it can be automated, it should be automated, and if it can't be automated, it should be rearchitected. 

    Close the feedback loop. Think about if you were telling a story. I'm thinking code is kind of like telling a story. You're telling a story, you're working on part of this story, and then you submit it to your publisher, and the publisher takes six months to come back with feedback. The feedback is, "This didn't work, that doesn't work, can you change this, can you reformat this?" 

    Now you're much further along in the story; you're working on a different part of it, and now you have to come back to this previous part of the story and get into the mindset of this part of the story and try and ignore the things you learned during the future part of the story, right? The character development has changed, and you have to go back to this old version of the character with an older style of how that character does things. 

    Now you need to do this differently, so now we're trying to make a change back here in the past when we have understanding from the future that would impact how we would do this, but we can't let that impact what we're doing. It's a huge cognitive load, right? And that's in code; it's exactly the same. Exactly the same thing happens when you have to do that. 

    So not only do we want automation in our process to automate tests so that we don't have as many false positives of tests failing or false negatives—anyway, false negatives are probably good and false positives, right?—but we also want to have those tests run as quickly as possible so that we get that feedback loop closed as quickly as possible. Preferably, I'm talking about minutes. That's what I'm talking about here: we want automation that runs in minutes that tells us whether we've passed or failed, preferably seconds, right? Or milliseconds, but minutes at the most, at the outside. 

    A great example of that is the Azure DevOps team. They used to have very long automation to find out whether they'd broken something; it was 48 to 72 hours because they had to run on a bunch of servers. They had long-running automated tests, but they were long-running system tests. Those are the wrong types of automation. So not all types of automation are the right type of automation. They were certainly better than trying to do it manually, right? 

    But those types of tests tend to be flaky, tend to have a lot of false positives, tend to be difficult. So they embarked on an endeavour to invert that triangle of tests, right? So if you think about it, you've got fast-running unit tests that your developers are creating; that's testing just the smallest unit of work, all the way down to long-running system tests at the bottom. 

    They had 38,000 long-running system tests and a very small number of unit tests at the top, so their pyramid was the wrong way up, right? So what they did was they spent a whole bunch of time, money, and effort flipping that pyramid round and working through moving all of those tests to fast, slick automation. They got from 24 to 48 hours to find out whether something was wrong all the way down to three and a half minutes to run their entire test infrastructure locally on the developer workstation. 

    So you close those feedback loops; you shorten those cycles. Manual is the longest cycle. Anything manual, whether it's deployments, whether it's testing, anything manual is prone to mistakes. So you get false positives and negatives, but it also takes time. It's time-consuming, and it's costly. 

    I worked with an organisation years ago; that's the only organisation I've ever encountered like this, so it is unusual; it's an outlier. But they had twice as many testers as coders. Their quality was so bad in their software that they had to do, I think it was, a thousand and a half hours of manual QA to validate whether any change the developers had made would actually work in the system. 

    So they had 600 testers and 300 coders in their teams, and it was all manual tests. That is just unsustainable. Think of the cost of having all of those people versus having fast, tight, secure, easy to understand, easy to validate automation that could run and check your software on a regular cadence. You will build more features; you will build those features with a higher level of quality, so you'll have happier customers. 

    And if you're building more features, you're having happier customers, you're doing more experiments, right? Because you're building more features, you're more likely to open out new markets, and you're more likely to make a profit.
- id: sidTi_uSsdc
  title: Discipline versus motivation?
  description: "Discover the key difference between discipline and motivation in high-performance teams with Martin Hinshelwood. Unlock your team's potential! #shorts #agile"
  captions: ""
- id: sIhG2i7frpE
  title: Improving workflow with Kanban
  description: 'Discover how to enhance your workflow using Kanban in this insightful video. Streamline your processes and boost productivity! #agile #kanban'
  captions: |-
    One of the key elements of Caman is improving, making improvements or changes to the system in order to see whether you've managed to improve things or not. So we use metrics and visual tools to enable us as humans to see patterns and understand the data to increase transparency, so that we can make changes to the system. 

    This allows us to ask more interesting questions of ourselves and others that participate in our system. Improving the workflow is absolutely critical. If you're not making changes to your workflow on a continuous basis, you're probably not doing Caman. 

    If you are making changes, are you looking at the right data? Do you have the right visualisations to really be seeing what's going on?
- id: sKYVNHcf1jg
  title: What was your worst day as an agile consultant?
  description: 'Join Martin Hinshelwood as he shares his toughest day as an agile consultant, revealing the challenges behind empowering teams in #agile and #scrum.'
  captions: ""
- id: SLZmpwEWxD4
  title: The Importance of Visualizing Your Work in a Kanban Strategy
  description: Unlock the power of Kanban! Discover best practices for visualising workflows, optimising processes, and boosting team productivity in this insightful video.
  captions: |-
    A hugely important technique for any implementation of a Kanban strategy is to visualize your work. Right, you need to be able to see what's going on. You need to be able to visualize that. I'm a big user of Azure DevOps. I know Jira has these features as well. But being able to visually see your cards moving across the board, the units of value, we don't really care about tasks at this point, right? But your smallest unit of value, to see them flow across the board, see where things are. So if somebody asks you, "Oh, where's this? What's that? What's going on with this?" it's very obvious where and what it is.

    That visualization, while super important, is actually super difficult to create because we generally make assumptions as a team about what each other think the system is. Okay, so I'll think the system works like this, and another team member will think it works like that. But we'll think we're on the same page. We'll think we understand each other on how the system works. But because we've never sat down and had that conversation about how this system works, then we don't.

    I have a, I don't know if you've noticed, but I kind of like board games. If you were to take any one of these board games off my shelf and throw away the rule book and get four people together to decide they're each going to decide how they're going to use the pieces and move them around in order to get points, and then you're going to work together to solve the problem that is the board game. You pick a cooperative one, but if we don't agree what the rules are, then how could we possibly have a successful game, right? We're not all playing the same game because we all have not agreed what the rules are. That's why games come with a rule book.

    The first part of implementing a Kanban strategy is you need to write your rule book. What are the rules of your system? How are you going to decide how things work? So even one of the first decisions is what are we going to visualize on this? What are we actually going to put on the board? Is it only things on our backlog, or is it bugs as well? What is it that's going to go on there, and how does it move through this system? So creating that visualization is super important.

    So what I recommend that you do is do a workshop. Get everybody together, sit down, and just build a board. Whether you're building it in Azure DevOps or you're building it in Jira or Trello, or just pop open Mural or Miro and use a digital whiteboard, right? Totally freeform. Make the decisions, write the notes, and document your system. How does your system work? And agree as a team. When you find differences on how your system is supposed to work, then at the very least, even if we've still got a system that might not be optimal, right? Because this is where we are right now, this is what we're doing right now, we at the very least have a system that we all agree is the same system, right? Because if we want to play together, we want to play as a team, and we want to be successful together, we need to define what those things are. We need to define how the system works, and we all need to be playing in the same system.

    If you're struggling to decide how your system works and create a definition of workflow, then we can help you. We provide world-class Kanban training from Pro Kanban, as well as consulting and coaching for teams trying to implement a Kanban strategy. If you're a Scrum team, then we always recommend bringing in flow metrics as a complimentary practice and have Kanban classes from Scrum the Door.

