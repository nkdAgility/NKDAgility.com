- FrontMatter:
    title: Convert Legacy Projects and ASP.NET MVC Apps to SDK-Style with Confidence
    date: 2025-05-29T09:00:00Z
    description: Learn how to upgrade legacy .NET and ASP.NET MVC projects to SDK-style for easier builds, modern tooling, and future readiness, including tips for class libraries and web apps.
    Watermarks:
      description: 2025-05-07T15:00:14Z
    ResourceId: 2mdv7QE2nIt
    ResourceType: engineering-notes
    ResourceContentOrigin: human
    ResourceImport: false
    slug: convert-legacy-projects-and-asp-net-mvc-apps-to-sdk-style-with-confidence
    aliases:
    - /resources/2mdv7QE2nIt
    categories:
    - Engineering Excellence
    tags:
    - Install and Configuration
    - Software Development
    - Pragmatic Thinking
    - Technical Mastery
    - System Configuration
    - Engineering Practices
    - Operational Practices
    - Troubleshooting
    - Application Lifecycle Management
    weight: 345
    concepts:
    - Tool
  BodyContent: |
    I’ve been working with a customer who, like many, is stuck in the past. They were on Team Foundation Version Control (TFVC), and the backbone of their application is .NET 4.5. This creates real problems for modern engineering practices because many new tools just won’t work, so I am moving to Git, and as part of that looking to ensure that their setup is future ready. I also need to do something with all those peskie legacy DLLs that are scattered around the solution. One of the key upgrades I was looking at is moving to SDK-style projects. This is a big deal because it’s the future of .NET development, and it’s a lot easier to work with than the old project format.

    The good news: you can move to SDK-style projects even if you’re targeting older .NET versions.

    For the Azure DevOps Migration Tools, a contributor did the upgrade, and the capabilities are outstanding:

    - Simpler project file format – no more messy MSBuild clutter
    - Supports `Directory.Build.props` to consolidate configuration across the solution
    - Supports `Directory.Packages.props` to consolidate NuGet versions and avoid DLL hell
    - Builds with `dotnet build` (your mileage may vary)

    Additional reasons to move to SDK-style projects:

    - Easier multi-targeting support
    - Better integration with modern CI/CD pipelines
    - Cleaner diffs in source control
    - Consistent experience across .NET Core, .NET 5+, and .NET Framework
    - Improved support for analyzers and code quality tools
    - Faster restore and build times in many cases

    The Azure DevOps Migration Tools combine class libraries and executables, with a mix of .NET 4.8.1, netstandard2.0, and net8.0 – they all have to interoperate smoothly.

    SDK-style `.csproj` files simplify your build system, cut the clutter, and prepare you for future upgrades. Here’s the pragmatic reality: Microsoft does not officially support converting classic ASP.NET MVC/WebForms projects to SDK-style. Class libraries? Straightforward. Web apps? That takes discipline, skill, and a willingness to dive into the details. With the right approach, you can absolutely get this working and keep IIS Express debugging alive.

    Let’s break it down with radical clarity.

    ## Class Library Conversion – No-Brainer

    Class libraries are the easy win. They have minimal impact on code but make it massively easier to organise and maintain.

    - Use .NET Upgrade Assistant or `try-convert`
      Automate the rewrite:

      ```shell
      try-convert -p YourProject.csproj --keep-current-tfms --no-backup
      ```

      This keeps you on `net481` but upgrades the project format.

    - Manual option if you want full control:

      ```xml
      <Project Sdk="Microsoft.NET.Sdk">
        <PropertyGroup>
          <TargetFramework>net481</TargetFramework>
        </PropertyGroup>
      </Project>
      ```

    - Switch from `packages.config` to `<PackageReference>`
      Drop the legacy baggage.

    - Test thoroughly. Rebuild, run tests, and make sure everything resolves. This is not the time to skip CI.

    In my experience, the "right-click on project" → Upgrade option in Visual Studio also works well.

    ## ASP.NET MVC/WebForms Conversion – Tread Carefully

    This is where it gets tricky. There’s no official support for MVC/WebForms in SDK projects, and it’s like wrestling a bear to get it all working.

    There are two practical approaches:

    ### 1. Use the Community MSBuild SDK (Recommended)

    - Convert with:

      ```shell
      try-convert -p YourWebApp.csproj --keep-current-tfms --no-backup --force-web-conversion
      ```

    - Change the SDK line:

      ```xml
      <Project Sdk="MSBuild.SDK.SystemWeb/4.0.97">
      ```

    Why this works:

    - Adds the missing web build magic (`System.Web`, Razor, content files)
    - Supports F5 debugging, publishing, transforms
    - Keeps you close to classic Visual Studio behaviour

    Keep an eye on [MSBuild.SDK.SystemWeb on GitHub](https://github.com/CZEMacLeod/MSBuild.SDK.SystemWeb) for updates.

    ### 2. Manual Tweaks Without External SDK

    - Set the output correctly:

      ```xml
      <OutputType>Library</OutputType>
      <OutputPath>bin\</OutputPath>
      <AppendTargetFrameworkToOutputPath>false</AppendTargetFrameworkToOutputPath>
      ```

    - Define the run command:

      ```xml
      <RunCommand>$(MSBuildExtensionsPath64)\..\IIS Express\iisexpress.exe</RunCommand>
      <RunArguments>/path:"$(MSBuildProjectDirectory)" /port:YOUR_PORT</RunArguments>
      ```

    - Check or add launchSettings.json, but note that Visual Studio often ignores it without the `RunCommand` fix.

    Recommendation: Don’t waste time fighting Visual Studio. Use SystemWeb SDK if you want a sustainable setup.

    ## Debugging Survival Guide

    Debugging can be a pain, and in complex apps, it’s still hit-or-miss. But here’s what typically works:

    If you get the "RunCommand not set" error, add `<RunCommand>` and `<RunArguments>` in your .csproj. This tells Visual Studio how to launch IIS Express.

    If you see the “Debugging Release build” warning, go to Project > Build and uncheck “Optimize code” in Debug. Also, confirm `<Optimize>false</Optimize>` is set in the .csproj and do a full clean and rebuild.

    If you’re attaching to a running process, enable “Suppress JIT optimization on module load” in the Visual Studio debugging options. This will let you step through code without the headache of optimized binaries.

    Finally, check that your PDB files are loaded. Open the Modules window during debugging and make sure the symbols are picked up from your output directory. Without them, breakpoints won’t bind, and you’re debugging blind.

    ## Known Limitations

    Some things just won’t come back. The "Web" tab in the project properties is gone, but you can configure everything you need using the .csproj and launchSettings.

    Out-of-the-box publish won’t work because it expects a .NET 5+ project. You’ll need to script your own publish steps to get the bits into the right place.

    If needed, you can leave web apps in the old format and just convert the class libraries. That’s a pragmatic call if you’re not planning to touch System.Web long-term.

    ## Final Recommendations

    Here’s where it all comes together. These are the pragmatic, no-nonsense calls you should make after working through the conversion process.

    - Convert class libraries: Yes, immediately.
    - Convert web apps:

      - Use MSBuild.SDK.SystemWeb if you want SDK-style.
      - Skip if you’re staying on .NET Framework and want zero friction.

    The point of this work isn’t to show off modern `.csproj` files. It’s to make your engineering system simpler, more maintainable, and ready for what’s next.
  FilePath: C:\Users\MartinHinshelwoodNKD\source\repos\NKDAgility.com\site\content\resources\engineering-notes\2025-05-28-convert-legacy-projects-aspnet-mvc-to-sdk-style\index.md
  FolderPath: C:\Users\MartinHinshelwoodNKD\source\repos\NKDAgility.com\site\content\resources\engineering-notes\2025-05-28-convert-legacy-projects-aspnet-mvc-to-sdk-style
- FrontMatter:
    title: How I Used Generative AI to Transform Site Tagging and Categories
    date: 2025-05-15T09:00:00Z
    ResourceType: engineering-notes
    description: Explains how generative AI and PowerShell scripts automate and improve blog post tagging and categorisation in Hugo, with human oversight and transparent audit trails.
    Watermarks:
      description: 2025-05-07T13:48:01Z
    ResourceId: oRStCAqLAY4
    ResourceContentOrigin: human
    ResourceImport: false
    aliases:
    - /resources/oRStCAqLAY4
    categories:
    - Engineering Excellence
    tags:
    - Artificial Intelligence
    - Personal
    - Software Development
    - Technical Mastery
    - Transparency
    - Pragmatic Thinking
    weight: 400
    concepts:
    - Tool
  BodyContent: |
    I moved my blog to Hugo in mid-2024 to regain control over my content and workflow. This proved to be a great decision as it gave me the ability to really focus on content for my site.

    My site has been around since 2006 and was originally on GeeksWithBlogs, then Blogger, and then WordPress. As it moved, it grew in complexity, but updating legacy things like "old posts" and "categories" became painful in WordPress when you have 800+ posts. If you have tried to use the WordPress API, you will understand the pain.

    So when I moved, I wanted my content to be portable, static, and in a data format that I could easily manipulate. I chose Hugo as the site generator, with Markdown and YAML as the data formats (with some JSON as you will see below). This made it very easy to write scripts over the top of the Markdown and update it dynamically. I built a bunch of such scripts, and it became apparent very early that AI could play a fantastic part in the story. My first implementation of AI was to use a prompt to ensure that my posts all had an SEO-friendly description. This worked great and produced some fantastic descriptions, but while it’s tempting to treat AI as a black box, that’s reckless. AI systems have **agency**, but it is limited to **tactical optimisation** — they can suggest, rank, and cluster, but they cannot **own accountability** for what gets published or accepted. That’s the human’s job.

    This post outlines the architecture and technical strategy I implemented to integrate OpenAI into my Hugo site for automated classification, while maintaining human oversight and ensuring auditability.

    ### Why I Built This

    I realised pretty quickly that many of my tags and categories were vague, uninteresting, and often just wrong. I had over 1,000 tags and 200 categories, which made discoverability a nightmare. So I decided to see if I could leverage generative AI to do the classification for me once I had pruned my categorisations.

    I finally ended up with about 140 tags and 12 categories, but more recently also introduced the idea of "Concepts" to classify the classifications — what fun.

    Overall I wanted to:

    - **Improve discoverability** across hundreds of blog posts.
    - **Align tags and categories** with consistent editorial standards.
    - **Reduce manual labour** but keep ultimate control in human hands.
    - **Make classification decisions traceable and explainable.**

    Automation without traceability is irresponsible. This system deliberately blends machine suggestions with deterministic validation and penalty logic to ensure reliable, explainable outcomes.

    While I started with just a dump of my 19 years of categories and tags, over that time I had moved from a web developer through DevOps and Scrum to a process consultant. As you can imagine, the context of the taxonomies and their intent changed a lot over time. I needed to really think about what I wanted for each, and the new classification system relies on three interconnected layers:

    - **Concepts** → High-level thematic anchors that describe foundational ideas (e.g., Philosophy, Practices, Methods, Values, Strategy). These are used to "classify the classifications."
    - **Categories** → Editorially curated groupings that organise posts into broad topic clusters aligned with user needs (e.g., Technical Leadership, Product Development, Scrum, Kanban). Some tags are promoted to categories for marketing purposes.
    - **Tags** → Detailed, fine-grained descriptors that capture specific topics, techniques, tools, or patterns (e.g., Scrum Mastery, CI/CD, Azure Pipelines).

    By combining these, I created a multi-level structure that improves searchability, recommendation accuracy, and editorial consistency. Tags have categories, and both tags and categories have concepts. I think of tags and categories as a single list, but categories are few, important, and go at the top of the content, and tags are many, tactical, and go at the bottom.

    The AI assigns tags, categories, and concepts to all the content items using "instructions" embedded in the classification pages.

    I'm a Windows user and have been for years, so I wrote all of the scripting in PowerShell. For me, this is the most flexible as it’s native, and anything I can’t do in PowerShell I can use C#. I do have some calls out to Python, but that’s another post. Iterating over a bunch of Markdown files with YAML front matter is a trivial experience, but I have built up a bunch of helper modules over the last 6 months that do a lot of the heavy lifting — so much so that it can take minutes to build new scripts for specific one-time tasks. For example, if I want to get a list of all my content resources pre-parsed into an ordered front matter hashtable and the content, then all I need to do is call `$hugoMarkdownObjects = Get-RecentHugoMarkdownResources -Path ".\site\content\resources\" -YearsBack 1` and I have a ready-to-iterate collection.

    I also have a batch version, and all of my code is in GitHub where it can be versioned, branched, and reviewed with a PR.

    ## How it Works

    I created a JSON cache format to reliably store the results from OpenAI. This design gives me a structured, reusable data layer that feeds the rest of the system with clean, consistent inputs. Initially, I tried creating a prompt with a list of all 160 tags and categories and asking the AI to select them, but that created a bunch of junk.

    Ultimately, I settled on a single call for each classification that tested that classification against the content, based on clear instructions from the classification front matter. The result is that for each piece of content and every classification, I have an entry in the cache file that looks like:

    ### Example Output

    ```json
    {
      "Technical Leadership": {
        "resourceId": "TwYNSm1pZOS",
        "category": "Technical Leadership",
        "calculated_at": "2025-05-05T13:24:31",
        "ai_confidence": 82.5,
        "ai_mentions": 7.5,
        "ai_alignment": 8.5,
        "ai_depth": 8.0,
        "ai_intent": null,
        "ai_audience": 7.0,
        "ai_signal": 8.0,
        "ai_penalties_applied": false,
        "ai_penalty_points": 0,
        "ai_penalty_details": "none",
        "final_score": 82.0,
        "reasoning": "The content discusses the Definition of Done (DoD) within the context of Scrum, which is a key aspect of agile methodologies. It directly addresses the importance of quality and transparency in software development, aligning well with the principles of technical leadership. The explicit mention of the DoD and its role in ensuring quality reflects a strong understanding of servant leadership and accountability within teams. The content thoroughly explores the implications of having a DoD, including its impact on team dynamics, accountability, and continuous improvement, which are all relevant to technical leadership. The intent is clearly to inform and guide teams on best practices, making it suitable for the target audience of technical leaders and practitioners. The signal-to-noise ratio is high, with minimal off-topic content, focusing instead on actionable insights and strategies for implementing a robust DoD. Overall, the content fits well within the category of Technical Leadership, meriting a high confidence score.",
        "level": "Primary"
      }
    }
    ```

    This creates a bunch of scores from 0–10 in Mentions, Alignment, Depth, Intent, Audience, and Signal. It assesses penalties and creates a final score, out of 100, that represents how confident the AI is that it matches.

    ### AI + Human Oversight

    AI here has **no editorial authority**. It supplies **probabilistic suggestions**. I maintain control using a mixture of content manipulation and reviewing the output, while the human-authored penalty and validation layers provide **deterministic enforcement**. This distinction is non-negotiable. AI is a tactical agent, not a strategic decision-maker.

    This reflects the ethos outlined in [Human and AI Agency in Adaptive Systems](https://preview.nkdagility.com/resources/ffJaR9AaTl7): **humans set direction and own accountability; AI optimises within defined boundaries**.

    ## Technical Strengths

    I've been very impressed with the capability, and I’ve also learned valuable lessons about where AI shines and where human judgment is indispensable. Every classification has a clear reason for being attached to the content that’s reviewable and transparent. I even use the reasoning on the site.

    - Quantitative, multi-factor assessment.
    - Penalty and validation rules strengthen match quality.
    - Human-readable reasoning baked into outputs.
    - Cached results reduce API calls and speed up builds.
    - Seamless integration into Hugo’s version-controlled structure.

    ### Future Enhancements

    - Track classification shifts over time.
    - Introduce ensemble AI models for cross-validation.
    - Add a human review loop to feed corrections back into system tuning.
    - Build a site-wide dashboard showing confidence trends, penalty patterns, and overall classification health.

    ## Closing Thoughts

    Bringing AI into this system has been transformative, and if you’re considering similar work, my advice is simple: don’t delegate accountability — use AI to amplify your judgment. By combining automation with careful oversight, I’ve turned a massive manual maintenance burden into a scalable, transparent process. This work has not only improved the quality and consistency of my site, but it has also deepened my own understanding of how to use AI responsibly: as a partner in execution, not as a substitute for accountability. I’m excited about where this will go next and how it can push the boundaries of what’s possible in content management.
  FilePath: C:\Users\MartinHinshelwoodNKD\source\repos\NKDAgility.com\site\content\resources\engineering-notes\2025-05-15-generative-ai-for-classifications\index.md
  FolderPath: C:\Users\MartinHinshelwoodNKD\source\repos\NKDAgility.com\site\content\resources\engineering-notes\2025-05-15-generative-ai-for-classifications
- FrontMatter:
    title: 'Building a Resilient Token Server: Engineering for Flow, Fault Tolerance, and Speed'
    description: Explains how to engineer a robust, fault-tolerant token counting server using FastAPI and PowerShell, covering error handling, retries, fallbacks, and resilient workflows.
    ResourceId: mjsboLP-N9P
    ResourceImport: false
    ResourceType: engineering-notes
    ResourceContentOrigin: human
    date: 2025-05-08T09:00:00Z
    weight: 260
    aliases:
    - /resources/mjsboLP-N9P
    categories:
    - Engineering Excellence
    - DevOps
    tags:
    - Pragmatic Thinking
    - Software Development
    - Technical Mastery
    - Operational Practices
    - Troubleshooting
    - Personal
    - Continuous Improvement
    - Engineering Practices
    - Working Software
    - Technical Excellence
    - Site Reliability Engineering
    - System Configuration
    Watermarks:
      description: 2025-05-07T12:49:10Z
    concepts:
    - Tool
  BodyContent: |
    Modern engineering is about making sure systems keep running reliably under load, failure, and unpredictable conditions. When I set out to build a fast, dependable way to calculate OpenAI token counts for my batch classification pipeline, I didn’t want a quick script or a one-off tool. I wanted a **resilient, observable, fault-tolerant system** that fit tightly into my PowerShell-first workflow and could hold up in real conditions, not just lab tests.

    **Background:** Midway through last year, I finally accepted that WordPress was no longer the future for me. To be honest, I’d known it for a while, but the pain of migrating was bigger than the pain of sticking with it—until it wasn’t. I made the deliberate choice to rebuild a decade of Wordpress content into Hugo, Markdown, YAML, and a layer of PowerShell automation for bulk editing. As I ramped up automation for pre-processing and OpenAI-driven bulk edits, one bottleneck hit hard: counting tokens. I had an existing method calling out to Python from PowerShell, but it clocked in at around five minutes per thousand prompts. That was unacceptable for the scale I wanted.

    This post lays out exactly how I tackled that challenge—what worked, what fell apart, and how I hardened the system into something that performs reliably under real-world pressure, or at least "my world" pressure.

    ## Starting Line: What I Set Out to Build

    - A Python FastAPI server (`token_server.py`) to expose token counting as a REST API.
    - A PowerShell wrapper (`TokenServer.ps1`) to orchestrate batch calls and control the server lifecycle.
    - Seamless integration into our `Update-ResourcesClassificationsBatch.ps1` script, which feeds Hugo markdown files for token counting.

    Sounds simple? Not so fast.

    ## Where It Fell Apart

    Here’s where the early implementation hit the wall:

    - Repeated server restarts led to **Windows port binding errors** (`Only one usage of each socket address...`).
    - Sockets sat in `TIME_WAIT`, leaving the system **unable to reuse ports quickly**.
    - PowerShell orchestration assumed the server was either always running or needed a restart; both assumptions failed under load. We were processing \~30k prompts, and it failed at about 6k.
    - Failure detection (`Test-TokenServer`) was **too aggressive**, flagging false negatives and triggering unnecessary restarts.

    In short, the system had no resilience. It worked when everything was perfect and collapsed under even minor hiccups. That’s not engineering; that’s gambling.

    It did get to the end of its run, but the logs told the real story, which is why I included the relevant segment here. Logs are not fluff or noise; they are the raw, transparent evidence that shows what really happened, reinforcing my ethos of engineering honesty and accountability, even when things get messy. The real story of errors and flakiness:

    ```bash
    [11:38:27 INF] Starting Token Server on 127.0.0.1:8000 using script .\.powershell\py\token_server.py
    [11:38:37 INF] Token Server is now running at 127.0.0.1:8000
    [11:38:41 INF] 1521 prompts Built for 9 of 169 markdown files (5%)
    [11:38:48 INF] 2843 prompts Built for 17 of 169 markdown files (10%)
    [11:38:54 INF] 4229 prompts Built for 26 of 169 markdown files (15%)
    [11:39:01 INF] 5476 prompts Built for 34 of 169 markdown files (20%)
    [11:39:09 INF] 6862 prompts Built for 43 of 169 markdown files (25%)
    [11:39:18 INF] Token Server already responding at 127.0.0.1:8000
    [11:39:18 ERR] Failed to get token count from server: Only one usage of each socket address (protocol/network address/port) is normally permitted. (127.0.0.1:8000)
    [11:39:24 INF] Stopping Token Server with tracked PID 13536
    [11:39:24 INF] Starting Token Server on 127.0.0.1:8000 using script .\.powershell\py\token_server.py
    [11:39:25 INF] Token Server is now running at 127.0.0.1:8000
    [11:39:25 ERR] Failed to get token count from server: Only one usage of each socket address (protocol/network address/port) is normally permitted. (127.0.0.1:8000)
    [11:39:29 INF] Token Server already responding at 127.0.0.1:8000
    [11:39:29 ERR] Failed to get token count from server: Only one usage of each socket address (protocol/network address/port) is normally permitted. (127.0.0.1:8000)
    [11:39:32 INF] Token Server already responding at 127.0.0.1:8000
    [11:39:32 ERR] Failed to get token count from server: Only one usage of each socket address (protocol/network address/port) is normally permitted. (127.0.0.1:8000)
    [11:39:34 ERR] Failed to get token count from server: Only one usage of each socket address (protocol/network address/port) is normally permitted. (127.0.0.1:8000)
    [11:39:40 INF] Stopping Token Server with tracked PID 121896
    [11:39:40 INF] Starting Token Server on 127.0.0.1:8000 using script .\.powershell\py\token_server.py
    [11:39:48 INF] Token Server is now running at 127.0.0.1:8000
    [11:39:48 ERR] Failed to get token count from server: Only one usage of each socket address (protocol/network address/port) is normally permitted. (127.0.0.1:8000)
    [11:39:53 INF] Token Server already responding at 127.0.0.1:8000
    [11:39:54 ERR] Failed to get token count from server: Only one usage of each socket address (protocol/network address/port) is normally permitted. (127.0.0.1:8000)
    [11:39:56 INF] 8094 prompts Built for 51 of 169 markdown files (30%)
    [11:39:59 ERR] Failed to get token count from server: Only one usage of each socket address (protocol/network address/port) is normally permitted. (127.0.0.1:8000)
    [11:40:01 ERR] Failed to get token count from server: Only one usage of each socket address (protocol/network address/port) is normally permitted. (127.0.0.1:8000)
    [11:40:05 INF] Token Server already responding at 127.0.0.1:8000
    [11:40:06 ERR] Failed to get token count from server: Only one usage of each socket address (protocol/network address/port) is normally permitted. (127.0.0.1:8000)
    [11:40:12 INF] Stopping Token Server with tracked PID 234028
    [11:40:12 INF] Starting Token Server on 127.0.0.1:8000 using script .\.powershell\py\token_server.py
    [11:40:13 INF] Token Server is now running at 127.0.0.1:8000
    [11:40:13 ERR] Failed to get token count from server: Only one usage of each socket address (protocol/network address/port) is normally permitted. (127.0.0.1:8000)
    [11:40:13 ERR] Failed to get token count from server: Only one usage of each socket address (protocol/network address/port) is normally permitted. (127.0.0.1:8000)
    [11:40:20 INF] Stopping Token Server with tracked PID 242448
    [11:40:20 INF] Starting Token Server on 127.0.0.1:8000 using script .\.powershell\py\token_server.py
    [11:40:21 INF] Token Server is now running at 127.0.0.1:8000
    [11:40:27 INF] Stopping Token Server with tracked PID 232540
    [11:40:27 INF] Starting Token Server on 127.0.0.1:8000 using script .\.powershell\py\token_server.py
    [11:40:37 INF] Token Server is now running at 127.0.0.1:8000
    [11:40:42 INF] 9480 prompts Built for 60 of 169 markdown files (35%)
    [11:40:50 INF] 10712 prompts Built for 68 of 169 markdown files (40%)
    ```

    This worked, but all the restarts made it painfully slow. However, to be clear, it was still faster than using local token calls.

    ## Refactoring for Resilience

    Here’s exactly how I hardened the system once it was working, keeping everything pragmatic, outcome-focused, and fully aligned to the engineering ethos I follow — minimising errors, maximising resilience, and ensuring flow without cutting corners or adding unnecessary complexity.

    ### Start/Stop Once Per Batch

    Originally, I tried a model of "start the server if it’s not running, and kill it if it faults," but that is what resulted in the errors above. Instead of cycling the server, I flipped to a **batch-wide lifecycle**:

    ```powershell
    Start-TokenServer
    # Process all files here...
    Stop-TokenServer
    ```

    This reduced port churn, avoided TIME_WAIT issues, and drastically simplified orchestration.

    ### Add Retries Instead of Restarts

    From the log above, you can see I was trying to solve the fake-response problem by restarting the server — but that was the wrong approach. Restarting might hide symptoms, but it never fixes the underlying fragility. It always reminds me of that ridiculous video where someone rigged up a second server with a CD tray and taped a pencil to it so that when the first server stopped responding, the second one would eject its tray and physically press the reset button on the first. Sure, it technically works, but it’s the software equivalent of sweeping dirt under a rug. Engineering excellence means you address the real issues directly, not patch over them with hacks.

    Inside `Get-TokenCountFromServer`, we wrapped the REST call in a retry loop:

    ```powershell
    for ($i = 0; $i -lt 3; $i++) {
        try {
            $response = Invoke-RestMethod -Uri $ServerUrl -Method Post -Body $body -ContentType "application/json" -TimeoutSec 10
            return [int]$response.token_count
        }
        catch {
            Start-Sleep -Seconds 1
        }
    }
    ```

    This change turned transient network failures from system-breaking to non-events with barely noticeable delays. At scale, I would want proper back-off logic and probably apply a circuit breaker pattern, but here I kept it simple because it delivers what matters: stable, predictable flow without unnecessary overengineering.

    ### Implement Local Fallback

    I already have logic to calculate the tokens locally and in isolation, which was the thing I wanted to refactor away. However, I realised that if the server was overloaded, I could either retry indefinitely, fail out, or regress to a local Python call. Sure, it was the slower path, but it acted as a deliberate, engineered fallback when the server stopped responding under load. This wasn’t just retries stacked on retries — it was a purposeful local option to guarantee the system would not fail completely and would keep moving, even if with a slight delay:

    ```powershell
    function Get-TokenCountLocally {
        param ([string]$Content)

        $tempFile = [System.IO.Path]::GetTempFileName()
        Set-Content -Path $tempFile -Value $Content -Encoding UTF8

        $tokenCount = python -c @"
    import tiktoken, sys
    with open(sys.argv[1], 'r', encoding='utf-8') as f:
        text = f.read()
    encoding = tiktoken.get_encoding('cl100k_base')
    print(len(encoding.encode(text)))
    "@ $tempFile

        Remove-Item $tempFile -Force
        return [int]$tokenCount
    }
    ```

    ## Final System Outcomes

    By the end, we delivered a system that:

    - Survives server restarts and network hiccups.
    - Automatically falls back to local processing if the server is down, giving it time to recover
    - Handles high-volume batch loads without choking.
    - Provides clear, observable logs for troubleshooting and improvement.

    This is not just about counting tokens. It is about building resilient, fault-tolerant architectures that hold up under real-world use — and taking full ownership of the engineering outcome, even when the tool is 'just a script' for my own workflows. This reflects my principles as an engineer: if I touch it, I am accountable for its resilience, its clarity, and its long-term behaviour. And this is the work I put into a simple script that only I need to run. If this were going to be a production system, I'd have to take it to a whole other level, and I would expect engineering teams I work with to do the same.

    I'm certainly not done, and the scripts I use get continuous refinement and are adapted as I learn more and need more. Here are some ideas for improvements:

    Here’s what I’m noodling on next:

    - **Separate concerns in PowerShell** — I already have reusable modules, but I can sharpen this by splitting orchestration logic and reusable functions into clean, distinct scripts or modules.
    - **Use structured exceptions** — Plain error logs only get me so far; I want to move to proper PowerShell error records or thrown exceptions so failures are surfaced intentionally, not passively.
    - **Add FastAPI health and version endpoints** — Lightweight /health and /version routes will let me run fast checks and diagnostics without poking deep into the server.
    - **Introduce concurrency handling** — Adding PowerShell locking or a mutex will protect against race conditions if multiple scripts try to touch the server at once.
    - **Dockerize the Python token server** — Packaging the server into a Docker container will give me environment isolation and smoother deployment, especially when moving between local and cloud setups.
    - **Implement structured logging** — I want to replace Write-Host or Write-InfoLog with structured log outputs (like JSON or key-value pairs) so logs are machine-readable and easier to pipeline.
    - **Write automated tests** — Pester for PowerShell unit tests and pytest with httpx for FastAPI endpoint testing will give me confidence this system holds up, even as I evolve and extend it.

    But for now, I'm happy that it's executing and reasonably resilient. Next time I run it I might not feel the same and make more changes.

    # Final Takeaway

    Building resilient systems is not about making them work once. It is about making sure they keep working when the environment turns hostile. This mindset aligns directly with the DevOps ethos: accountability, continuous improvement, and designing systems that deliver reliable value no matter the noise or disruption. This Token Server journey was a microcosm of that challenge, and the lessons apply far beyond token counting.

    - Design for failure.
    - Build in observability.
    - Create fallback paths.
    - Prioritise flow, not brute-force restarts.
  FilePath: C:\Users\MartinHinshelwoodNKD\source\repos\NKDAgility.com\site\content\resources\engineering-notes\2025-05-08-resilient-token-server\index.md
  FolderPath: C:\Users\MartinHinshelwoodNKD\source\repos\NKDAgility.com\site\content\resources\engineering-notes\2025-05-08-resilient-token-server

