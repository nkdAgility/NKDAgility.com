- FrontMatter:
    title: Join a machine to your azure hosted domain controller
    description: Learn how to join a machine to your Azure-hosted domain controller with our step-by-step guide. Simplify your setup and enhance your network management!
    ResourceId: 7RVNi9gLHYY
    ResourceType: blog
    ResourceContentOrigin: Human
    ResourceImport: true
    ResourceImportId: 10892
    ResourceImportSource: Wordpress
    ResourceImportOriginalSource: Wordpress
    date: 2014-12-31
    weight: 1000
    creator: Martin Hinshelwood
    layout: blog
    resourceTypes: blog
    slug: join-machine-azure-hosted-domain-controller
    aliases:
    - /resources/7RVNi9gLHYY
    aliasesArchive:
    - /blog/join-machine-azure-hosted-domain-controller
    - /join-machine-azure-hosted-domain-controller
    - /join-a-machine-to-your-azure-hosted-domain-controller
    - /blog/join-a-machine-to-your-azure-hosted-domain-controller
    - /resources/blog/join-machine-azure-hosted-domain-controller
    tags:
    - System Configuration
    - Install and Configuration
    - Windows
    categories:
    - Uncategorized
    preview: nakedalm-windows-logo-6-6.png
  BodyContent: |
    Now that you have finished [moving your Domain Controller Azure VM to a Virtual Network](http://nkdagility.com/move-azure-vm-virtual-network/)\] you need to be able to join a machine to your azure hosted domain controller.

    ![clip_image001](images/clip-image0014-1-1.png "clip_image001")
    { .post-img }

    You need to make sure that you have your machine within the correct virtual network, and [move your Azure VM to a Virtual Network](http://nkdagility.com/move-azure-vm-virtual-network/) if necessary. On top of that you need to have the your [domains DNS server configured for your virtual network](http://nkdagility.com/configure-a-dns-server-for-an-azure-virtual-network/) so that the guest machine knows where to look for the domain.

    ![clip_image002](images/clip-image0024-2-2.png "clip_image002")
    { .post-img }

    If everything is in order you should connect to the VM you want to join to the domain that you have created. On the Dashboard tab of the VM you should see a 'connect' button at the bottom of the screen. Clicking it will launch Remote Desktop and connect it to the server.

    ![clip_image003](images/clip-image0034-3-3.png "clip_image003")
    { .post-img }

    Once on the server the DBS setting should be correctly configured automatically as part of the DHCP for the Virtual Network that we configured before. This should make it fairly simple to join the machine to the domain. This is no different from local domains.

    Note What I really want is to be able to join these machines to AAD so that I do not have to maintain a separate set of local domain controllers for this purpose. For me it gets a little more complex as I have no physical servers, only Azure and Office 365.

    ![clip_image004](images/clip-image0043-4-4.png "clip_image004")
    { .post-img }

    If you right-click on the start button and select "System" you will see the current machine name and domain affiliation. Most likely it will be "Workshop". To make the change we need to click "Change Settings" to open the dialogs.

    ![clip_image005](images/clip-image0053-5-5.png "clip_image005")
    { .post-img }

    Set the radio-button to "Domain" and enter the name of the domain that you want to join. As I setup "env.nakedalmweb.wpengine.com" that is what I need to enter. Once you click "OK" you will be asked for a domain administrator account to join the machine.

    After that a simple reboot will allow you to login to the domain with any of the domain accounts that you have configured.
  FilePath: C:\Users\MartinHinshelwoodNKD\source\repos\NKDAgility.com\site\content\resources\blog\2014\2014-12-31-join-machine-azure-hosted-domain-controller\index.md
  FolderPath: C:\Users\MartinHinshelwoodNKD\source\repos\NKDAgility.com\site\content\resources\blog\2014\2014-12-31-join-machine-azure-hosted-domain-controller
- FrontMatter:
    title: Understanding TFS migrations from on-premise to Visual Studio Online
    description: Explore effective strategies for migrating TFS from on-premise to Visual Studio Online. Discover scenarios, tools, and insights to streamline your transition.
    ResourceId: lov38doo6uB
    ResourceType: blog
    ResourceContentOrigin: Human
    ResourceImport: true
    ResourceImportId: 10987
    ResourceImportSource: Wordpress
    ResourceImportOriginalSource: Wordpress
    date: 2014-12-17
    weight: 1000
    creator: Martin Hinshelwood
    layout: blog
    resourceTypes: blog
    slug: understanding-tfs-migrations-premise-visual-studio-online
    aliases:
    - /resources/lov38doo6uB
    aliasesArchive:
    - /blog/understanding-tfs-migrations-premise-visual-studio-online
    - /understanding-tfs-migrations-premise-visual-studio-online
    - /understanding-tfs-migrations-from-on-premise-to-visual-studio-online
    - /blog/understanding-tfs-migrations-from-on-premise-to-visual-studio-online
    - /resources/blog/understanding-tfs-migrations-premise-visual-studio-online
    tags: []
    categories:
    - Uncategorized
    preview: nakedalm-experts-visual-studio-alm-5-5.png
  BodyContent: |
    I have recently been doing a lot of migrations and [Willy](http://blogs.msdn.com/b/willy-peter_schaub/) asked me to write a white-paper about understanding TFS migrations from on-premise to Visual Studio Online.

    ![clip_image001](images/clip_image0012-1-1.png "clip_image001")
    { .post-img }

    On writing and understanding TFS migrations from on-premise to Visual Studio Online we found that the story was so poor that we broke it into two parts. The first part is ready and focuses on what the options are and the stories for migrating. Its interesting as many people believe that it is Microsoft's job to provide tools to migrate from any other product into their own product. While I would love to agree there are just way to many products out there to make that a realistic situation.

    - PDF: [Understanding TFS migrations from on-premise to Visual Studio Online](https://vsarguidance.codeplex.com/releases/view/178488)

    We kind of looked at a number of scenarios:

    - **Team Project to Team Project** – While not common it is the simplest situation.  
       ![clip_image002](images/clip_image0022-2-2.png "clip_image002")
      { .post-img }
    - **Consolidating Team Projects** – With the move to 2012+ this is the most common ask I have from customers. Wither on-premises or while moving to VSO, many folks are taking the time to pay back the technical cruft that has built up over the years.  
       ![clip_image003](images/clip_image0032-3-3.png "clip_image003")
      { .post-img }
    - **Splitting Team Projects** – While not as common I have seen this as well. Splitting your data is an interesting situation and can be the result of selling parts of your portfolio or just some teams moving on or changing process. Maybe you use it as a staged migration to VSO.  
       ![clip_image004](images/clip_image0042-4-4.png "clip_image004")
      { .post-img }
    - **Consolidating Platforms on VSO** - Many customers have Perforce, Git, TFS, SVN, or any of 50 different systems. I have customer that have one of everything.

    Like I said the story is not currently that good but you can read about each of the scenarios and see what the main issues are. We have also mapped tools to scenarios so that you can try to get started solving whatever your problems are:

    - PDF: [Understanding TFS migrations from on-premise to Visual Studio Online](https://vsarguidance.codeplex.com/releases/view/178488)

    The ALM Rangers will also be releasing a walk-through for the simplest of migrations which is to use Excel for work items and do a tip migration of code. That will be coming real soon.
  FilePath: C:\Users\MartinHinshelwoodNKD\source\repos\NKDAgility.com\site\content\resources\blog\2014\2014-12-17-understanding-tfs-migrations-premise-visual-studio-online\index.md
  FolderPath: C:\Users\MartinHinshelwoodNKD\source\repos\NKDAgility.com\site\content\resources\blog\2014\2014-12-17-understanding-tfs-migrations-premise-visual-studio-online
- FrontMatter:
    title: Create log entries in Release Management
    description: Learn how to create effective log entries in Release Management using PowerShell. Enhance your deployment process and ensure success with detailed logs!
    ResourceId: 1jC1jE7shiY
    ResourceType: blog
    ResourceContentOrigin: Human
    ResourceImport: true
    ResourceImportId: 10975
    ResourceImportSource: Wordpress
    ResourceImportOriginalSource: Wordpress
    date: 2014-12-12
    weight: 690
    creator: Martin Hinshelwood
    layout: blog
    resourceTypes: blog
    slug: create-log-entries-release-management
    aliases:
    - /resources/1jC1jE7shiY
    aliasesArchive:
    - /blog/create-log-entries-release-management
    - /create-log-entries-release-management
    - /create-log-entries-in-release-management
    - /blog/create-log-entries-in-release-management
    - /resources/blog/create-log-entries-release-management
    tags:
    - Troubleshooting
    categories:
    - Uncategorized
    preview: nakedalm-experts-visual-studio-alm-5-5.png
  BodyContent: |
    I have been working through my demos for NDC London next week. And I found it almost impossible to create log entries in Release Management where I wanted.

    While in London for NDC 2014 I was in the same building as the filming of Mission Impossible 5. I worked on a TV show for my work experience at school and ended up with an [IMDB profile](http://www.imdb.com/name/nm4402255/) and what always struck me was how much time was spent getting one a few minutes or even seconds of footage. If you ever get a chance to even be in the audience for a 30 minute comedy show, be warned… you will be there for at least 6 hours to get only 25 minutes of air time.

    Sometimes the same thing happens for demos. My demo for NDC was an end to end presentation of Visual Studio ALM with VSO. For that I needed to have a full release pipeline for my application and as I just downloaded Fabirkam Fibre I had to [create that release pipeline](http://nkdagility.com/create-release-management-pipeline-professional-developers/) from scratch. While I was building this out I ran into a few issues and one that was kind of annoying was an inability to get a log to output so I could review what happened during the deployment.

    If you have a deployment script it is really easy to fail it out. All you need to do is have an error occur, or deliberately call a "Write-Error" command. Simples. But what about having a log of the good things that happened?

    ![clip_image001](images/clip_image0011-1-1.png "clip_image001")
    { .post-img }

    If everything goes swimmingly then you get an empty space where the log should be. So how do I get an output. Well if I was creating a build script I could just have "Write-Host" and the build system would capture and log all the output.

    ```
    #### Update Web.config
    $config = Get-Content $destinationPathweb.config
    $config = $config -replace "__connectionString__", $connectionString
    Set-Content $destinationPathweb.config $config
    Write-Host "Updated web.config"
    ```

    Well lets try "Write-Host"…

    ![clip_image002](images/clip_image0021-2-2.png "clip_image002")
    { .post-img }

    Well, that’s not good. Looks like the Release Management team forgot to pipe the output that is intended for the "host" to the file. While "host" in the normal context is normally the "command prompt", a script should not just fail because you are running it differently. You should always make sure that you pipe the output to the correct location for your context.

    A command that prompts the user failed because the host program or the command type does not support user interaction. Try a host program that supports user interaction, such as the Windows PowerShell Console or Windows PowerShell ISE, and remove prompt-related commands from command types that do not support user interaction, such as Windows PowerShell workflows.

    ```
    +At C:WindowsDtlDownloadsFabrikamFiber.WebDeploySimpleDeploy.ps1:31 char:1
    + Write-Host "destinationPath: $destinationPath"
    + ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    CategoryInfo :NotImplemented: (:) [Write-Host], HostException
    FullyQualifiedErrorId :HostFunctionNotImplemented,Microsoft.PowerShell.Commands.WriteHostCommand
    Moo.. That’s just a nasty error that should never happen. SO lets try a simple "Write-Output" shall we.
    Write-Output "applicationAnalyticsKey: $applicationAnalyticsKey"
    ```

    ![clip_image003](images/clip_image0031-3-3.png "clip_image003")
    { .post-img }

    Dam… "Write-Output" just disappears into the ether. It really should end up in the output but… well… it does not.. And "Write-Verbose" also end up nowhere, but that is a little more expected. At this point I am at a loss and ping the product team. Really, if I write something to the output and I would see it if running from the command line I want to see it in the log file. However for RM you need to explicitly declare output by using the "-verbose" command to tell PowerShell to actually write the verbose statements.

    ```
    Write-Verbose "applicationAnalyticsKey: $applicationAnalyticsKey" -verbose
    ```

    ![clip_image004](images/clip_image0041-4-4.png "clip_image004")
    { .post-img }

    Well… now I get some output and a lovely log to view for later. While I may not ever look, when I do need something it will be there. Success logs are just as important as failure ones…
  FilePath: C:\Users\MartinHinshelwoodNKD\source\repos\NKDAgility.com\site\content\resources\blog\2014\2014-12-12-create-log-entries-release-management\index.md
  FolderPath: C:\Users\MartinHinshelwoodNKD\source\repos\NKDAgility.com\site\content\resources\blog\2014\2014-12-12-create-log-entries-release-management
- FrontMatter:
    title: 'NDC London 2014: Why TFS no longer sucks and VSO is awesome'
    description: Discover why TFS has transformed and how VSO enhances your development process. Join Martin Hinshelwood's insights from NDC London 2014 for agile success!
    ResourceId: Lz8JHBC_e2Z
    ResourceType: blog
    ResourceContentOrigin: Human
    ResourceImport: true
    ResourceImportId: 10980
    ResourceImportSource: Wordpress
    ResourceImportOriginalSource: Wordpress
    date: 2014-12-10
    weight: 690
    creator: Martin Hinshelwood
    layout: blog
    resourceTypes: blog
    slug: ndc-london-2014-why-tfs-no-longer-sucks-and-vso-is-awesome
    aliases:
    - /resources/Lz8JHBC_e2Z
    aliasesArchive:
    - /blog/ndc-london-2014-why-tfs-no-longer-sucks-and-vso-is-awesome
    - /ndc-london-2014-why-tfs-no-longer-sucks-and-vso-is-awesome
    - /ndc-london-2014--why-tfs-no-longer-sucks-and-vso-is-awesome
    - /blog/ndc-london-2014--why-tfs-no-longer-sucks-and-vso-is-awesome
    - /resources/blog/ndc-london-2014-why-tfs-no-longer-sucks-and-vso-is-awesome
    tags:
    - Software Development
    categories:
    - Uncategorized
    preview: nakedalm-experts-visual-studio-alm-3-3.png
  BodyContent: |
    I was in London last week to do a talk on why TFS no longer sucks entitled “[Second Look, Team Foundation Server & VSO](http://nkdagility.com/ndc-london-second-look-team-foundation-server-vso/)”. I had a tone of preparatory work to do too make the demos smooth. The great god Murphy was however not smiling, but he was not angry. Some errors occurred, but no blue screens.

    There are many folks that have used older versions of TFS and dismissed future versions on that basis. However I wanted to do an end to end demonstration (soup to nuts) to show what TFS can bring to the table since it was updated in 2012. TFS prior to 2010 was a cumbersome, enterprise only endeavour and now it really is not. I have done demos before with install and configure of a local TFS server in under 30 minutes, so that part is easy. With the launch of Visual Studio Online (VSO) which is effectively Team Foundation Server (TFS) on steroids in the cloud most of the issues are gone while the stigma remains.

    \[embed\]https://vimeo.com/113604455\[/embed\]

    All of my demos were in VSO so that I could leverage the latest and greatest features, and everything I did could also be done on a local network.

    https://twitter.com/ebrucucen/status/540480195363602432

    I opted to follow the bug workflow story and I managed to get through almost everything I wanted. I skipped the Agile Planning walkthrough and a bug prevented me from getting through the bug workflow. Irony at its best. Also, note that I am playing four parts during the demo. I really should have had three hats or some other indicator of identity, but I think it Here are my actual notes for the demo above:

    **Part 1**

    1. PO sends Feedback Request to Stakeholder\[PO\]
    2. Customer provides feedback and reports bug as part of review \[STAKEHOLDER\]
    3. Product Owner breaks feedback down into new Backlog Items \[PO\]
    4. \[WALKTHROUGH: Agile Planning\]
    5. Product Owner assigns Feedback Response to Tester and request that they verify bug \[PO\]

    **Part 2**

    1. Tester uses exploratory testing and creates bug and test case \[TEST\]
    2. \[WALKTHROUGH: Microsoft Test Manager\]
    3. Product Owner approves the bug and requests that the Development Team expedite \[PO\]

    **Part 3**

    1. Development Team agrees to expedite bug if Sprint commitment reduced \[Coder+Tester\]
    2. Coder finds and fixes the bug \[Coder\]
    3. \[WALKTHROUGH: Code Lens and Git\]
    4. Coder uses the Test Case to verify that they have fixed the bug \[Coder\]
    5. \[WALKTHROUGH: Test Management\]
    6. Automated deployment to “nkd-ff-f1” environment \[TFS\]
    7. Tester verifies fix in “nkd-ff-f1” environment \[TEST\]
    8. \[WALKTHROUGH: Action Recording\]

    **Part 4**

    1. Development Team agrees that the bug is DONE
    2. Automated deployment to “nkd-ff-f2” environment \[TFS\]
    3. Product Owner verifies bug and asks customer to check.

    While almost everything went well I had two SNAFU’s during the demos that I did a little follow up on later.

    ![image](images/image-1-1.png "image")
    { .post-img }

    First was that the Action Recording data collector in Microsoft Test Manager failed to start. It looks like while Windows 10 9860 was in sync the new update that got pushed out broke MTM. In Windows 10 9879 the version of .NET is slightly older than a bugfix that shipped just as Visual Studio 2015 Preview did. Unfortunately as .NET is bound to the OS and especially in a Preview OS I am stuck with MTM not working for now. I have also tested and verified in Visual Studio 2013 that the same issue occurs, but meh… preview bits on preview bits… can’t complain.

    ![image](images/image1-2-2.png "image")
    { .post-img }

    The second error cam in the flavour of a release failure. As it turned out the simple deployment script that I created was a little too simple. IIS was hanging onto a file handle and this resulted in the first command not being able to delete all of the files. Even when logging onto the server I was unable to manually delete and someone, thanks by the way, shouted out to do an IIS Reset. Well that let me remove the lock and empty the folder. After doing a retry on the failed deployment all went as expected… So your simple deployment should really stop IIS, then update, before enabling it.

    As part of prepping for this demo I did a bunch of work around release management and creating the release pipeline:

    - [Create a Release Management pipeline for Professional Developers](http://nkdagility.com/create-release-management-pipeline-professional-developers/)
    - [Create a Standard Environment for Release Management in Azure](http://nkdagility.com/create-standard-environment-release-management-azure/)
    - [Configure a DNS server for an Azure Virtual Network](http://nkdagility.com/configure-a-dns-server-for-an-azure-virtual-network/)
    - [Move your Azure VM to a Virtual Network](http://nkdagility.com/move-azure-vm-virtual-network/)
    - [Configuring a DC in Azure for AAD integrated Release Management](http://nkdagility.com/configuring-dc-azure-aad-integrated-release-management/)
    - [Create log entries in Release Management](http://nkdagility.com/create-log-entries-release-management/) (coming 2014/12/12)
    - [Join a machine to your azure hosted domain controller](http://nkdagility.com/join-machine-azure-hosted-domain-controller/) (coming 2014/12/17)

    Most of which became irrelevant when Release Management for VSO became available and I no longer had to configure a release management server myself. With the new release cadence from the TFS team, things can only get better…

    My slides are available on Slide Share: [http://www.slideshare.net/MrHinsh/ndclondon2014](http://www.slideshare.net/MrHinsh/ndclondon2014)
  FilePath: C:\Users\MartinHinshelwoodNKD\source\repos\NKDAgility.com\site\content\resources\blog\2014\2014-12-10-ndc-london-2014-why-tfs-no-longer-sucks-and-vso-is-awesome\index.md
  FolderPath: C:\Users\MartinHinshelwoodNKD\source\repos\NKDAgility.com\site\content\resources\blog\2014\2014-12-10-ndc-london-2014-why-tfs-no-longer-sucks-and-vso-is-awesome
- FrontMatter:
    title: Create a Release Management pipeline for Professional Developers
    description: Learn to create a Release Management pipeline for developers, enhancing automation and deployment efficiency with Team Foundation Server. Dive in now!
    ResourceId: WYBOx1X0R-3
    ResourceType: blog
    ResourceContentOrigin: Human
    ResourceImport: true
    ResourceImportId: 10970
    ResourceImportSource: Wordpress
    ResourceImportOriginalSource: Wordpress
    date: 2014-12-04
    weight: 430
    creator: Martin Hinshelwood
    layout: blog
    resourceTypes: blog
    slug: create-release-management-pipeline-professional-developers
    aliases:
    - /resources/WYBOx1X0R-3
    aliasesArchive:
    - /blog/create-release-management-pipeline-professional-developers
    - /create-release-management-pipeline-professional-developers
    - /create-a-release-management-pipeline-for-professional-developers
    - /blog/create-a-release-management-pipeline-for-professional-developers
    - /resources/blog/create-release-management-pipeline-professional-developers
    tags:
    - Software Development
    - Azure Pipelines
    - Continuous Delivery
    - Operational Practices
    - Pragmatic Thinking
    - Azure DevOps
    - Release Management
    - Product Delivery
    - Install and Configuration
    - Application Lifecycle Management
    - Deployment Strategies
    - Technical Excellence
    - Working Software
    - System Configuration
    - Technical Mastery
    categories:
    - Engineering Excellence
    - DevOps
    preview: nakedalm-experts-visual-studio-alm-46-46.png
  BodyContent: |
    Now that I have [it configured](http://nkdagility.com/create-standard-environment-release-management-azure/) I want to show how to create a Release Management pipeline for Professional Developers and Development Teams.

    I was speaking at NDC London 2014 this week and as my talk is all about how [Team Foundation Server does not suck](http://nkdagility.com/ndc-london-second-look-team-foundation-server-vso/) like it used to back before 2012 I need to demonstrate automatic environment deployments as part of my demos with a Release Management pipeline. This session is specifically geared towards users of 2005, 2008, or 2010 that got frustrated with the lack of some features. Specifically hierarchical work item relationships and teams among others. I want to show that the advances since the 2012 release of the product really make it worth considering again.

    ![clip_image001](images/clip_image001-1-1.png "clip_image001")
    { .post-img }

    In order to show the power of TFS I want to do an end to end walkthrough. Most of the above is something that I demo constantly, but the DevOps part has a new version of Release Management and the addition of Application Insights for monitoring. So along with the rest, I want to show some binary assets flowing through a release pipeline.

    ### Create the Source and Build for Fabrikam Fibre

    Now there is not going to be enough time to dive into every little detail, but I want to give a 1000 foot view of all the features. Even then, this is a 3 hour demo squished into about 45 minutes. My plan is to have as much automated as possible and for this I have a copy of the [Fabrikam Fibre web application](http://fabrikam.codeplex.com/) in a Git repository in my VSO account.

    ![clip_image002](images/clip_image002-2-2.png "clip_image002")
    { .post-img }

    This is a fairly simple and well known application, however, key is that it is not an Azure application and is usually deployed locally to your own server. It even uses an old version of .NET and an ancient version of MVC.

    You can either create a new Git Repository on an existing Team Project or create a new Team Project to work in. I am a [firm believer in larger team projects](http://nkdagility.com/one-team-project/) and I have a simple rule for when to create a new Team Project:

    > If you have resources that interact (with resources defined as Work Items, Code, or People) then they should be in the same Team Project. Otherwise create a new one.

    This simple rule means that you don’t get tones of unnecessary team projects and if you are coming from 2010 you will know what I mean.

    ![clip_image003](images/clip_image003-3-3.png "clip_image003")
    { .post-img }

    The first step is to create a repository for your code. You can use the URL and your favourite tool, or just connect with visual studio.

    ![clip_image004](images/clip_image004-4-4.png "clip_image004")
    { .post-img }

    Not the simplest of activities if you don’t currently have a connection to TFS, but if you do you can skip to the clone of the repository. The thing to note is that even a single Team Project can have multiple Git repositories that are all accessible.

    ![clip_image005](images/clip_image005-5-5.png "clip_image005")
    { .post-img }

    Once in you get access to all of the branches in your Git repository and here you can see that I have a bunch already configured. I started with the default "master" branch and committed the new source. After pushing to the server I created a "develop" branch to work on. Many of the things I will be configuring will be done in a quick and dirty manor. Sometimes a little hand waving is necessary when you configure demos as this is not a real application or a real project.

    ![clip_image006](images/clip_image006-6-6.png "clip_image006")
    { .post-img }

    Now I have code I need some way to create my binary output. This is very strait forward and can be done in just a few clicks. SO I created a new build definition, gave it a name and set it to build continuously. That means that if I push code to any of the branches listed above it will queue a new build based on this template.

    #### Hand Wave #1

    I have configured this to do a build for any branch. In the real world you don’t want your minor branches hitting your binary pipeline. You should have a Build configured on MASTER that hits your production pipeline, and a another build for DEVELOP that hits your dev pipeline of feedback servers. If you have feature branches then you may also been feedback pipelines on them as well to create a tight feedback loop.

    ![clip_image007](images/clip_image007-7-7.png "clip_image007")
    { .post-img }

    The only real configuration here is top select the solution. If you have a single solution in your repo then it will be pre populated and you can create a build without changing anything… now Queue it…

    ![clip_image008](images/clip_image008-8-8.png "clip_image008")
    { .post-img }

    No, you don’t need a build controller, VSO is proving it. No, you don’t need a drop location as TFS is providing that as well. Just queue and go…

    #### Hand Wave #2

    My first builds were partially successful. By default if the compile works and tests fail you get a partially successful build. One of the first things I do is make the build fail if the tests fail. To me, a build with no tests is not a useful thing. A little tweak to the code and the build passes no problem.

    ![clip_image009](images/clip_image009-9-9.png "clip_image009")
    { .post-img }

    If we open the build we get our first piece of awesome. The build knows what work items were associated with the code it just built. Not bad… However we need to check the drop location. At the top you will see an 'open drop location' link.

    ![clip_image010](images/clip_image010-10-10.png "clip_image010")
    { .post-img }

    Here we get directed to the web access as the drop location for the build is set to save inside the TFS server. This is an un-versioned store inside of TFS that is automatically maintained. It has an advantage over a network share in that it is backed up with your TFS server. Always make sure that you backup your assets, and your build output is an asset of your organisation.

    ### Create release pipeline for Fabrikam Fibre

    So, we have some build output but nowhere to put it…

    ![clip_image011](images/clip_image011-11-11.png "clip_image011")
    { .post-img }

    wait… I just [created a standard environment in azure for this](http://nkdagility.com/create-standard-environment-release-management-azure/). The idea now is that we need to create a release pipeline to get our stuff from the drop location all the way into this environment.

    ![clip_image012](images/clip_image012-12-12.png "clip_image012")
    { .post-img }

    Its time to fire up the Release Management Client for Visual Studio 2013. Make sure that you have at least update 4 an you can connect to the Release Management Server that is provided as part of VSO.

    ![clip_image013](images/clip_image013-13-13.png "clip_image013")
    { .post-img }

    We also need to connect everything up to Azure as my environments are being hosted there. If you are using the hosted Release Management server then you can only do Azure right now, but that will expand in time. With a locally deployed Release Management Server you can use agents, DSC, or PowerShell to action deployments.

    Here I am restricted to Azure but that’s O)K with me for now. You will need your management certificate key to move forward and RM will stick a bunch of bits into your storage account. It all throw away stuff but for some reason, not sure what, it pushes from the drop folder in TFS to Azure storage before it ends up on you target environment.

    ![clip_image014](images/clip_image014-14-14.png "clip_image014")
    { .post-img }

    Next thing you need to do is specify your stages. I can't reiterate enough how much I hate calling these things Dev / QA / UAT, or any of the other meaningless names in circulation. That my binary output is in "development", or "user acceptance" mode is really merely a state of the output. I can deploy the "uat" output to 10 different environments. So do I call them UAT? What if I deploy a set of "development" bits to that same environment for another set of testing? Do they all have to be renamed to be "dev" environments. We need a name that conveys purpose rather than state or quality. I tend to use the word "feedback". It's really what we are always doing with the environment regardless of the quality of the bits that are deployed to it. So I have Feedback1-3 for my stages…

    ![clip_image015](images/clip_image015-15-15.png "clip_image015")
    { .post-img }

    Now we get down to the actual configuration. If you head over to the "Configure Paths" tab and then click on the "environments" link you should see a list of your currently configured environments. To configure a new one you can either create a local standard environment by using an existing server that you have on your network, or you can use an Azure environment.

    ![clip_image016](images/clip_image016-16-16.png "clip_image016")
    { .post-img }

    In this case I am linking to my brand new "nkd-ff-f2" environment that we created above. Click "Link Azure Environment" and then select the one you want. Then link one or more servers to the definition. This will be where we deploy our bits and you can have as many as you like.

    ![clip_image017](images/clip_image017-17-17.png "clip_image017")
    { .post-img }

    Next head to the "vNext Release Paths". In a local RM instance you will have "Release Paths" as well for the previous version of the release process. All of the setups I am showing is part of the preview of vNext that is in the product. Here I am configuring two stages using the names that we previously created. Then I am aligning my Environments with the stages. So that the "Feedabck-1" stage uses the "nkd-ff-f1" environment.

    You should see from the setup above that while these bits are deployed automatically there is a post-moderation stage where someone needs to approve the bits for moving forward. This is the Development Teams feedback environment and both the coders and testers (the Development Team) need to be happy to move forward.

    "Feedback-2" is owned by the Product Owner and is used as his preview environment for Stakeholders. He will be soliciting feedback here as this is the official working version. While Feedback-1 is updated for every check-in, Feedback-2 should be updated less frequently. We still need to tell Stakeholders that it can be unavailable and set other expectations, but that’s a stakeholder training issue and not a technical one.

    ![clip_image018](images/clip_image018-18-18.png "clip_image018")
    { .post-img }

    Now that we have our continuous deployment pipeline created we need something to deploy to it. This is where we tell Release Management where to find the bits that we want to have deployed. In this case we have a web application that is output to the drop location and I am specifying to get the whole drop.

    #### Hand Wave #3

    I really should spend a little more time on this and figure out how to scope the drop to only a single web application. If I had time I would specify only the web application from the \_PublishedWebsites folder and have the "deploy" folder (later) in there but excluded from the copy.

    ![clip_image019](images/clip_image019-19-19.png "clip_image019")
    { .post-img }

    We can also specify variables that we will use for the deployment. For now the only thing I care about is where the bits get deployed on the target server. I am going to have my servers pre-configured and just xcopy the new bits. This parameter just tells me where.

    ![clip_image020](images/clip_image020-20-20.png "clip_image020")
    { .post-img }

    Now that we have a saved component we can create a "vNext Release Template". This is where we specify the workflow for deployment at each stage. This has been vastly simplified from previous versions.

    I first dragged on a "sequence" and then dropped on a "DSC\\PowerShell" deployment activity. You no longer select tools or drag components on, instead you select the component from the "ComponentName" pick-list above. You also select the server that you want the activity to be executed on. In order to execute a remote PowerShell you need to enter an account with the privileges necessary.

    #### Hand Wave #4

    In the real world you would take the time to scope the permissions for this account down to only what is necessary. I am using the administrator account because its… well… easier.

    Here we also need add our parameter. Technically we already specified a default but what the heck. Click the plus and then select the property name from the pick-list. Enter the value that you want passed. And now we are stuck. I have a configured component, and passed variables, but no execution.

    ### Create a deployment script for Fabrikam Fibre

    In previous versions this would be taken care of for you with a pre-configured custom component. No longer… we need to create a PowerShell.

    ![clip_image021](images/clip_image021-21-21.png "clip_image021")
    { .post-img }

    In the root of my Git repository I have added a ".deploy" folder and in there I have put a "SimpleDeploy.ps1" that clears out the passed destination folder and then copies the new version of the site over.

    #### Hand Wave #5

    If I was doing this for real I might do a 'take offline' file and then remove it after the update. Simple to do but takes a little time to get it right.

    But how do we get this stuff over there. I need for the script to be in the bin folder when the drop folder is uploaded so that it is available in the bits that are downloaded and pushed to the server.

    ![clip_image022](images/clip_image022-22-22.png "clip_image022")
    { .post-img }

    If I was fixing Hand Wave #3 I would likely want to put the script in the website folder and customise the deployment to copy everything except that file. To get the deployment script into the right place we need to get the build to do it for us. So we need another PowerShell that executes as part of the build and moves the bits over. I have saved this script in the ".build" folder.

    ![clip_image023](images/clip_image023-23-23.png "clip_image023")
    { .post-img }

    We can then customise the build definition to call that script after the tests have been executed. This ensures that we have a good build before we "finalise" the drop location. We can do any other manipulation we want here, but this is good enough.

    We now have the SimpleDeploy.ps1 file in the drop location where we can get at it as part of the deployment.

    ![clip_image024](images/clip_image024-24-24.png "clip_image024")
    { .post-img }

    If we then add the script location, relative to the component we can call it and pass all of the variables that we have configured. If we execute a release and select a build we will then get our bits deployed to the correct folder on our server.

    ### Configure Azure Environment for Fabrikam Fibre

    So let's hit that server… goto [http://nkd-ff-f2.cloudapp.net/](http://nkd-ff-f2.cloudapp.net/) and you will get…

    ![clip_image025](images/clip_image025-25-25.png "clip_image025")
    { .post-img }

    Woops… we need to configure ports and IIS… n'stuff…

    ![clip_image026](images/clip_image026-26-26.png "clip_image026")
    { .post-img }

    I need to install and configure IIS and Fabrikam Fiber comes with a MSI to do a basic install. After running both of those I also need to map a port. Fabrikam Fibre runs by default on port 1337…

    ![clip_image027](images/clip_image027-27-27.png "clip_image027")
    { .post-img }

    Configuring a port is easy and since we already have a domain name I just mapped an external port 80 to 1337 on the server. Simples…

    ![clip_image028](images/clip_image028-28-28.png "clip_image028")
    { .post-img }

    So after installing SQL Server Express, SQL Management Studio, IIS, and the Fabrikam Fibre MSI The site is up… wohoo… but this is currently a static version of the application… time for the pipeline to go to work. The error being thrown above is due to the connection string not being right and we need to have parameters configured per environment… to include both the connection string, and the application insights key… these parameters will definitely be different between environments.

    ![clip_image029](images/clip_image029-29-29.png "clip_image029")
    { .post-img }

    Speaking of Application Insights, since I am on the server I am going to go ahead and install the status monitor. This monitor will augment the data collected in JavaScript with server performance as well.

    ![clip_image030](images/clip_image030-30-30.png "clip_image030")
    { .post-img }

    Configuring is easy… just launch the configuration tool, select your application pool, and pick the collector to send the telemetry to. After a restart of IIS your telemetry should be winging its way to AI in just a few seconds.

    ### Pass parameters to the deployment scripts for Fabrikam Fibre

    Now all we need is a working deployment…and from our pipeline so that we have the right parameters…

    ![clip_image031](images/clip_image031-31-31.png "clip_image031")
    { .post-img }

    So if we head back to the Component in Release Management we can go ahead and add a connectionString parameter. As I know we will also need an applicationAnalyticsKey for Application Insights I am going to add that now as well.

    ![clip_image032](images/clip_image032-32-32.png "clip_image032")
    { .post-img }

    Then we head on over to the release template and add the parameters and defaults to both the new values. Here I I changing the SQL database name to be "nkd-ff-f2-db" to match the environment and to make sure there is no possibility of cross talk.

    ![clip_image033](images/clip_image033-33-33.png "clip_image033")
    { .post-img }

    You get the application insights key from the code above. Just select "Get code to monitor my web application" and look for the instrumentation key entry.

    ### Configure Fabrikam Fibre to accept dynamic parameters

    We need to parameterise both the application analytics key and the connection string. Once done the deployment should update the values for each environment when it is actioned.

    ![clip_image034](images/clip_image034-34-34.png "clip_image034")
    { .post-img }

    If you right-click on your web application in Visual Studio you should get the option to "Add Application Insights", once done you can open your ApplicationInsights.config and replace the key with "\_\_applicationAnalyticsKey\_\_". This matches the parameter that we configured and will be using to do the replacement.

    ![clip_image035](images/clip_image035-35-35.png "clip_image035")
    { .post-img }

    The web.config is a little more complicated. We need to be able to debug locally and update dynamically on deployment. The trick is to use the built in dynamic web.config transformation engine. You should fill out the debug connection string as the default in the web.config. This will allow debug easily locally.

    Then you need to update the web.release.config to transform the config to what we need for deployment. I am using the "release" config as… well… that seems to make some semblance of sense. With the transformation in place you need to make a couple of updates to the build configuration and away we go…

    ![clip_image036](images/clip_image036-36-36.png "clip_image036")
    { .post-img }

    I have changes the configuration to build to be that "release" type build. If you have created a custom configuration type then you can just type it in here and the build server will build it. But it will not by default do the transformation. This is usually only done when you use the "publish" button in Visual Studio.

    You know, there is really nothing more dangerous than a publish button in Visual Studio. Yes, lets give every single member of the team the ability to shove any old crap directly to production. I wish the product team would give the ability to forcibly disable that option. I only want build assets deployed, not crappy, untested, and un secured local builds.

    Anyhoo… if we also add "/p:UseWPP_CopyWebApplication=true /p:PipelineDependsOnBuild=false" to the MS Build argument the transformation specified in the configuration will be done and the output will only have one config.

    ![clip_image037](images/clip_image037-37-37.png "clip_image037")
    { .post-img }

    The last piece for the parameters is to have the values replaced. We have a bunch of places that have the "\_\_parameterName\_\_" entry that we need to update. Above you can see that I am just opening and replacing the parameter in specific files… this is the easy way out and I am sure you should do something even more awesome with a HashTable and a loop. This does however work…

    ### Triggering a Release from a Build

    All our ducks are in a row and we now need to make sure that a build triggers a release.

    ![clip_image038](images/clip_image038-38-38.png "clip_image038")
    { .post-img }

    We can edit the vNext Release Template and go to the properties. Here we can select a build, and if that build can trigger a release. In the previous version you had to use a special template on the other end for the build to call the release. With the new version (vNext) this requirement is gone and Release Management now subscribes to the build events. When a build completes it triggers auto-magically.

    ![clip_image039](images/clip_image039-39-39.png "clip_image039")
    { .post-img }

    So let's kick of a build and see where we get to. I already have the first environment working so I now need to make sure that the second is up and running. I only really need two environments for my demos as they centre around the development team, but I am sure that you can see how you can build quite substantial pipelines with ease.

    ![clip_image040](images/clip_image040-40-40.png "clip_image040")
    { .post-img }

    Well, the deployment triggered and successfully deployed to "nkd-ff-f1" as expected.

    ![clip_image041](images/clip_image041-41-41.png "clip_image041")
    { .post-img }

    Not only that but it works and we get data… now to approve the feedback1 version and get the build pushed to the feedback2 environment.

    ![clip_image042](images/clip_image042-42-42.png "clip_image042")
    { .post-img }

    Head over to the "My Approval Requests" tab of the Release Management Client tool and approve the latest push to feedback2. Just approve the one you want to push through, in this case the one with the latest date.

    ![clip_image043](images/clip_image043-43-43.png "clip_image043")
    { .post-img }

    Dam it… now I have to go debug… For some reason the script was unable to remove the "views" folder. What I would try is just to remove it completely and have the tool stick it back… so … I got the same error locally, but managed to retry until is deleted. Let's run another release.

    ![clip_image044](images/clip_image044-44-44.png "clip_image044")
    { .post-img }

    If you just want to re-run a release without creating a new build you can just create a release manually and select the build. However you also get a "Retry failed deployment button", but remember that you have to use the same bits. Its for cases like this where it was a server configuration and you don’t want to go through the while rigmarole.

    ![clip_image045](images/clip_image045-45-45.png "clip_image045")
    { .post-img }

    Awesome… End to end deployment of Fabrikam Fibre to two consecutive feedback environments… DONE!

    ### Conclusion

    Although it took me about 6-12 hours to get this all configured much of it was waiting around for infrastructure tasks to complete. Getting the first build and then the first release are also time consuming as you just have to keep running it and working through the errors and mistakes. A typo can cost you 20 minutes of time.

    Once you are done, however, there is a great sense of achievement of getting your application to deploy end to end. For my demo I plan to do a local change, test, and commit that triggers a release to feedback1. Then the tester verifies that what I have done is correct before the development team approve the release to feedback2. At this point the Product Owner solicits feedback from his stakeholders.

    I am looking forward to the demo and hope all goes well… my backup is the Brian Keller VM that can do end to end Fabrikam Fibre all built in… but not as much fun as VSO and RMO…
  FilePath: C:\Users\MartinHinshelwoodNKD\source\repos\NKDAgility.com\site\content\resources\blog\2014\2014-12-04-create-release-management-pipeline-professional-developers\index.md
  FolderPath: C:\Users\MartinHinshelwoodNKD\source\repos\NKDAgility.com\site\content\resources\blog\2014\2014-12-04-create-release-management-pipeline-professional-developers
- FrontMatter:
    title: Create a Standard Environment for Release Management in Azure
    description: Learn to create a standard environment for Release Management in Azure with Visual Studio. Streamline your deployment process and enhance your workflow!
    ResourceId: iI7MvY2p7RU
    ResourceType: blog
    ResourceContentOrigin: Human
    ResourceImport: true
    ResourceImportId: 10923
    ResourceImportSource: Wordpress
    ResourceImportOriginalSource: Wordpress
    date: 2014-12-04
    weight: 640
    creator: Martin Hinshelwood
    layout: blog
    resourceTypes: blog
    slug: create-standard-environment-release-management-azure
    aliases:
    - /resources/iI7MvY2p7RU
    aliasesArchive:
    - /blog/create-standard-environment-release-management-azure
    - /create-standard-environment-release-management-azure
    - /create-a-standard-environment-for-release-management-in-azure
    - /blog/create-a-standard-environment-for-release-management-in-azure
    - /resources/blog/create-standard-environment-release-management-azure
    tags:
    - Software Development
    - Azure DevOps
    - Install and Configuration
    - System Configuration
    - Release Management
    - Application Lifecycle Management
    - Product Delivery
    categories:
    - Uncategorized
    preview: nakedalm-windows-logo-16-16.png
  BodyContent: |
    Next week I will be [speaking at NDC London 2014](http://nkdagility.com/ndc-london-second-look-team-foundation-server-vso/) and I have been working on my demo. Since Connect() everything for a little bit easier and I need to create an environment for Release Management for Visual Studio 2013.

    I have been working on a new end to end (soup to nuts) demo for Visual Studio ALM that \[tells a story\] and shows what it can do. My presentation at NDC London is aimed at those folks that have used older versions of TFS in the past and kinda think it sucks. I think they are wrong, and I want to prove it. So for the last week I have been creating a demo using:

    - **Visual Studio Online (VSO)** - TFS hosted by Microsoft so zero hassle.
    - **Release Management for VSO** - Hosted Release Management that connects to Visual Studio Online, again with the zero hassle.
    - **Visual Studio 2015 Preview** - coz, well, its out and it works.
    - **Git** \- for source control there is no better platform
    - **Application Insights** - free way to gather swim data for your applications.

    The idea is to show an end to end resolution of a bug from it being reported to is being fixed using all of the tools that come, out of the box, with Visual Studio ALM. I will be using the Fabrikam Fiber code, imported into a Git repo and linked to work items in TFS.

    ![clip_image001[6]](images/clip_image0016-1-1.png "clip_image001[6]")
    { .post-img }

    I have already configured an automated deployment to one of my environments and every check-in is magically deployed to the server. This, while sitting in Azure, is just a Virtual Machine. I picked VM, or Standard Environment, so that I can emulate as closely as possible what you can do on premises.

    ![clip_image002[6]](images/clip_image0026-2-2.png "clip_image002[6]")
    { .post-img }

    For this I need a Feedback 2 environment and for that I need to configure it in Azure. Many of the steps and capabilities are just the same as for an on-premises environment but for Azure I don’t need to configure a domain, TFS, or Release Management servers. Indeed I don’t even need any hardware. I just need a credit card :).

    ![clip_image003[6]](images/clip_image0036-3-3.png "clip_image003[6]")
    { .post-img }

    For my currently existing Feedback 1 (nkd-FF-F1) environment I have spent only £3.34 of my £99 MSDN Ultimate allotment and the expected total cost is less than £2 per day for a VM with 2 cores and 3.5GB of RAM. Merger but adequate for this purpose.

    ![clip_image004[6]](images/clip_image0046-4-4.png "clip_image004[6]")
    { .post-img }

    If you are like me and you like things to be in order I recommend that you first create the Resource Group (Cloud Service) in the old portal ([http://manage.windowsazure.com](http://manage.windowsazure.com)). The new portal does not give you good control over naming and as you can't rename stuff your resource group has a DNS name of the first resource added… poopy…

    However if you create a Cloud Service in the old portal you get to name it and it becomes an empty container in the new portal. Simple, but effective. I actually find that I jump a lot between portals as the new one has a bunch of functionality that the old one does not. Its kinda fragmented just now and I know that Microsoft want just one, but the new portal is taking a while.

    ![clip_image005[6]](images/clip_image0056-5-5.png "clip_image005[6]")
    { .post-img }

    Now that I have an empty Resource Group I want to add a server. Again the new portal is not going to cut it as you can't select a template: you can only select from the bits there. So… back to the old portal.

    ![clip_image006[6]](images/clip_image0066-6-6.png "clip_image006[6]")
    { .post-img }

    The first task is to create a storage location. I want to be able to isolate all of the data so I will have a "nkdfff2" storage account. I really wish that the Azure team was consistent with naming as you can't use a "-" here. Might be a small issue but it peeves me off every time I hit it. It’s a stupid issue to have at all.

    ![clip_image007[6]](images/clip_image0076-7-7.png "clip_image007[6]")
    { .post-img }

    Then comes the custom Virtual Network. If you want to have more than one server in your environment then you probably want to have a Virtual Network. Even though I only have one here, it’s a good exercise to create this. Ultimately you would want to script this out once you have a flow defined. I am only doing this twice, so scripting is a little bit of overkill at this point. But if I wanted many feedback environments I would want this scripted.

    ![clip_image008[6]](images/clip_image0086-8-8.png "clip_image008[6]")
    { .post-img }

    We need somewhere to deploy out application. AS I am emulating Standard Environments I will be using a simple VM rather than a "Azure Website". This will better reflect a local instance of RM and TFS for folks. Unfortunately in the new portal you can't use the template gallery, or at least I could not figure out how. But on the old/current portal I can easily do this.

    ![clip_image009[6]](images/clip_image0096-9-9.png "clip_image009[6]")
    { .post-img }

    You might ask why I would pick the Windows Server Technical Preview but I would say, why not :).

    ![clip_image010[6]](images/clip_image0106-10-10.png "clip_image010[6]")
    { .post-img }

    Here I am using the same naming convention as for my other feedback environments and keeping to the A2 server size.

    ![clip_image011[6]](images/clip_image0116-11-11.png "clip_image011[6]")
    { .post-img }

    We then get asked to select all of those little things that we just created. Pick the cloud service so that you have the right URL and address space. We have a network and storage account that we created earlier which, while overkill, emulates more accurately how you would actually create an environment. Due diligence recommends that you always run your environments, especially if they are multi server, in isolated vlans so that you can prevent any sort of cross access between them.

    Large companies are really good at this but smaller ones usually just take the risks. If you are small however, Azure provides simple cheap isolation.

    ![clip_image012[6]](images/clip_image0126-12-12.png "clip_image012[6]")
    { .post-img }

    Almost done; we have the VM running our favourite flavour of windows, or at least it will be once the provisioning has completed. Now to round out our Resource Group I want to bind some data collection for analytics.

    ![clip_image013[6]](images/clip_image0136-13-13.png "clip_image013[6]")
    { .post-img }

    We should use the Application Insights for our application analytics even if we are building an on-premises application. Have you ever wondered what and how Microsoft gathers their "provide us feedback", well AI is an attempt to commercialise that capability for the rest of us.

    You can track feature usage within your application be it Web, Windows, or Java. For some it is easier than others but both server and client data can be gathered.

    ![clip_image014[6]](images/clip_image0146-14-14.png "clip_image014[6]")
    { .post-img }

    Again we need to give the resource, in this case the Application Insights data collector, a name so that we can refer to it later. Under the covers it gets a GUID that we will be using later.

    ![clip_image015[6]](images/clip_image0156-15-15.png "clip_image015[6]")
    { .post-img }

    I now have a Resource Group (Cloud Compute) that is a representation of my Standard Environment that I can use for my application. Using Azure this is easy to configure and fast. Indeed I can extend the capabilities with more servers at almost the drop of a hat.

    This environment contains:

    - **Domain Name (nkd-ff-f2)** - remember we had to use the old portal otherwise the new portal gives it a domain name of the first resource that requires one.
    - **Virtual Machine (nkd-ff-f2-svr01)** - This is where we will be deploying our application and as it is IAAS (Infrastructure as a Service) it is identical to an on-premises instance.
    - **Application Insights (nkd-ff-f2-AI)** - Collects the application analytics that we will push into our application. If you are deploying multiple applications on the same hardware you may want to separate the data.

    Next time I will be deploying an application to this environment that we created.
  FilePath: C:\Users\MartinHinshelwoodNKD\source\repos\NKDAgility.com\site\content\resources\blog\2014\2014-12-04-create-standard-environment-release-management-azure\index.md
  FolderPath: C:\Users\MartinHinshelwoodNKD\source\repos\NKDAgility.com\site\content\resources\blog\2014\2014-12-04-create-standard-environment-release-management-azure
- FrontMatter:
    title: Configure a DNS server for an Azure Virtual Network
    description: Learn how to configure a DNS server for your Azure Virtual Network, ensuring seamless domain integration and network management. Enhance your cloud setup today!
    ResourceId: sQQqpuZV5xQ
    ResourceType: blog
    ResourceContentOrigin: Human
    ResourceImport: true
    ResourceImportId: 10878
    ResourceImportSource: Wordpress
    ResourceImportOriginalSource: Wordpress
    date: 2014-11-26
    weight: 1000
    creator: Martin Hinshelwood
    layout: blog
    resourceTypes: blog
    slug: configure-a-dns-server-for-an-azure-virtual-network
    aliases:
    - /resources/sQQqpuZV5xQ
    aliasesArchive:
    - /blog/configure-a-dns-server-for-an-azure-virtual-network
    - /configure-a-dns-server-for-an-azure-virtual-network
    - /resources/blog/configure-a-dns-server-for-an-azure-virtual-network
    tags:
    - Install and Configuration
    - System Configuration
    categories:
    - Uncategorized
    preview: nakedalm-windows-logo-4-4.png
  BodyContent: |
    I just got done [configuring a DC in Azure for AAD integrated Release Management](http://nkdagility.com/configuring-dc-azure-aad-integrated-release-management/) and I need to now Configure a DNS server for an Azure Virtual Network. This will tell Azure that any servers that are added to this virtual network should use this DNS server. This should allow any machine added to this virtual network to be able to join the domain that we have configured.

    Before we can set a DNS server default we need to have a fixed IP Address for the server. Although DNS provides name resolutions so that we do not need to use IP's all the time you need a place to start. In the big bad internet there are 'name servers' that start the ball rolling that exist at a well known level. Within our virtual network we need to create our own well known starting point.

    ![clip_image001](images/clip-image0012-1-1.png "clip_image001")
    { .post-img }

    There is a simple command to give your server a fixed IP within your virtual network. You can apply it to any server and it allows the internal virtual network IP to persist even if you turn off the server. This does not affect the external IP.

    ```
    Get-AzureVM -ServiceName nkd-infra -Name nkd-inf-svrdc01 | Set-AzureStaticVNetIP -IPAddress 10.0.0.4 | Update-AzureVM
    ```

    There is also a 'check IP' command that, as I only currently have a single server is a little pointless. I just set the servers current IP as the fixed IP for the future.

    ![clip_image002](images/clip-image0022-2-2.png "clip_image002")
    { .post-img }

    We first need to create a DNS server definition that we can select later. Here we simply set the name and IP of the DNS server to create a registration of that DNS server.

    ![clip_image003](images/clip-image0032-3-3.png "clip_image003")
    { .post-img }

    We then need to go to the virtual network that we created and tell it that the DNS server should be the one to use. If we had a large network we may set more than one DNS server, but in this case we are just pottering around with the configuration for demos. Select the network and go to the configuration tab. Here we can select our pre-created DNS server.

    If you create new machines, or reboot the existing machines in the virtual network, they will then be given this DNS server when DHCP assigns configuration. In this way you can create quite complicated network configurations and even create backup domains controllers to allow you to extend your local network to the cloud.
  FilePath: C:\Users\MartinHinshelwoodNKD\source\repos\NKDAgility.com\site\content\resources\blog\2014\2014-11-26-configure-a-dns-server-for-an-azure-virtual-network\index.md
  FolderPath: C:\Users\MartinHinshelwoodNKD\source\repos\NKDAgility.com\site\content\resources\blog\2014\2014-11-26-configure-a-dns-server-for-an-azure-virtual-network
- FrontMatter:
    title: Microsoft Surface 3 unable to boot from USB
    description: Struggling to boot your Microsoft Surface 3 from USB? Discover the solution to this frustrating issue and get your device back on track with our guide!
    ResourceId: a3t-_ypf_UJ
    ResourceType: blog
    ResourceContentOrigin: Human
    ResourceImport: true
    ResourceImportId: 10907
    ResourceImportSource: Wordpress
    ResourceImportOriginalSource: Wordpress
    date: 2014-11-24
    weight: 1000
    creator: Martin Hinshelwood
    layout: blog
    resourceTypes: blog
    slug: microsoft-surface-3-unable-boot-usb
    aliases:
    - /resources/a3t-_ypf_UJ
    aliasesArchive:
    - /blog/microsoft-surface-3-unable-boot-usb
    - /microsoft-surface-3-unable-boot-usb
    - /microsoft-surface-3-unable-to-boot-from-usb
    - /blog/microsoft-surface-3-unable-to-boot-from-usb
    - /resources/blog/microsoft-surface-3-unable-boot-usb
    tags:
    - Troubleshooting
    - Windows
    categories:
    - Uncategorized
    preview: nakedalm-windows-logo-5-5.png
  BodyContent: |
    Recently I had a crash of Windows 10 and while usually you can boot into recovery mode, Windows was uncooperative. This was when I found my Microsoft Surface 3 unable to boot from USB!

    I ordered a new Surface 3 Pro 512 to be delivered in the USA that I would pick up when I was at the MVP Summit a few weeks ago. I had ordered it more than 2 months ago for it to be delivered to my good friends at Northwest Cadence. While the other parts of my order were delivered, Docking Station and cables, the Surface 3 was marked as "Pending" in the web. Then all of my orders disappeared entirely.

    Now, I have orders on both the US and UK site so I just assume that things are a little squirrely and give them a call. The was 2 weeks before I was due to be in the USA for only 8 days so the window was a little tight. Long and short the support desk ended up reissuing the Surface 3 and I got delivery conformation a few days later. Awesome…

    I headed out to the Summit and Amanda (from Northwest Cadence) dropped my orders off at me hotel. On arrival I had a wonderful unboxing event and found that the first order had indeed been fulfilled. I had 2 Surface Pro 3's, one will have to go back, but that will have to wait.

    So I stuck one in my suitcase to deal with later and opened one for me. First thing I did was install Windows 10, and everything went well for a while until I got a dreaded blue screen.

    ![clip_image001](images/clip-image0011-1-1.jpg "clip_image001")
    { .post-img }

    > Recovery
    >
    > Your PC/Device needs to be repaired
    >
    > The digital signature for this file could not be verified.
    >
    > File: \\windows\\system32\\winload.efi
    >
    > Error Code: 0xc0000428

    I was at a session with the product team, and after a few minutes poking at buttons and powering on and off I managed to get the device to boot into Recovery mode and ran a refresh. Simples…

    Unfortunately it happened again, the same error, only a few days later and worse, this was on the Saturday a few hours before my flight back to the UK at the end of the Summit. Tis time I could not get it to boot into recovery. Not at all, so I instead a USB to do a full refresh and found that my Microsoft Surface 3 was unable to boot from the USB stick.

    Strange… So I fired up my Surface 2 Pro (hey, it was a travel day, I had everything) and tried it there. Sure enough the Surface 2 was able to boot from the same USB.

    Not good, but I was in Starbucks at Bellevue Mall and headed into the Microsoft Store and bought a new USB stick to see if it was my stick that was the issue. I used my Surface 2 to create a bootable USB and tested it there. Still no dice. But again, after about 40 minutes of jiggery-pokery (really no other work for it) I ended up in Restore mode and was able to reset Windows.. Phew…

    Now, if I was sensible, at this point I really should have done a full wipe and reinstall. This file had messed up twice and the like hood was high that it would happen again. However I was running for a flight to Scotland with a 6 hour layover before flying out onsite to a customer in Norway. Yes, that’s a five country trip: Seattle -> Rekiovik -> Glasgow -> Amsterdam -> Oslo. So I am blaming travel, stress, and jetlag on my lack of judgement.

    I was onsite with a customer in Oslo for only 2 days when my Surface 3 flashed the same error. After 2 hours of jiggery-pokery I got absolutely nowhere! Luckily the customer provided me with a Dell brick of a laptop which had the saving grace that it booted. I spent some time each night onsite trying to get the bloody thing to boot and searched the interwebs for results. Microsoft have good documentation for how to boot your Surface from a USB.

    > ##### Start from a bootable USB device when Surface is off
    >
    > **Step 1:** Attach a bootable USB device to the USB port.
    >
    > **Step 2:** Press and hold the volume-down button.
    >
    > **Step 3:** Press and release the power button.
    >
    > **Step 4:** When the Surface logo appears, release the volume-down button.  
    > Surface will start the software on your USB device.  
    > \-[Boot Surface from a USB device](http://www.microsoft.com/surface/en-gb/support/storage-files-and-folders/boot-surface-pro-from-usb-recovery-device)

    When I got home I broke out the second Surface that Microsoft shipped me by accident and took that to my customer. I tend to fly home late on Friday night and then out again on Sunday. This usually gives me Saturday and Sunday morning with the kids, but the Oslo flights are a little suckie. The later in the day I fly the more expensive the flights get. 06:00 flight is half the price of the 15:00, not chump change, and the later flights get filled up first. So this weekend I flew out at 06:00… One day home and back out to the customer. Dam I am glad that MSFT shipped me that extra tablet (Microsoft is still trying to figure out how to ship it back to the USA).

    So, this time I did not install Windows 10. I spent all week this week on Windows 8.1 and missing lots of features from 10:

    - **Modern in a Window** - On a small tablet this sucks (re Dell Venue 8) but on a desktop replacement that I mostly use with keyboard and mouse.
      ![clip_image002](images/clip-image0026-2-2.png "clip_image002")
      { .post-img }
    - **Mini modern start menu** - This is touch and go. I really like the full screen start menu, but the largest screen I use is my Surface 3. I have seen folks using it on a 32" screen and it is more like a punch in the face.
      ![clip_image003](images/clip-image0035-3-3.png "clip_image003")
      { .post-img }

    At the beginning of the week I reached out to some contacts in MSFT to see if we could not figure out the USB issue. I found that \[Clement\] also had the same issue and while was able to use Recovery Mode was unable to get his surface to boot from USB either. So that’s 3 for 3.

    I even tried getting Windows 8.1, on my surface, to create Recovery USB and booting from that. Still no dice.

    ## Solution: Microsoft Surface 3 unable to boot from USB

    After some time I managed to get in touch with someone at MSFT who knew about this part of the system and indeed had an identical one to me. He had no issues booting so we set about diagnosing the issue. I pointed at the documentation and he asked a few questions:

    > Do any of the Pro3 systems you are currently using still boot into Windows?  
    > If so, what have version of Windows have you got installed at this point?
    >
    > Do the volume up/down buttons work in the OS?
    >
    > How did you create the USB key? Is it labelled “BOOTME”?

    Wait! What!... "BOOTME"???

    I am not sure where this came from but when I renamed my USB to "BOOTME" sure enough, without issue, the Surface 3 booted from it. OMG! That’s two weeks wasted on an undocumented feature that prevents even a Windows created Recovery Image to be unusable. By default there are three ways to create a bootable USB:

    1. **Manually** \- Format, Create partition, mark as active, then copy files
    2. **Windows 7 USB** - From Microsoft in Windows 7 timeframe came an executable that you give an ISO and it creates a bootable USB
    3. **Windows Recovery** - In Windows you can, and should, create a recovery image that you can use to boot your computer and reset it. This boot USB will be named “Recovery” by Windows.

    In not a single one of these methods does the USB get named "BOOTME". Ahh, well… never mind… Now we all know.

    ![clip_image004](images/clip-image0044-4-4.png "clip_image004")
    { .post-img }

    So if you are wanting to boot your Surface Pro 3 from a USB you need to make sure that the device you want to boot from is labelled "BOOTME".
  FilePath: C:\Users\MartinHinshelwoodNKD\source\repos\NKDAgility.com\site\content\resources\blog\2014\2014-11-24-microsoft-surface-3-unable-boot-usb\index.md
  FolderPath: C:\Users\MartinHinshelwoodNKD\source\repos\NKDAgility.com\site\content\resources\blog\2014\2014-11-24-microsoft-surface-3-unable-boot-usb
- FrontMatter:
    title: Move your Azure VM to a Virtual Network
    description: Learn how to move your Azure VM to a virtual network with this step-by-step guide. Optimize your setup for better performance and connectivity!
    ResourceId: nTSBTp1_gBy
    ResourceType: blog
    ResourceContentOrigin: Human
    ResourceImport: true
    ResourceImportId: 10874
    ResourceImportSource: Wordpress
    ResourceImportOriginalSource: Wordpress
    date: 2014-11-19
    weight: 1000
    creator: Martin Hinshelwood
    layout: blog
    resourceTypes: blog
    slug: move-azure-vm-virtual-network
    aliases:
    - /resources/nTSBTp1_gBy
    aliasesArchive:
    - /blog/move-azure-vm-virtual-network
    - /move-azure-vm-virtual-network
    - /move-your-azure-vm-to-a-virtual-network
    - /blog/move-your-azure-vm-to-a-virtual-network
    - /resources/blog/move-azure-vm-virtual-network
    tags:
    - Install and Configuration
    - System Configuration
    categories:
    - Uncategorized
    preview: nakedalm-windows-logo-8-8.png
  BodyContent: |
    When I first completed [configuring a DC in Azure for AAD integrated Release Management](http://nkdagility.com/configuring-dc-azure-aad-integrated-release-management/) I did not add my virtual machine to a virtual network. And I really should have and in the usual poopyness that is servers you can't move it. You effectively need to delete your VM leaving the disks and create a new machine definition that is correctly configured.

    ![clip_image001](images/clip-image0011-1-1.png "clip_image001")
    { .post-img }

    First we need to configure the virtual network. Create a new virtual network in the correct region. The region should be the same as the one that you want to create the vm's in, in my case western Europe fits that bill.

    ![clip_image002](images/clip-image0021-2-2.png "clip_image002")
    { .post-img }

    Then the poopy part, we need to delete the Virtual Server that we created and promoted to be a domain controller. Make sure that you do not delete the disks.

    ![clip_image003](images/clip-image0031-3-3.png "clip_image003")
    { .post-img }

    We now need to create a new VM in the correct domain. Give it a few minutes to clear the name in the tubes of Azure so that we can reuse it and then create a new VM but select the Gallery.

    ![clip_image004](images/clip-image0041-4-4.png "clip_image004")
    { .post-img }

    In the gallery you should find a "my disks" section at the very bottom that lists all of your free floating disks that are not attached to a VM. I found that one of my servers did not exist and I had to wait a few more minutes for it to show up. Select your disks and click next…

    ![clip_image005](images/clip-image0051-5-5.png "clip_image005")
    { .post-img }

    Give the machine the same name and pick the A0 instance size that we wanted before. We should not have to log into the server at this time.

    ![clip_image006](images/clip-image0061-6-6.png "clip_image006")
    { .post-img }

    On the second screen we need to make sure that we select the virtual network that we just created. This will alter the other options that we can select but it is very simple to configure. On the next screen you need only pick what additional bits that you want and I only really want the VM tools for an AD box, but for other boxes you may want more.

    ![clip_image007](images/clip-image0071-7-7.png "clip_image007")
    { .post-img }

    You should now see your domain controller as part of your virtual network that we just created. Even if we have many cloud services we can add their containing machines to this network and allow communication between them.

    **Useful links:**

    - [http://azure.microsoft.com/en-us/documentation/articles/active-directory-new-forest-virtual-machine/#createvnet](http://azure.microsoft.com/en-us/documentation/articles/active-directory-new-forest-virtual-machine/#createvnet)
    - [http://msdn.microsoft.com/library/azure/dn630228.aspx](http://msdn.microsoft.com/library/azure/dn630228.aspx)
    - [http://blogs.msdn.com/b/walterm/archive/2013/05/29/moving-a-virtual-machine-from-one-virtual-network-to-another.aspx](http://blogs.msdn.com/b/walterm/archive/2013/05/29/moving-a-virtual-machine-from-one-virtual-network-to-another.aspx)
  FilePath: C:\Users\MartinHinshelwoodNKD\source\repos\NKDAgility.com\site\content\resources\blog\2014\2014-11-19-move-azure-vm-virtual-network\index.md
  FolderPath: C:\Users\MartinHinshelwoodNKD\source\repos\NKDAgility.com\site\content\resources\blog\2014\2014-11-19-move-azure-vm-virtual-network
- FrontMatter:
    title: Configuring a DC in Azure for AAD integrated Release Management
    description: Learn how to configure a Domain Controller in Azure for AAD integrated Release Management, enhancing your cloud demo capabilities with practical insights.
    ResourceId: 6z5uZom4gof
    ResourceType: blog
    ResourceContentOrigin: Human
    ResourceImport: true
    ResourceImportId: 10865
    ResourceImportSource: Wordpress
    ResourceImportOriginalSource: Wordpress
    date: 2014-11-14
    weight: 790
    creator: Martin Hinshelwood
    layout: blog
    resourceTypes: blog
    slug: configuring-dc-azure-aad-integrated-release-management
    aliases:
    - /resources/6z5uZom4gof
    aliasesArchive:
    - /blog/configuring-dc-azure-aad-integrated-release-management
    - /configuring-dc-azure-aad-integrated-release-management
    - /configuring-a-dc-in-azure-for-aad-integrated-release-management
    - /blog/configuring-a-dc-in-azure-for-aad-integrated-release-management
    - /resources/blog/configuring-dc-azure-aad-integrated-release-management
    tags:
    - Install and Configuration
    - Windows
    - System Configuration
    - Software Development
    - Technical Mastery
    categories:
    - Uncategorized
    preview: nakedalm-windows-logo-22-22.png
  BodyContent: |
    I will be [speaking at NDC London: Second Look, Team Foundation Server & VSO](http://nkdagility.com/ndc-london-second-look-team-foundation-server-vso/) and I am planning to be a little adventurous with the demo. For this I will be configuring a DC in Azure for AAD integrated Release Management so that I can do cloud demos.

    While potentially similar to the [Brian Keller VM demos](http://blogs.msdn.com/b/briankel/archive/2013/08/02/visual-studio-2013-application-lifecycle-management-virtual-machine-and-hands-on-labs-demo-scripts.aspx) I wanted a more end to end solution that was a little more real world. I decided to run everything in Azure after the success of [configuring the BKVM in Azure for Training](http://nkdagility.com/creating-training-virtual-machines-azure/). I can make no guarantees that this will end up as the final demo, but it will be fun to build.

    ![clip_image001](images/clip-image001-1-1.png "clip_image001")
    { .post-img }

    The first environment that I need is my permanent infrastructure. Top left in the diagram. I need to be able to join a bunch of computers to domain and sync that domain with AAD. All in the cloud.

    #### Note

    It would be far superior if I was able to just join any machine that I like to an AAD domain. However, currently, there is no support for a direct join. You need to have the traditional static domain for now.

    ### Creating the virtual machine in Azure for Active Directory

    Creating a new machine is almost the bread and butter of Azure. You can create as many machines as you can afford and anything from small, to large, to really enormous. There is the A series machines that are cheap and cheerful. They are the slowest options and good for low demand operations, like active directory.

    ![clip_image002](images/clip-image002-2-2.png "clip_image002")
    { .post-img }

    You first need to have a cloud service to add your machine to. This is a little bit like a resource group as each cloud service is limited to a certain number of cores, with that defaulting to 20.

    #### Note

    You can send a support request to get the number of available cores increased as I did when I was [setting up training VM's](http://nkdagility.com/creating-training-virtual-machines-azure/) for my recent course.

    So head from [http://manage.microsoftazure.com](http://manage.microsoftazure.com) to "Cloud Services | New" to pop the menu and select "Quick Create" and fill out your details. You can't move region later so pick the right one now, closest to the consumers. In this case I want "West Europe".

    ![clip_image003](images/clip-image003-3-3.png "clip_image003")
    { .post-img }

    We also need a storage location to store the hard disks for our VM's.

    ![clip_image004](images/clip-image004-4-4.png "clip_image004")
    { .post-img }

    Now we have our cloud service we can get going creating the first VM that will be the Domain Controller (DC). Select "Virtual Machines | New" to pop the menu and choose "From Gallery".

    ![clip_image005](images/clip-image005-5-5.png "clip_image005")
    { .post-img }

    Now, I'm a progressive kinda guy and there is already a listing for Windows Server Technical Preview and that’s what I will be using. This applies to all of my servers that I create and I would only use something else if I encounter a problem with an application. I feel that this is low risk as they are now on a cadence of making smaller changes more frequently. Much more likely to find and fix the issue in the integration the fewer things that you have changed at once.

    ![clip_image006](images/clip-image006-6-6.png "clip_image006")
    { .post-img }

    For a domain controller you really don’t need much oomph… however to use the machine interactively you really don’t want it to lag… so for install and configuration I will run an A2 (2 cores and 3GB RAM), and then once I am done I will downgrade it to a A0 (shared core, 0.9GB RAM). That’s plenty for a domain controller. If I need to physically configure the machine later then more power is only a reboot away.

    ![clip_image007](images/clip-image007-7-7.png "clip_image007")
    { .post-img }

    You should at this point go off and create a network to add your server to. That will save you having to \[Move your Domain Controller Azure VM to a Virtual Network\] at a later time. Alas I did not know that for now and so I just configured it for the West Europe data centre and all the trappings of storage and cloud service.

    ![clip_image008](images/clip-image008-8-8.png "clip_image008")
    { .post-img }

    There are lots of cool options for managing your server and deploying bits to it if you want, but this is going to be a simple light weight DC.

    ![clip_image009](images/clip-image009-9-9.png "clip_image009")
    { .post-img }

    Woo… after a few minutes I can then log onto my server and tinker. If you find it to slow you can head to the "Configure" tab and bump up the server. Like I said… I bumped to A2 for interactive configuring.

    ### Install Active Directory required components

    Installing active directory in your VM on Azure is uncannily identical to installing it on any other machine. That’s because it is! So if you know how to do this bit you can skip to the end.

    ![clip_image010](images/clip-image010-10-10.png "clip_image010")
    { .post-img }

    Everything in Windows is added using the "add roles and features" wizard. To be honest it just build a little PowerShell and that does all the work… and I use PowerShell… but not when there is a perfectly servable UI.

    ![clip_image011](images/clip-image011-11-11.png "clip_image011")
    { .post-img }

    Active Directory is a Server Roll and it’s a simple checkbox away. When you check it the wizard will ask you to add a bunch of other stuff. It’s a bunch of management tools and the PowerShell commandlets. Those might be useful later.

    ![clip_image012](images/clip-image012-12-12.png "clip_image012")
    { .post-img }

    You also need to add the DNS role as active directory relies very heavily on dynamic dns for resolutions. The AD wizard will automatically populate this stuff for you so you will not need to configure it separately.

    ![clip_image013](images/clip-image013-13-13.png "clip_image013")
    { .post-img }

    Whoh… can you even add a static IP in Azure? For a DNS server to work it needs to have the same IP address. It is the thing that is going to be providing resolution for your other servers and they need to known where to find it. Turns out you can easily [configure a static ip for a dns server in your virtual network](http://nkdagility.com/configure-a-dns-server-for-an-azure-virtual-network/?preview=true).

    ![clip_image014](images/clip-image014-14-14.png "clip_image014")
    { .post-img }

    Following the lead of the TFS team it looks like all of Microsoft products are moving to a model where you keep installation and configuration separate. This is a far less error prone model than the traditional all in one model.

    Now that we have the bits installed we can move to configuration.

    ### Promote your Azure VM to be a domain controller

    With Active Directory we are doing a lot more than just adding a capability to our server. This is not the same as setting up IIS, we are changing the fundamental way that this server functions.

    ![clip_image015](images/clip-image015-15-15.png "clip_image015")
    { .post-img }

    You should see a little triangle after the installation is complete prompting you to "promote this server to domain controller". So lets go ahead and give this server special powers.

    ![clip_image016](images/clip-image016-16-16.png "clip_image016")
    { .post-img }

    As we are starting from scratch we need to create a whole new infrastructure. For this we need to "add a new forest" and give it a DNS name. I have been through this a few times and favour "env.mydomain.com" for this. If I was creating a primary production domain I would use the same name as my public domain, and I would never…ever… pick mydomain.local.

    ![clip_image017](images/clip-image017-17-17.png "clip_image017")
    { .post-img }

    There are quite a few configuration screens and I am not going to bore you with them all you have to set your NetBIOS name ("nakedalm") and where you want your data to reside but most of the screens are fairly self-explanatory. However the DNS screen will give you an error if you ticked the DNS option, as I did, during the feature selection. The result of that wordy warning is that "no action is required" so we are good.

    ```
    Import-Module ADDSDeployment
    Install-ADDSForest `
    -CreateDnsDelegation:$false `
    -DatabasePath "C:\Windows\NTDS" `
    -DomainMode "Win2012R2" `
    -DomainName "env.nakedalmweb.wpengine.com" `
    -DomainNetbiosName "nkdalm" `
    -ForestMode "Win2012R2" `
    -InstallDns:$true `
    -LogPath "C:\Windows\NTDS" `
    -NoRebootOnCompletion:$false `
    -SysvolPath "C:\Windows\SYSVOL" `
    -Force:$true
    ```

    ![clip_image018](images/clip-image018-18-18.png "clip_image018")
    { .post-img }

    Check the configuration, ignore the warnings and away we go… I do however miss the "this will take some time… or considerably longer" message the old AD installation had, however it was pretty quick…

    ![clip_image019](images/clip-image019-19-19.png "clip_image019")
    { .post-img }

    At some point you will be asked to sign out and your server will be restarted to be embowed with the powers of AD.

    ![clip_image020](images/clip-image020-20-20.png "clip_image020")
    { .post-img }

    When it comes back up you will no longer be able to log into your server locally and will log into the domain. Your local accounts will have been converted to domains accounts and will be listed in Active Directory Users and Computers.

    Now that you have completed the install you can drop the server down to the A0 machine level to save money.

    ![clip_image021](images/clip-image021-21-21.png "clip_image021")
    { .post-img }

    We effectively drop down to 11p per day for the server. I am sure that if we started hitting it with loads of domain joined machines then I expect the price to go up, however this minimalist cost can be easily supported with your MSDN benefits…
  FilePath: C:\Users\MartinHinshelwoodNKD\source\repos\NKDAgility.com\site\content\resources\blog\2014\2014-11-14-configuring-dc-azure-aad-integrated-release-management\index.md
  FolderPath: C:\Users\MartinHinshelwoodNKD\source\repos\NKDAgility.com\site\content\resources\blog\2014\2014-11-14-configuring-dc-azure-aad-integrated-release-management
- FrontMatter:
    title: Installing Visual Studio 2015 side by side with 2013 on Windows 10
    description: Learn how to install Visual Studio 2015 alongside 2013 on Windows 10 effortlessly. Get tips, fixes, and insights for a smooth setup experience!
    ResourceId: HPkHnEoaEbR
    ResourceType: blog
    ResourceContentOrigin: Human
    ResourceImport: true
    ResourceImportId: 10886
    ResourceImportSource: Wordpress
    ResourceImportOriginalSource: Wordpress
    date: 2014-11-12
    weight: 1000
    creator: Martin Hinshelwood
    layout: blog
    resourceTypes: blog
    slug: installing-visual-studio-2015-side-side-2013-windows-10
    aliases:
    - /resources/HPkHnEoaEbR
    aliasesArchive:
    - /blog/installing-visual-studio-2015-side-side-2013-windows-10
    - /installing-visual-studio-2015-side-side-2013-windows-10
    - /installing-visual-studio-2015-side-by-side-with-2013-on-windows-10
    - /blog/installing-visual-studio-2015-side-by-side-with-2013-on-windows-10
    - /resources/blog/installing-visual-studio-2015-side-side-2013-windows-10
    tags:
    - Windows
    - Install and Configuration
    categories:
    - Uncategorized
    preview: nakedalm-experts-visual-studio-alm-8-8.png
  BodyContent: |
    You know that I like nothing better than taking a few risks and I have been very happy in the past running MSFT preview bits on bare metal. As Microsoft has just announced Visual Studio 2015 Ultimate Preview I need to be running it on my local computer for demos.

    - [Download Visual Studio 2015 Preview](http://www.visualstudio.com/en-us/downloads/visual-studio-2015-downloads-vs)

    If you are running into an error installing Visual Studio and you are pointed at KB2999226 you may struggle to find the issue. There are a few folks in the comments that have has issues, so I escalated it with the Visual Studio team and there is a fix. You should download and install the [Windows 10 Universal C Runtime](http://www.microsoft.com/en-us/download/details.aspx?id=48234) prior to installing Visual Studio 2015. This will grease the wheels.

    As you will see it is also just plain better and I prefer codding with the latest tools. Git has been updated to the latest version and this in itself is reason to start using 2015 in production. I do however fully expect to have to reset my machine every now and again but starting with Windows 8 this has become so trivial that it does not even need an iso. Indeed I updated to Windows 10 the day it released (on the train home from Scrum training in Cheltenham) and while I have had to do a couple of resets all is well. I have noticed that I can no longer update to 9860 on my Surface 3 so I am on 9841. I am guessing that 9860 was the reason for my recent blue screen issue in the US.

    ![clip_image001](images/clip-image0013-1-1.png "clip_image001")
    { .post-img }

    You can download Visual Studio 2015 Preview with a Web Installer or with the ISO and if you have Windows 8 and up you can mount the ISO by simply double-clicking it. This will mount it like a CD and give you access to the files.

    ![clip_image002](images/clip-image0023-2-2.png "clip_image002")
    { .post-img }

    The installer is fairly strait forward and you have few options. First you get to select where to install the files. I would recommend that you leave this as the default. There is no benefit of installing applications to other drives unless you are installing them to other physical drives for speed. And I don’t know of anyone that has a setup like that in 10 years.

    ![clip_image003](images/clip-image0033-3-3.png "clip_image003")
    { .post-img }

    Not sure why, but by default none of the options are selected and you need to select at least one to move forward. I find that "Select All" is the best option with development tools. I have never regretted installing everything but I have always, without fail, regretted installing things partially. The feature you want is always in the bit that you did not install.

    ![clip_image004](images/clip-image0042-4-4.png "clip_image004")
    { .post-img }

    Microsoft is embracing both open source and cross-platform. Out of the box they are installing much goodness that is not them:

    - Android Native Development Kit (R10, 32)
    - Android SDK (API Level 19)
    - Apache Ant (1.9.3)
    - Git CLI
    - Google Chrome
    - Java SE Development Kit (7.0.550.13)
    - Joynt Node.js
    - Microsoft Visual Studio Emulator for Android
    - SQLLite
    - WebSocket 4Net

    Microsoft has been unhappy with the other Android emulators out there, and if you have used them you will know what I mean. To that end they have provided an emulator based on the fantastic Windows Phone emulator engine. You will still have access to the other emulators installed by the Android SDK but they really are night and day…

    ![clip_image005](images/clip-image0052-5-5.png "clip_image005")
    { .post-img }

    Again I am choosing everything and want to make sure that I am not missing anything.

    #### Note

    If like me you are installing on a plane with no internet you can rerun this part of the install by running "SecondaryInstaller.exe" that you can find in "C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\SecondaryInstaller".

    ![clip_image006](images/clip-image0062-6-6.png "clip_image006")
    { .post-img }

    Along with signing in (I have no interne on this flight) you get to choose your defaults. The development settings are based on what 'most developers' for that platform like and everything can be customised later.

    ![clip_image007](images/clip-image0072-7-7.png "clip_image007")
    { .post-img }

    Awesome. I now have both Visual Studio 2013 and Visual Studio 2015 installed side by side on Windows 10. I don’t expect having any issues as many of the product team also work in this configuration. I think it was about 50/50 at the MVP Summit with product team members on Windows 8.1 or Windows 10. This I think is good as hopefully, even beyond testing, they will run into issues before they get to us. As you can imaging they are very proud of the concurrent compatibility so you can expect to be able to continue to open the solution in 2010, 2013 and 2015 at the same time.

    - [Download Visual Studio 2015 Preview](http://www.visualstudio.com/en-us/downloads/visual-studio-2015-downloads-vs)

    Its presents from Microsoft time. Go on be a kid again and install Visual Studio 2015.
  FilePath: C:\Users\MartinHinshelwoodNKD\source\repos\NKDAgility.com\site\content\resources\blog\2014\2014-11-12-installing-visual-studio-2015-side-side-2013-windows-10\index.md
  FolderPath: C:\Users\MartinHinshelwoodNKD\source\repos\NKDAgility.com\site\content\resources\blog\2014\2014-11-12-installing-visual-studio-2015-side-side-2013-windows-10
- FrontMatter:
    title: Could not find mappings for all states defined in 'Test Suit' work item type
    description: Resolve the 'Could not find mappings for all states' error in TFS 2013.3 with our expert solutions. Ensure smooth test management and avoid case issues!
    ResourceId: tAug7ZHHqTM
    ResourceType: blog
    ResourceContentOrigin: Human
    ResourceImport: true
    ResourceImportId: 10899
    ResourceImportSource: Wordpress
    ResourceImportOriginalSource: Wordpress
    date: 2014-11-11
    weight: 840
    creator: Martin Hinshelwood
    layout: blog
    resourceTypes: blog
    slug: find-mappings-states-defined-test-suit-work-item-type
    aliases:
    - /resources/tAug7ZHHqTM
    - /resources/blog/could-not-find-mappings-for-all-states-defined-in-test-suit-work-item-type
    aliasesArchive:
    - /blog/find-mappings-states-defined-test-suit-work-item-type
    - /find-mappings-states-defined-test-suit-work-item-type
    - /could-not-find-mappings-for-all-states-defined-in-'test-suit'-work-item-type
    - /blog/could-not-find-mappings-for-all-states-defined-in-'test-suit'-work-item-type
    - /resources/blog/find-mappings-states-defined-test-suit-work-item-type
    - /resources/blog/could-not-find-mappings-for-all-states-defined-in-test-suit-work-item-type
    tags:
    - Troubleshooting
    - Install and Configuration
    - Software Development
    - System Configuration
    categories:
    - Uncategorized
    preview: nakedalm-experts-visual-studio-alm-3-3.png
  BodyContent: |
    After upgrading to TFS 2013.3 you get some reports of errors in MTM from your users. Specifically they get a "Could not find mappings for all states defined in 'Test Suit' work item type" error when connecting. I have also seen "TF400860: TestManagement: Version 19, MinVersion: 19" in some cases in the web access.

    ![clip_image001](images/clip-image001-1-1.jpg "clip_image001")
    { .post-img }

    This should only happen if you have customised the Test Suit or Test Plan work items but as they have just been created by the upgrade process it is a little confusing as to why it does not work.

    ![clip_image002](images/clip-image0025-2-2.png "clip_image002")
    { .post-img }

    You might additionally find a similar error when trying to edit a Lab Template as it also queries the Test Data.

    In these cases, if you look real closely at the error message you will see that while "Completed" and "In Planning" look good there is an issue with "In progress". The test team in their infinite wisdom decided that if you have not customised anything that the system defaults to a hard coded set of process configuration for Tests. This sounds reasonable unless you factor in organisations that make silly customisations to their process template.

    In case you have not figured it out yet, the "In process" state should be "In Process". Yes, that’s a case sensitivity issue. When you are uploading work item types this is case-insensitive and is rationalised across work items. So if you have one work item in there with "In process" set and then you add another work item with "In Process" TFS will fix it for you on import. However the test teams hard coded the process configuration which is case sensitive.

    I am pretty sure that the product team will go fix it, but for now you two solutions:

    ### Solution 1: Fix the configuration of your process

    You should be mindful to make sure that you get all of your casing right. In this case I would check all of my work items to make sure that they have the same case and that it is the correct and out-of-the-box "In Process" text.

    ### Solution 2: Add a custom process configuration override

    In your process configuration you need to add a little bit of customisation to continue to use the case you have configured.

    ```
        <TestSuiteWorkItems category="Microsoft.TaskCategory">
          <States>
            <State type="Proposed" value="In planning" />
            <State type="InProgress" value="In progress" />
            <State type="Complete" value="Completed" />
          </States>
        </TestSuiteWorkItems>

    ```

    Once added you should not have an issue.

    ### Conclusion

    These entries are only required when connecting from older clients. From Visual Studio 2013.3 onwards this is a non-issue so it might be your chance to get all of your users to update to the latest and greatest.

    Remember that this issue only affects older clients, and when you have miss typed the casing of the states in your custom process template. Most folks should not run into this and it is a simple fix if you do.
  FilePath: C:\Users\MartinHinshelwoodNKD\source\repos\NKDAgility.com\site\content\resources\blog\2014\2014-11-11-find-mappings-states-defined-test-suit-work-item-type\index.md
  FolderPath: C:\Users\MartinHinshelwoodNKD\source\repos\NKDAgility.com\site\content\resources\blog\2014\2014-11-11-find-mappings-states-defined-test-suit-work-item-type
- FrontMatter:
    title: Use corporate identities with existing VSO accounts
    description: Learn how to configure ADFS for seamless SSO with existing VSO accounts, ensuring continuity and easy access to Azure and Office 365. Get started now!
    ResourceId: yIJgOpFCdJE
    ResourceType: blog
    ResourceContentOrigin: Human
    ResourceImport: true
    ResourceImportId: 10797
    ResourceImportSource: Wordpress
    ResourceImportOriginalSource: Wordpress
    date: 2014-10-28
    weight: 1000
    creator: Martin Hinshelwood
    layout: blog
    resourceTypes: blog
    slug: use-corporate-identities-existing-vso-accounts
    aliases:
    - /resources/yIJgOpFCdJE
    aliasesArchive:
    - /blog/use-corporate-identities-existing-vso-accounts
    - /use-corporate-identities-existing-vso-accounts
    - /use-corporate-identities-with-existing-vso-accounts
    - /blog/use-corporate-identities-with-existing-vso-accounts
    - /resources/blog/use-corporate-identities-existing-vso-accounts
    tags:
    - Install and Configuration
    - System Configuration
    - Windows
    categories:
    - Uncategorized
    preview: nakedalm-experts-visual-studio-alm-11-11.png
  BodyContent: |
    If you configure Active Directory Federated Services (ADFS) you can use corporate identities with existing VSO accounts. Link to your internal domain and you can get a completely seamless Single-Sign-on from your local network to the cloud for Office 365, SharePoint Online, and now Visual Studio Online (VSO).

    I have Office 365 and now that I am starting to use Azure (I have training this week), I am running into the usual authentication issue as I do not have a "single-sign-on" but instead have two. I have a Microsoft ID (martin@nakedalm.com) and an OrgId (martin@nakedalm.com) and I am getting clashes. I get a little more as they both have the same name, but for VSO this is becoming a pain and I need to switch my VSO over to OrgID. This is actually a trivial task, but you want to make sure you have continuity.

    #### Warning

    Your Microsoft ID (was Live ID) mush match your corporate ID to be able to maintain history. Without this VSO will treat you as a new user.

    #### Label

    Not only can you [use multiple email alias with your existing Microsoft ID](http://nkdagility.com/using-multiple-email-alias-existing-microsoft-id/) but you can consolidate accounts. This allows you to associate your work account with your existing identity, and to migrate an existing work account over. It does take 30 days to do the migration though.

    ![clip_image001](images/clip-image0014-1-1.png "clip_image001")
    { .post-img }

    If you log into your Azure with your OrgId you should see your Active Directory configuration listed. This was created by the Office 365 folks if you are on a more small scale service like me, or it may be integrated with your local Active Directory.

    ![clip_image002](images/clip-image0024-2-2.png "clip_image002")
    { .post-img }

    If you select "Active Directory | \[Your Directory\] | Users" you should see a list of all of the users that have been configured for access. You will need to add future users of VSO here and then specifically licence them on VSO. You can however add both your own domain accounts and you can add foreign principals.

    #### Note

    Foreign principals is a feature within the control of your AD administrators

    The foreign principals feature allows you to add accounts from other directories as well as Microsoft ID's to give them access. I have a feeling that this will go a long way to enabling my long requested feature of being able to have single-sign-on across organisations for consultants and contractors. Can you imagine being able to go onsite at a customer and just use your existing Windows Login to access their network. Awesome….

    ![clip_image003](images/clip-image0034-3-3.png "clip_image003")
    { .post-img }

    In this case you should head over to the users tab in Visual Studio Online and make sure that each of the users there has an account created in your AD. There is no simple way to do this, but if you select a user with your mouse and Ctrl+C you get a two row table, with headers, that you can paste into notepad and extract the bits that you want. Namely, name and email address. This really needs an export option.

    <table class="table table-condensed"><tbody><tr><td>FROM</td><td>TO</td><td>EXAMPLE FROM :</td><td><p>EXAMPLE TO:</p></td><td><p>HISTORY PRESERVED</p></td></tr><tr><td><p>Microsoft Account A</p></td><td><p>Microsoft Account Foreign Principal A</p></td><td><p>Windows Live\john@live.com</p></td><td><p>contoso\john@live.com</p></td><td>Yes</td></tr><tr><td><p>Microsoft Account B</p></td><td><p>OrgID B</p></td><td><p>Windows Live\john@contoso.com</p></td><td><p>contoso\john@contoso.com</p></td><td>Yes</td></tr><tr><td>Microsoft Account A</td><td><p>OrgID B</p></td><td><p>Windows Live\john@live.com</p></td><td>contoso\john@contoso.com</td><td>No</td></tr></tbody></table>

    Now, here you may want to pause and give users the [30 days needed to consolidate Microsoft ID's](http://nkdagility.com/using-multiple-email-alias-existing-microsoft-id/) before moving ahead so that they can maintain continuity. For big corporations this could be a pain and you may choose to forgo, but for smaller shops it is a must. Get your users to add their corporate email as an alias to their existing Microsoft ID (yes, the one they use to log into Xbox and Outlook.com). They should then switch the primary email to their corporate one (they can change it back later) so that VSO will update all of the data to their OrgId when you move over.

    #### Note

    This is a one time shot. Here are no do-overs and if folks mess it us they are done. You should see a list of corporate ID's above when done.

    #### Warning

    if Users already have two live ID's and use their personal one to log in it can take 30 days to migrate the corporate email over. There is a 30 days cooling off period after deleting an account before the email can be used again.

    That done you can now go ahead and link your VSO account to your Azure account. You can [reuse your MSDN benefits with your Org ID](http://nkdagility.com/reuse-msdn-benefits-org-id/) even if you have already configured it but you need to cancel the subscription first.

    ## Linking your VSO account to your Azure Account

    Now that you have configured your Active Directory you need to Link your existing VSO account to your Azure account before we can configure anything.

    ![clip_image004](images/clip-image0043-4-4.png "clip_image004")
    { .post-img }

    If you head to the "Visual Studio Online" node in Azure you should see a "Create or Link to a Visual Studio Online Account". If you don’t have one then this is the time to create one. However I already have a VSO account that I created way back when this was TFS Preview. If you select your VSO account from the populated list and make sure you associate it with your MSDN benefits Subscription.

    ![clip_image005](images/clip-image0053-5-5.png "clip_image005")
    { .post-img }

    You should now see your new or existing VSO account listed above. It is now possible to enable unlimited build and to integrate with the Azure Active Directory service. If you don’t see the one you want you can click “Add” at the bottom of the screen and you will be asked to Add or associate.

    ![clip_image006](images/clip-image0063-6-6.png "clip_image006")
    { .post-img }

    Select the account that you want to integrate and go to the "Configure" tab. Here you can connect to the appropriate AAD. At this point you should make sure that you have added all of the accounts that you want to continue to have access. Anyone that is not a foreign principal or an actual AD account will not have access after this. Easy to fix later, but you need to know why they don’t have access.

    ![clip_image007](images/clip-image0072-7-7.png "clip_image007")
    { .post-img }

    I am linking directly into my AAD created for Office 365 and creating a seamless integration across the board. Once you click the tick Microsoft will go off and convert your VSO to an AAD integrated one.

    ![clip_image008](images/clip-image0082-8-8.png "clip_image008")
    { .post-img }

    If you now hit your account you will get automatically signed out of your Microsoft ID (MSA) and presented with the Login Screen.

    ![clip_image009](images/clip-image0092-9-9.png "clip_image009")
    { .post-img }

    Here you should pick your Organisational ID to now login with. You may get a screen saying that it failed on the logout. If this ins the case try just hitting your account again. Sometimes it did work, just MSA getting a little confused. If not go to [http://accounts.live.com](http://accounts.live.com) and actually logout. That should do it…

    ![clip_image010](images/clip-image0102-10-10.png "clip_image010")
    { .post-img }

    VSO now identifies me as my Organisational ID and not my Microsoft ID (MSA).  Now my account has been migrated. Just as it would be if you [run the change identities command](http://nkdagility.com/batched-domain-migration-with-tfs-while-maintaining-identity/) from TFS.

    ## Conclusion

    If you configure Active Directory Federated Services (ADFS) to link to your internal domain you can get a completely seamless Single-Sign-on from your local network to the cloud. While there is a little extra configuration to get true SSO internally this is a big step towards truly unique and trusted identities across all of your platforms.

    Now if only, for Threshold (Windows 10) Microsoft would allow me to join my computers directly to my Azure Active Directory (AAD) domain I will be a happy man.
  FilePath: C:\Users\MartinHinshelwoodNKD\source\repos\NKDAgility.com\site\content\resources\blog\2014\2014-10-28-use-corporate-identities-existing-vso-accounts\index.md
  FolderPath: C:\Users\MartinHinshelwoodNKD\source\repos\NKDAgility.com\site\content\resources\blog\2014\2014-10-28-use-corporate-identities-existing-vso-accounts
- FrontMatter:
    title: 'TFS Build reports Licencies.licx: unable to load type'
    description: Discover solutions for the 'unable to load type' error in TFS build reports. Learn how to streamline your CI builds and enhance your development process.
    ResourceId: om6UWMd_ONd
    ResourceType: blog
    ResourceContentOrigin: Human
    ResourceImport: true
    ResourceImportId: 10730
    ResourceImportSource: Wordpress
    ResourceImportOriginalSource: Wordpress
    date: 2014-10-23
    weight: 660
    creator: Martin Hinshelwood
    layout: blog
    resourceTypes: blog
    slug: tfs-build-reports-licencies-licx-unable-load-type
    aliases:
    - /resources/om6UWMd_ONd
    - /resources/blog/tfs-build-reports-licencies.licx-unable-to-load-type
    aliasesArchive:
    - /blog/tfs-build-reports-licencies-licx-unable-load-type
    - /tfs-build-reports-licencies-licx-unable-load-type
    - /tfs-build-reports-licencies-licx--unable-to-load-type
    - /blog/tfs-build-reports-licencies-licx--unable-to-load-type
    - /resources/blog/tfs-build-reports-licencies-licx-unable-load-type
    - /resources/blog/tfs-build-reports-licencies.licx-unable-to-load-type
    tags:
    - Software Development
    - Troubleshooting
    - Pragmatic Thinking
    categories:
    - Uncategorized
    preview: nakedalm-experts-visual-studio-alm-3-3.png
  BodyContent: |
    When you do a build you can get an "unable to resolve type" message from the licencies.licx file that you have checked in. This prevents you from having a successful build in Team Build.

    I have been working with a team of coders in Athens moving them over from StarTeam to TFS with Git. We are now, for the first time ever, trying to configure a build for their 25k file project on an automated build server. The main solution has 66 projects and currently we need to do two passes to get a successful build locally. This is not the best place to be, but it is where we are. There are many things that they are going to have to do to improve their platform over the coming years, but they really need a CI build to help them stay on the right road.

    I configured a build server and got one of their coders to install the 10GB of additional components that we need to get the build working. They have Infragistics and Developer Express, along with a plethora of other tools and components for both WPF, WinForms, and Web. After a few builds we got to a seemingly confusing error. There was a licence.licx file referencing version 12.2 of Infragistics that was causing the build server to throw an error. But it built just fine on the local developer workstations.

    ![clip_image001](images/clip-image0012-1-1.png "clip_image001")
    { .post-img }

    The error looks fairly strait forward but the referenced components were installed on the server. Because they use a lot of components they have a lovely spreadsheet with all of the versions and what component the developers need with a link to the network share where the installers live. It should be a case of walking the list, installing everything, and we are good to go.

    My Project\\licenses.licx (1): Unable to resolve type 'Infragistics.Win.UltraWinGrid.DocumentExport.UltraGridDocumentExporter, Infragistics4.Win.UltraWinGrid.DocumentExport.v12.2, Version=12.2.20122.1006, Culture=neutral, PublicKeyToken=7dd5c3163f2cd0cb'

    My Project\\licenses.licx (5): Unable to resolve type 'Infragistics.Win.UltraWinGrid.ExcelExport.UltraGridExcelExporter, Infragistics4.Win.UltraWinGrid.ExcelExport.v12.2, Version=12.2.20122.1006, Culture=neutral, PublicKeyToken=7dd5c3163f2cd0cb'

    Now if any of you have tried to get a build working for an existing piece of software, you will know that it is an arduous task of building and fixing that makes your hair gray. A build server tends to be a lot more… unforgiving than local Visual Studio. Not only that there are always hundreds of ' the coders just know how to solve that' errors that you will have to resolve.

    I did find one error where a DLL was missing. The coders, it turns out, all knew to get that specific version of the DLL from an attachment to an email in their inbox and put it in a specific location - Solved that one with ProGet and a NuGet package!

    In this case after spending some time scratching my head I noticed that the build numbers in the version did not match. The build numbers in the licence file matched the error, but not the version of Infragistics that was installed. Doh… but I thought that there was a handy dandy spreadsheet?

    ![clip_image002](images/clip-image0022-2-2.png "clip_image002")
    { .post-img }

    At some point a newer version of 2012.2 was downloaded and dropped onto the network share, but since the developers all likely had the previous version installed as well as the new one, the licence.licx file would resolve for them locally. Since the build server was new, any new developer would have the same problem, this issue reared its ugly head. This is a frustrating problem as it means that something is not getting build correctly for everyone that is just slipping on by.

    If only it was easy to reimage developer workstations overnight so that they only had the current versions of all the components. These problems would be found quickly and fixed often.

    Moral of the story… always either reimage your workstation often, or uninstall components you don’t need any more. Ideally setup and configure an automated build now if you don’t have one. If its hard then suck it up and take the time to get it working.
  FilePath: C:\Users\MartinHinshelwoodNKD\source\repos\NKDAgility.com\site\content\resources\blog\2014\2014-10-23-tfs-build-reports-licencies-licx-unable-load-type\index.md
  FolderPath: C:\Users\MartinHinshelwoodNKD\source\repos\NKDAgility.com\site\content\resources\blog\2014\2014-10-23-tfs-build-reports-licencies-licx-unable-load-type
- FrontMatter:
    title: Upcomming Scrum at Scale Workshop from Scrum.org
    description: Join the Scrum at Scale Workshop by Scrum.org to drive agile transformation in your organization. Learn to implement effective change and improve culture!
    ResourceId: eSPePG8uDaZ
    ResourceType: blog
    ResourceContentOrigin: Human
    ResourceImport: true
    ResourceImportId: 10824
    ResourceImportSource: Wordpress
    ResourceImportOriginalSource: Wordpress
    date: 2014-10-22
    weight: 750
    creator: Martin Hinshelwood
    layout: blog
    resourceTypes: blog
    slug: announcing-scrum-at-scale-workshop-scrum-org
    aliases:
    - /resources/eSPePG8uDaZ
    - /resources/blog/upcomming-scrum-at-scale-workshop-from-scrum.org
    aliasesArchive:
    - /blog/announcing-scrum-at-scale-workshop-scrum-org
    - /announcing-scrum-at-scale-workshop-scrum-org
    - /upcomming-scrum-at-scale-workshop-from-scrum-org
    - /blog/upcomming-scrum-at-scale-workshop-from-scrum-org
    - /resources/blog/announcing-scrum-at-scale-workshop-scrum-org
    - /resources/blog/upcomming-scrum-at-scale-workshop-from-scrum.org
    tags:
    - Agile Frameworks
    - Agile Transformation
    - Software Development
    categories:
    - Scrum
    preview: nakedalm-experts-professional-scrum-2-2.png
  BodyContent: |
    If you are scaling agility in the enterprise through scrum then you will understand when I say that this can be a very difficult problem. And it's mostly about people and about culture and the [Scrum at Scale Workshop](http://nkdagility.com/training/courses/scrum-at-scale-workshop/) can help you.

    Scrum.org have long since recognised that many efforts to implement agile in the enterprise have failed or not resulted in the benefits that people were hoping for. This is almost universally due to a lack of commitment to change. And this in turn is the result of your current organisational culture that is finely tuned to produce the current situation. To move forward you need to change that culture.

    Scrum.org first introduced a Scrum at Scale model called [Evidence-based Management for Software Organisations](http://nkdagility.com/experts/evidence-based-management-for-software-organisations/). This was an embodiment of the empirical techniques inherent in Scrum as applied to organisational change and especially in enterprise. This gave you a way to incrementally change your organisation for the better while maintaining a sense of urgency. As part of this framework for organisational change a tool to help you monitor that change was also introduced. This calculated an [Agility Index from key metrics](http://nkdagility.com/evidence-based-management-gathering-metrics/) that gave you an idea of where you currently are and allowed you to continuously measure and improve.

    Knowing where you are and how much you have improved is good, and we also need to know where to go. Agility Index also included a [practices assessment that assessed the maturity of your organisation](http://nkdagility.com/metrics-that-matter-with-evidence-based-management/). This assessment is based on what we know works within organisations and is graded as all practices, or depth of practice, do not provide value of all companies.

    These concepts are not new and [my father was using these techniques in business 30 years ago](http://nkdagility.com/what-my-father-taught-me-about-agility-path-34-years-before-it-was-invented/). While not new, the application of these techniques to adapt your organisation based on metrics and best practices are not easy to implement.

    This is why Scrum.org (Ken Schwaber) & Scrum Inc. (Jeff Sutherland) have joined forces to provide the [Scrum at Scale Workshop](http://nkdagility.com/training/courses/scrum-at-scale-workshop/) for those people who are responsible for change within their organisation. If you are leading the change within your organisation you could be a development manager, or coach, or just the someone with a vested interest in success.

    ![Scrum at Scale Workshop ](images/enterpriseandscrum-light-150x150-1-1.png)The [Scrum at Scale Workshop](http://nkdagility.com/training/courses/scrum-at-scale-workshop/) provides a baseline within which you can begin to implement that organisational change without floundering as others have. While the Professional Scrum Foundations is the first thing you need to level set your teams before you commence with a Scrum implementation, the [Scrum at Scale Workshop](http://nkdagility.com/training/courses/scrum-at-scale-workshop/) is what you need to get started at the organisational level.
    { .post-img }

    The first two ever public courses for Scrum at Scale is available: [https://www.scrum.org/courses/scrum-at-scale](https://www.scrum.org/courses/scrum-at-scale)
  FilePath: C:\Users\MartinHinshelwoodNKD\source\repos\NKDAgility.com\site\content\resources\blog\2014\2014-10-22-announcing-scrum-at-scale-workshop-scrum-org\index.md
  FolderPath: C:\Users\MartinHinshelwoodNKD\source\repos\NKDAgility.com\site\content\resources\blog\2014\2014-10-22-announcing-scrum-at-scale-workshop-scrum-org
- FrontMatter:
    title: Reuse your MSDN benefits with your Org ID
    description: Learn how to transfer your MSDN benefits to your Org ID effortlessly. Follow our step-by-step guide to maximize your Azure subscription today!
    ResourceId: kBoaPAZssaV
    ResourceType: blog
    ResourceContentOrigin: Human
    ResourceImport: true
    ResourceImportId: 10786
    ResourceImportSource: Wordpress
    ResourceImportOriginalSource: Wordpress
    date: 2014-10-21
    weight: 1000
    creator: Martin Hinshelwood
    layout: blog
    resourceTypes: blog
    slug: reuse-msdn-benefits-org-id
    aliases:
    - /resources/kBoaPAZssaV
    aliasesArchive:
    - /blog/reuse-msdn-benefits-org-id
    - /reuse-msdn-benefits-org-id
    - /reuse-your-msdn-benefits-with-your-org-id
    - /blog/reuse-your-msdn-benefits-with-your-org-id
    - /resources/blog/reuse-msdn-benefits-org-id
    tags: []
    categories:
    - Uncategorized
    preview: nakedalm-experts-visual-studio-alm-7-7.png
  BodyContent: |
    If you are trying to reuse your MSDN benefits with your Org ID but you previously activated against your Microsoft ID then you will need to cancel your subscription to reuse it.

    If, like me, you are moving all of your Microsoft ID (was Live ID) corporate stuff over to Organisational Id (Office 365) and you have previously activated your MSDN benefits against your Microsoft ID you have a bit of work to move those benefits over.

    ![clip_image001](images/clip-image0013-1-1.png "clip_image001")
    { .post-img }

    In [https://msdn.microsoft.com/subscriptions](https://msdn.microsoft.com/subscriptions) you might see your MSDN linked to your Organisational Account or your Microsoft Account. Either way, the only thing that matters is where you see the benefits when you log in. In my case it was on my Microsoft ID. To swap it over we need to remove all of our subscriptions.

    ![clip_image002](images/clip-image0023-2-2.png "clip_image002")
    { .post-img }

    Using an 'InPrivate' browser tab head over to [http://account.windowsazure.com](http://account.windowsazure.com) and cancel all of your MSDN benefit your subscriptions.

    ![clip_image003](images/clip-image0033-3-3.png "clip_image003")
    { .post-img }

    Once there you can select the subscription that came from your MSDN benefits, in my case I had three old subscriptions as well. It only takes a few seconds to cancel, but be warned. You will lose all access to anything you created in there.

    ![clip_image004](images/clip-image0042-4-4.png "clip_image004")
    { .post-img }

    You should get your cancelation email really quickly… mine came in almost as quickly as I clicked the button.

    ![clip_image005](images/clip-image0052-5-5.png "clip_image005")
    { .post-img }

    Head back to [https://msdn.microsoft.com/subscriptions](https://msdn.microsoft.com/subscriptions) and select "Activate Microsoft Azure" and login with your Organisational Account. If you already have an active subscription it will say that you can't activate your trial. But below you should see a "What can I do?" question and below that an option to use your MSDN allotment.

    ![clip_image006](images/clip-image0062-6-6.png "clip_image006")
    { .post-img }

    Now on my organisational account I have a nice $150 per month subscription right where I can use it. Single-sign-on and no faffing around with InPrivate or having to log-out all the time.
  FilePath: C:\Users\MartinHinshelwoodNKD\source\repos\NKDAgility.com\site\content\resources\blog\2014\2014-10-21-reuse-msdn-benefits-org-id\index.md
  FolderPath: C:\Users\MartinHinshelwoodNKD\source\repos\NKDAgility.com\site\content\resources\blog\2014\2014-10-21-reuse-msdn-benefits-org-id
- FrontMatter:
    title: Uncommitted changes messing up your sync in Git with Visual Studio
    description: Learn how uncommitted changes in Git can disrupt your sync in Visual Studio. Discover effective strategies to manage your workflow and enhance productivity.
    ResourceId: CuWpL0GPBwN
    ResourceType: blog
    ResourceContentOrigin: Human
    ResourceImport: true
    ResourceImportId: 10732
    ResourceImportSource: Wordpress
    ResourceImportOriginalSource: Wordpress
    date: 2014-10-16
    weight: 840
    creator: Martin Hinshelwood
    layout: blog
    resourceTypes: blog
    slug: uncommitted-changes-messing-sync-git-visual-studio
    aliases:
    - /resources/CuWpL0GPBwN
    aliasesArchive:
    - /blog/uncommitted-changes-messing-sync-git-visual-studio
    - /uncommitted-changes-messing-sync-git-visual-studio
    - /uncommitted-changes-messing-up-your-sync-in-git-with-visual-studio
    - /blog/uncommitted-changes-messing-up-your-sync-in-git-with-visual-studio
    - /resources/blog/uncommitted-changes-messing-sync-git-visual-studio
    tags:
    - Software Development
    categories:
    - Uncategorized
    preview: naked-alm-git-2-2.png
  BodyContent: |
    Sometimes when you try to Pull changes from the server to your local repository you get a "cannot pull because there are uncommitted changes". This is uncommitted changes messing up your sync

    I have been onsite in Athens working with a customer that is moving from StarTeam to Git. When you are moving from one source control system to another there are always difficulties and a difference in workflow between the systems. A workflow that works well in StarTeam might not in TFVC. However when you add that you are moving from a Server version Control System (SVCS) to a Distributed Version Control System (DVCS) you are going to go through a paradigm shift.

    However, it can be the simplest of activities that bite you in the ass. For example, in most server based version control systems when you have made a bunch of changes locally and you want check-in to the server it is always prudent to do a "Get" first in order to verify that your local changes work with the latest version of the server. So you dutifully just do a "Get" and if your files locally conflict with the server you resolve the conflicts using the conflict resolution tools.

    ![clip_image001](images/clip-image001-1-1.jpg "clip_image001")
    { .post-img }

    When you try to do this in Git it complains that you have uncommitted changes that you either need to undo the changes and get rid of them, or you can commit them to the local repository.

    In our server based scenario we have no choice but to do a merge from the server directly to our live edited files locally. If we complete the merge, and we messed up…. Well that’s just a little bit tough and you should have chosen better. So we have a lossey (or lousey) merge where we can potentially not end up with what we want and have no way back.

    Git blocks this potential loss of code by forcing you to choose wither you want to lose the changes or persist them. Once they are persisted they can't be lost without deliberately resetting the repository or deleting it. Although your workflow is changing it is for the better as you are less likely to have a frustrating issue.

    DVCS is just better than SVCS…
  FilePath: C:\Users\MartinHinshelwoodNKD\source\repos\NKDAgility.com\site\content\resources\blog\2014\2014-10-16-uncommitted-changes-messing-sync-git-visual-studio\index.md
  FolderPath: C:\Users\MartinHinshelwoodNKD\source\repos\NKDAgility.com\site\content\resources\blog\2014\2014-10-16-uncommitted-changes-messing-sync-git-visual-studio
- FrontMatter:
    title: 'NDC London: Second Look, Team Foundation Server &amp; VSO'
    description: Join Martin Hinshelwood at NDC London for a deep dive into Team Foundation Server and VSO, showcasing powerful ALM features and live demos. Don't miss out!
    ResourceId: bI6ySrK2G51
    ResourceType: blog
    ResourceContentOrigin: Human
    ResourceImport: true
    ResourceImportId: 10811
    ResourceImportSource: Wordpress
    ResourceImportOriginalSource: Wordpress
    date: 2014-10-15
    weight: 410
    creator: Martin Hinshelwood
    layout: blog
    resourceTypes: blog
    slug: ndc-london-second-look-team-foundation-server-vso
    aliases:
    - /resources/bI6ySrK2G51
    - /resources/blog/ndc-london-second-look-team-foundation-server-amp-vso
    aliasesArchive:
    - /blog/ndc-london-second-look-team-foundation-server-vso
    - /ndc-london-second-look-team-foundation-server-vso
    - /ndc-london--second-look,-team-foundation-server-&amp;-vso
    - /blog/ndc-london--second-look,-team-foundation-server-&amp;-vso
    - /ndc-london--second-look--team-foundation-server-&amp;-vso
    - /blog/ndc-london--second-look--team-foundation-server-&amp;-vso
    - /resources/blog/ndc-london-second-look-team-foundation-server-vso
    - /resources/blog/ndc-london-second-look-team-foundation-server-amp-vso
    tags:
    - Application Lifecycle Management
    - Software Development
    - Release Management
    - Product Delivery
    categories:
    - Uncategorized
    preview: metro-event-icon-7-7.png
  BodyContent: |
    While I have spoken at many events in the USA while I lived there, and even did a few keynotes for the Visual Studio 2012 launch, I have been trying to figure out the scene here in Europe. As such I submitted to a few events and got accepted to speak at NDC London.

    [![martin-hinshelwood-ndc-london-2014-tfs-vso](images/martin-hinshelwood-ndc-london-2014-tfs-vso-800x450-5-6.png)](http://www.ndcvideos.com/#/app/video/2641)
    { .post-img }

    My session, Second Look, Team Foundation Server & VSO, will be aimed at those folks that have previously tried TFS and found it lacking. Most of those folks previously used a version of TFS prior to TFS 2012 where things started to get really interesting. Indeed if you are building an application using the Microsoft stack there is no better ALM platform.

    ![1030_image_thumb_0AF311DD](images/1030-image-thumb-0AF311DD-1-1.png "1030_image_thumb_0AF311DD")
    { .post-img }

    The main reason that IBM scores a little higher on completeness is that they have better support for other platforms but at a loss of features for any specific one. While Visual Studio ALM has good support for any platform, the support for the Microsoft stack is second to none.

    With the time constraint and the amount of things I want to show my session will need to be demo heavy. The type of person that I am gearing this session towards are the hard core who tried TFS prior to 2012 and don’t believe the marketing. Do demos it is… but I just looked back at what I submitted:

    > You may have looked at Team Foundation Server before and if it was before 2012 then you should have another look. It is not the same product it used to be. Come and see Martin do an end to end walk through from Ideation through Coding, Testing and Release with monitoring and feedback. Martin will cover some of the new advances with Storyboarding, Agile Project Management, and Agile Portfolio Management. He will then delve into the new ALM features added since 2010 for coders like My Work, Code Sense, and Local Workspaces and even Git. With the new Test Management tools in the web complimented with Microsoft Test Manager your testers can easily manage, execute and report on your test plans. All the while we will be using the new Release Management tools to push our application to each environment and ultimately to production. Once there we can monitor our application for usage and performance with rich statistics.
    >
    > All in all TFS is a world premier ALM solution that provides everything that you need to manage the Application Lifecycle of your application.

    Oh my… look what I signed myself up to!

    Wana see this session? Sign up for a ticket at: [http://ndc-london.com](http://ndc-london.com)

    So it's going to have to be 45+ minutes Demo, and I have two options. I can do everything in my local demo box, and that will be the backup scenario, or I can go Azure crazy. Between now and NDC London I will be blogging on setting up and configuring continuous everything with VSO and Azure. What do I mean by continuous everything?

    [![clip_image002](images/clip-image002-thumb-3-3.png "clip_image002")](http://nkdagility.com/wp-content/uploads/2014/10/clip-image0025-4-4.png)
    { .post-img }

    Well, I want the full lifecycle with Azure Active Directory integration driving authentication and collaboration with Azure, Visual Studio Online, and Office 365. This would be a huge demo if I stopped to explain all the features along the way, so I am not going to. The audience at NDC is very smart and this is going to be a level 400 high-speed walkthrough of the core features added to TFS since 2012.

    There will however need to be trade-offs so I am looking for your help to see what features to spend the most time on and what to just mention in passing. Are you going to NDC?

    [Feedback Request - What Features do you most want to see?](https://www.surveymonkey.com/r/C2FCM79)

    I am not yet sure if I will be using green field or brownfield as each have their pros and cons. In my flying time deliberations I have been contemplating three main scenarios:

    1. Greenfield  
       Start with an empty team project and build everything up from scratch. That would mean getting the code in, creating a backlog, writing some code, followed by some testing and then an automated build. I would then get a few minutes while the build executes to create a release management pipeline and push to the environments.
    2. Greenfield TFS / brownfield project  
       Again, start with an empty team project but import from somewhere else. Maybe pull in a Github project and do the same as above.
    3. Brownfield  
       Have an existing end to end setup and just walk through adding a feature or fixing a bug and the interactions involved.

    I guess it depends how long my session is with brownfield being the easiest to pull off. A plan then would be to get brownfield working and then, if there is time, look into the other options. So let's see what the scenarios are that I plan on tackling:

    ### Brownfield Scenario 1 - The new Feature

    In this scenario we have a new feature and we are going to implement a single PBI to do with this feature. We need to have a Storyboard to go with the PBI for coder context and Test Cases prior to commencing the work. We then make two passes, the first with build and deploy of the new code. The second with automation of the now passing test case.

    1. Create new Feature in TFS
    2. Create Storyboard to show feature
    3. Create PBI's to reflect feature
    4. Create Test Cases for one PBI
    5. Code till test cases pass using TDD
    6. Push to Repo
    7. Build & Test with Team Build
    8. Deploy with Release Manager to Feedback01
    9. Coder Validation
    10. Deploy with Release Manager to Feedback02
    11. Tester Validation & Recording
    12. Coder Automates Test Case
    13. Deploy with Release Manager to Feedback01
    14. Coder Validation of Automation
    15. Deploy with Release Manager to Feedback02
    16. Tester review of Results
    17. Deploy with Release Manager to Feedback03
    18. Product Owner Validation
    19. Review of Application Analytics usage data

    ### Brownfield Scenario 2 - The Bug

    In this scenario we have a user who, is the process of providing feedback, finds an issue. The Product Owner gets this bug verified by a Tester and a relevant test case created to prove that it exists. This is then prioritized and enters the current sprint, maybe with something dropping out the bottom. The coder then fixes it and the tester verifies it before automating the result to prevent regression.

    1. User Gets feedback request and actions
    2. User Finds and reports bug as part of feedback
    3. Product Owner breaks feedback down into PBI's
    4. Product Owner reviews feature usage stats and notifies tester of possible important bug
    5. Tester validates existence of bug and creates rich bug and Test Case
    6. Bug prioritized and added to current sprint
    7. Code till test cases pass using TDD
    8. Push to Repo
    9. Build & Test with Team Build
    10. Deploy with Release Manager to Feedback01
    11. Coder Validation
    12. Deploy with Release Manager to Feedback02
    13. Tester Validation & Recording
    14. Coder Automates Test Case
    15. Deploy with Release Manager to Feedback01
    16. Coder Validation of Automation
    17. Deploy with Release Manager to Feedback02
    18. Tester review of Results
    19. Deploy with Release Manager to Feedback03
    20. Product Owner Validation and emails User

    ### Scenario Choice

    As well as understanding what features you, as the audience on the day, want to hit I also want to know which scenario is more interesting.

    [Feedback Request - Which scenario looks most desirable?](https://www.surveymonkey.com/r/CCN7ZR9)

    ## Conclusion

    I am really looking forward to this session as it will give me a chance to directly target nay-sayers that are not really aware of the capabilities of the product. If you are building for .NET then there is no better platform.

    Please provide me with some feedback on the polls above. I am very interested in focusing on what will solve the most problems for attendees. I will also be around for the full 3 days and would be happy to do add-hock demos and problem solving sessions… Unless there is a supper interesting session on the go I would be happy to provide free TFS consulting for any and all attendees of NDC London on the days.

    If you are on a tight schedule I would be happy to have you pre-book some time. Email info@nakedalm.com to get some free TFS & VSO consulting at NDC London.
  FilePath: C:\Users\MartinHinshelwoodNKD\source\repos\NKDAgility.com\site\content\resources\blog\2014\2014-10-15-ndc-london-second-look-team-foundation-server-vso\index.md
  FolderPath: C:\Users\MartinHinshelwoodNKD\source\repos\NKDAgility.com\site\content\resources\blog\2014\2014-10-15-ndc-london-second-look-team-foundation-server-vso
- FrontMatter:
    title: Move an Azure storage blob to another store
    description: Learn how to efficiently move Azure storage blobs between accounts using PowerShell. Simplify your VHD management with this step-by-step guide!
    ResourceId: b5bHXT2rLoN
    ResourceType: blog
    ResourceContentOrigin: Human
    ResourceImport: true
    ResourceImportId: 10778
    ResourceImportSource: Wordpress
    ResourceImportOriginalSource: Wordpress
    date: 2014-10-14
    weight: 1000
    creator: Martin Hinshelwood
    layout: blog
    resourceTypes: blog
    slug: move-azure-storage-blob-another-store
    aliases:
    - /resources/b5bHXT2rLoN
    aliasesArchive:
    - /blog/move-azure-storage-blob-another-store
    - /move-azure-storage-blob-another-store
    - /move-an-azure-storage-blob-to-another-store
    - /blog/move-an-azure-storage-blob-to-another-store
    - /resources/blog/move-azure-storage-blob-another-store
    tags: []
    categories:
    - Uncategorized
    preview: nakedalm-windows-logo-4-4.png
  BodyContent: |
    Move an Azure storage blob to another store took a little bit longer than I thought that it would. All I wanted to do was move a VHD from one storage account to another. However this is a little more complicated than it seems on the surface.

    I am working on teaching the [Managing Projects with Microsoft Visual Studio Team Foundation Server 2013](http://nkdagility.com/training/courses/managing-projects-microsoft-visual-studio-team-foundation-server-2013/) course in Cheltenham this week and have been [creating training virtual machines in Azure](http://nkdagility.com/creating-training-virtual-machines-azure/). My template is 80GB and it is quite an arduous task to upload it. I now want to move it to a new, less temporary, home.

    ![clip_image001](images/clip-image0012-1-1.png "clip_image001")
    { .post-img }

    I want to move my training VM from the "trainingeu" storage account to the "almtrainingvm" one. This is really just a refactor now that I have everything working and have thought about a new home. The copy process however is a little bit convoluted, especially as both containers are marked as private.

    What I really want to be able to do is just call "Copy-AzureStorageBlob -source [https://trainingeu.blob.core.windows.net/vhds/bkvm-2013-3.vhd](https://trainingeu.blob.core.windows.net/vhds/bkvm-2013-3.vhd) -destination [https://almtrainingvm.blob.core.windows.net/vhds/bkvm-2013-3.vhd](https://almtrainingvm.blob.core.windows.net/vhds/bkvm-2013-3.vhd)" and be done with it. But alas… this is not to be as we need to authenticate to both storage accounts separately even though we are authenticated against the main account.

    So… we need a little more PowerShell than I wanted:

    ```
    Select-AzureSubscription "Pay-As-You-Go"

    ### Source VHD

    $sourceUri = "https://trainingeu.blob.core.windows.net/vhds/bkvm-2013-3.vhd"

    $sourceStorageAccount = "trainingeu"

    $sourceStorageKey = "bla"

    $sourceContext = New-AzureStorageContext –StorageAccountName $srcStorageAccount `

    -StorageAccountKey $srcStorageKey

    ### Target VHD

    $targetName = "bkvm-2013-3.vhd"

    $targetStorageAccount = "almtrainingvm"

    $targetStorageKey = "bla"

    $targetContainerName = "vhds"

    $targetContext = New-AzureStorageContext –StorageAccountName $targetStorageAccount `

    -StorageAccountKey $targetStorageKey

    $blob1 = Start-AzureStorageBlobCopy -srcUri $sourceUri -SrcContext $sourceContext -DestContainer $targetContainerName -DestBlob $targetName -DestContext $targetContext

    ```

    ![clip_image002](images/clip-image0022-2-2.png "clip_image002")
    { .post-img }

    Why we can't do this with URL's and an authenticated account I do not know… but this is what we got and we have to roll with it.

    ![clip_image003](images/clip-image0032-3-3.png "clip_image003")
    { .post-img }

    Now that I have my VHD over here I can change my default store and create my Virtual Machines from this VHD instead of the other one. Not the easiest task, but now I have some lovely PowerShell I should be able to move VHD's between Azure Storage Accounts any time I like.
  FilePath: C:\Users\MartinHinshelwoodNKD\source\repos\NKDAgility.com\site\content\resources\blog\2014\2014-10-14-move-azure-storage-blob-another-store\index.md
  FolderPath: C:\Users\MartinHinshelwoodNKD\source\repos\NKDAgility.com\site\content\resources\blog\2014\2014-10-14-move-azure-storage-blob-another-store
- FrontMatter:
    title: Bug in the Visual Studio Git integration that results in a merge conflict
    description: Discover how to resolve merge conflicts in Visual Studio's Git integration. Learn effective workflows to streamline your development process and enhance collaboration.
    ResourceId: GSH94xtzGy6
    ResourceType: blog
    ResourceContentOrigin: Human
    ResourceImport: true
    ResourceImportId: 10734
    ResourceImportSource: Wordpress
    ResourceImportOriginalSource: Wordpress
    date: 2014-10-09
    weight: 690
    creator: Martin Hinshelwood
    layout: blog
    resourceTypes: blog
    slug: bug-visual-studio-git-integration-results-merge-conflict
    aliases:
    - /resources/GSH94xtzGy6
    aliasesArchive:
    - /blog/bug-visual-studio-git-integration-results-merge-conflict
    - /bug-visual-studio-git-integration-results-merge-conflict
    - /bug-in-the-visual-studio-git-integration-that-results-in-a-merge-conflict
    - /blog/bug-in-the-visual-studio-git-integration-that-results-in-a-merge-conflict
    - /resources/blog/bug-visual-studio-git-integration-results-merge-conflict
    tags:
    - Troubleshooting
    - Software Development
    - Modern Source Control
    categories:
    - Uncategorized
    preview: naked-alm-git-2-2.png
  BodyContent: |
    When doing a PULL from Git from an origin\\branch but you have changes locally you may encounter a bug in the Visual Studio Git integration that results in a merge conflict fetching the head of the remote Git repository.

    <script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>

    If like me you are new to Git there are a lot of things to learn. This last two weeks I have been onsite in Athens moving source from StarTeam to Git in TFS. As we have just started getting users onto Git we have started hitting workflow issues. In StarTeam they were not really using branching and were doing manual merges. This was for many reasons all of them seemingly perfectly reasonable but it takes up a lot of time. With nearly 50 coders it was well past being manageable and they are currently looking at 100 man-days to do a merge.

    I am trying to move them to a much more supportable model and for that I am moving them towards Git. I know this would be a steep learning curve for them but I believe it is the only reasonably way to support the issues that they have in their software strategy.

    More on that later, but for now I have been running into an issue with merge conflicts in Git when coders try to Pull from the server. It seems that whatever workflow they are used to when they follow it in Git they get a "libgit2.Catagory = FetchHead (MergeConflict)" message that adds that there are 3 conflicts that prevent a checkout. While my first thought is that they have uncommitted changes locally this is not the case. They maybe have 2-3 local commits and no outstanding uncommitted changes.

    ![clip_image001](images/clip-image0013-1-1.png "clip_image001")
    { .post-img }

    While this should be able to be handled it is not and I have 50 coders hitting it reputedly. The only way I have found so far to resolve the issue is to move their changes onto a temporary branch and rollback the local copy of the server branch. This allows them to do a Pull and then merge their local branch changes across.

    https://twitter.com/ethomson/status/520262367616053248

    Note: This is a bug in Visual Studio 2013.3 that has been fixed in 2013.4. You can download and install, as we did, the 2013.4 CTP to resolve the issue.

    While not ideal it does work. So in order to mitigate this issue permanently, pending an update from the TFS team, I am changing their workflow. When planning on working on an origin\\branch all developers will:

    1. Create a new "feature-\[name\]" or "hotfix-\[name\]" branch locally.
    2. They make all changes on this branch
    3. Pull regularly from origin\\branch and merge to FEATURE or HOTFIX branch

    When they are ready to Push to the server they then:

    1. Pull from origin\\branch to bring it up to date
    2. Merge into "feature-\[name\]" and get working
    3. Merge from "feature-\[name\]" to branch
    4. Push to origin\\branch
    5. DONE

    If they are then done with the feature of hotfix they can delete it, if not they can keep it around for reuse. Simple…and it works for them every time. However sometimes they forget to do the change on the feature or hotfix branch and end up in the position above.

    It depends when they figure out what they have done what the solution is. If they have not committed to the branch then they can solve this in a few clicks.

    1. Create a new "feature-\[name\]" or "hotfix-\[name\]" unpublished branch
    2. Commit to the new unpublished branch

    One of the nice features of Git is that I you made a bunch of changes and have not checked in you can just "checkout" another branch and the local changes you have made will be preserved. Simple quick fix.

    If they only figure it out after they have committed one or more times to the branch then they have a few extra steps to resolve the committed bits on the published branch.

    1. Create a new "feature-\[name\]" or "hotfix-\[name\]" unpublished branch - This will take a copy of the commits that have not yet been pushed to the server. This preserving the changes they have already made.
       1. Checkout the branch you want to rollback
       2. Use "git reset --hard HEAD~\[n\]" where \[n\] is the number of commits you want to back peddle
    2. Pull from origin\\branch to bring it up to date

    After that they can happily Pull to the published branch and continue to code away on the unpublished local branch. Yes this means that every developer effectively has one or more (they may have more than one set of work on the go) personal branches. While this was a bad practice in a Server Version Control System (SVCS) it is a perfectly good practice for a Distributed Version Control System (DVCS) where merging and branching is cheap and easy.

    If you can you should install the Visual Studio 2013.4 CTP that fixes this issue and you can carry on as normal.
  FilePath: C:\Users\MartinHinshelwoodNKD\source\repos\NKDAgility.com\site\content\resources\blog\2014\2014-10-09-bug-visual-studio-git-integration-results-merge-conflict\index.md
  FolderPath: C:\Users\MartinHinshelwoodNKD\source\repos\NKDAgility.com\site\content\resources\blog\2014\2014-10-09-bug-visual-studio-git-integration-results-merge-conflict
- FrontMatter:
    title: Bruce Lee on Scrum and Agile
    description: Explore Bruce Lee's philosophy on Scrum and Agile, emphasizing simplicity and adaptability to enhance your organization's processes and customer satisfaction.
    ResourceId: mDZ9tLsZEI-
    ResourceType: blog
    ResourceContentOrigin: Human
    ResourceImport: true
    ResourceImportId: 10800
    ResourceImportSource: Wordpress
    ResourceImportOriginalSource: Wordpress
    date: 2014-10-07
    weight: 840
    creator: Martin Hinshelwood
    layout: blog
    resourceTypes: blog
    slug: bruce-lee-on-scrum-and-agile
    aliases:
    - /resources/mDZ9tLsZEI-
    aliasesArchive:
    - /blog/bruce-lee-on-scrum-and-agile
    - /bruce-lee-on-scrum-and-agile
    - /resources/blog/bruce-lee-on-scrum-and-agile
    tags:
    - Agile Philosophy
    categories:
    - Uncategorized
    preview: nakedalm-experts-professional-scrum-2-2.png
  BodyContent: |
    There are wise people in this world and that wisdom often transcends the topic that they are intentionality addressing. Good philosophy can be applied beyond those bounds.

    ![bruce-lee-enterprises-3](images/bruce-lee-enterprises-3-1-1.jpg "bruce-lee-enterprises-3")As an example, here is Bruce Lee on Scrum and Agile:
    { .post-img }

    > I have not invented a "new style," composite, modified or otherwise that is set within distinct form as apart from "this" method or "that" method. On the contrary, I hope to free my followers from clinging to styles, patterns, or moulds. Remember that Scrum is merely a name used, a mirror in which to see "ourselves". . . Scrum is not an organized institution that one can be a member of. Either you understand or you don't, and that is that.
    >
    > There is no mystery about my style. My movements are simple, direct and non-classical. The extraordinary part of it lies in its simplicity. Every movement agile is being so of itself. There is nothing artificial about it. I always believe that the easy way is the right way. Scrum is simply the direct expression of one's feelings with the minimum of movements and energy. The closer to the true way of Agile, the less wastage of expression there is.
    >
    > Finally, a Scrum man who says Scrum is exclusively Scrum is simply not with it. He is still hung up on his self-closing resistance, in this case anchored down to reactionary pattern, and naturally is still bound by another modified pattern and can move within its limits. He has not digested the simple fact that truth exists outside all moulds; pattern and awareness is never exclusive.
    >
    > Again let me remind you Scrum is just a name used, a boat to get one across, and once across it is to be discarded and not to be carried on one's back.
    >
    > Learn the principle, abide by the principle, and dissolve the principle. In short, enter a mould without being caged in it. Obey the principle without being bound by it.
    >
    > LEARN, MASTER AND ACHIEVE!!!

    This is almost the quantification of what Scrum and Agile should mean to the people within your organization. Use it as a tool to aid in your movement towards a state within which you have your own tailored and adaptive process.

    A process that allows your organization to continuously delight your customers.
  FilePath: C:\Users\MartinHinshelwoodNKD\source\repos\NKDAgility.com\site\content\resources\blog\2014\2014-10-07-bruce-lee-on-scrum-and-agile\index.md
  FolderPath: C:\Users\MartinHinshelwoodNKD\source\repos\NKDAgility.com\site\content\resources\blog\2014\2014-10-07-bruce-lee-on-scrum-and-agile
- FrontMatter:
    title: Creating training virtual machines in Azure
    description: Learn to create training virtual machines in Azure effortlessly. This guide simplifies setup for technical courses, ensuring a smooth teaching experience.
    ResourceId: 93Qe5n6ux9U
    ResourceType: blog
    ResourceContentOrigin: Human
    ResourceImport: true
    ResourceImportId: 10771
    ResourceImportSource: Wordpress
    ResourceImportOriginalSource: Wordpress
    date: 2014-10-07
    weight: 875
    creator: Martin Hinshelwood
    layout: blog
    resourceTypes: blog
    slug: creating-training-virtual-machines-azure
    aliases:
    - /resources/93Qe5n6ux9U
    aliasesArchive:
    - /blog/creating-training-virtual-machines-azure
    - /creating-training-virtual-machines-azure
    - /creating-training-virtual-machines-in-azure
    - /blog/creating-training-virtual-machines-in-azure
    - /resources/blog/creating-training-virtual-machines-azure
    tags:
    - Install and Configuration
    - System Configuration
    categories:
    - Uncategorized
    preview: nakedalm-windows-logo-16-16.png
  BodyContent: |
    I am teaching the [Managing Projects with Microsoft Visual Studio Team Foundation Server 2013](http://nkdagility.com/training/courses/managing-projects-microsoft-visual-studio-team-foundation-server-2013/) class next week in Cheltenham and for that I need 16 VM's based on the [Visual Studio 2013 Update 3 ALM Virtual Machine](http://aka.ms/vs13almvm). To make life easier I will be creating training virtual machines in Azure.

    If you have ever had to teach a training course, especially a technical one, it’s the equipment that is the most painful thing to configure. Azure has matured a lot over the last few years and although I have configured training in Amazon's AWS service I wanted to go all Microsoft.

    The kind of tough thing is that the virtual hard disk (VHD) used by the virtual demo machine form Brian Keller is 80GB. And yes, I have to upload that beast to Azure.

    ## Uploading your Hyper-V virtual machine

    The first task is [to download and install the Azure PowerShell](http://go.microsoft.com/fwlink/p/?linkid=320376&clcid=0x409) using the web platform installer. This will get all of the pre-requisites and install them for you.

    ![clip_image001](images/clip-image0011-1-1.png "clip_image001")
    { .post-img }

    Once you have this installed you will have "Microsoft Azure PowerShell" available on your Start Menu. Run this and wait a bit for it to load all of its gubbens.

    ![clip_image002](images/clip-image0021-2-2.png "clip_image002")
    { .post-img }

    At this point we need a couple of things to be setup in the Azure Portal. I am sure that you can do this stuff with PowerShell as well, but I usually prefer to only drop to PowerShell for repeatable tasks. We need to configure a container for our VHD.

    ![clip_image003](images/clip-image0031-3-3.png "clip_image003")
    { .post-img }

    So if you do not already have one head over to the azure portal and create a Storage location. You will need to give it an unchangeable URL so pick carefully. I wish I had picked something other than "trainingeu" but that’s the way the cookie crumbles.

    NoteYou can create a new one with the right name and 'copy' the content over before deleting the old one. This, for now, is too much hassle.

    ![clip_image004](images/clip-image0041-4-4.png "clip_image004")
    { .post-img }

    We then create a 'folder' called a 'container' in there to hold our VHD's. We are now good to go for uploading out 80GB VHD from Microsoft. You will need to download all of the bits from them and unpack it locally to extract the VHD. That will take a while, even on my SSD's it takes about 30 minutes. Once there though you have a nave a clean clean VHD ready to go.

    ![clip_image005](images/clip-image0051-5-5.png "clip_image005")
    { .post-img }

    The first thing we need to do is authenticate with Azure which is fairly simple. You can automate this but for now the UI approach is best. If you call "Add-AzureAccount" without any parameters it will pop a UI authentication box. This did not work with "two-factor" and I had to turn that off, but that [could be a Windows 10 issue](http://nkdagility.com/agility-windows-10-upgrading-surface-pro-2/) rather than an Azure one.

    Once you are all logged in you can run any commands that you have permissions for and in this case we need to send our local VHD file to the VHD folder that we created.

    ![clip_image006](images/clip-image0061-6-6.png "clip_image006")
    { .post-img }

    ```
    Add-AzureVhd -Destination "https://trainingeu.blob.core.windows.net/vhds/bkvm-2013-3.vhd" -LocalFilePath "V:ServersBKVM2013.3WMIv2Virtual Hard Disksbkvm-2013-3.vhd"
    ```

    The command completes in three phases. First it creates an MD5 hash to verify the file. This can take a while. Then it looks for empty blocks in the disk. Your VHD may be 780GB on disk, but if it is only 60% full you only need to upload the 60%. Woot… win there.

    Then the upload happens. I left it running overnight on my 100mb Virgin Cable connection and it got about 27% through. Then I had to go onsite in Cheltenham to teach the Professional Scrum Foundations course and only had hotel or customer external bandwidth at no more than 0.5Mbps (more like 0.1Mbps at the hotel).

    The customer offered to wire me directly into their 100Mb synchronous line, and then pulled out the stops when they found out it was for their training course next week. 80GB done in 3 hours. Problem solved.

    NoteThe upload is reusable but only from the same computer. It does need to do the preamble each time though and as long as the file has not changed you can restart the upload.

    ## Creating an instance of your Virtual Machine

    So now I have a really big file on the internet. Time to make it do something useful. First we need to tell Azure that it really is a VHD and we really want to be a template so we can create lots of duplicates. In this case I need 16 on the day.

    ![clip_image007](images/clip-image0071-7-7.png "clip_image007")
    { .post-img }

    You need to create an 'image' that we will use as that template. If you go to "Virtual Machines | Images | Create" you will get the dialog you need to create the image. You will have to select "I have syspreped the virtual machine" even though you have not. We want duplicates of the same computer as is, rather than something that can be integrated into the network or domain. Sysprep would give us the out-of-box experience of a new install so that we can rename the computer. These VM< 's don’t need to talk to each other and will all have the same SID, name, and users. They are copies.

    ![clip_image008](images/clip-image0081-8-8.png "clip_image008")
    { .post-img }

    After few minutes our new template will be available in the gallery. Head to "Virtual Machines | Instances | New | From Gallery" to get the dialog above.

    ![clip_image009](images/clip-image0091-9-9.png "clip_image009")
    { .post-img }

    We can now create, through the UI, as many instances as we like and all we need to pick is the size / cost that we are willing to pay. The BKVM is a beast and has new than just TFS on there; SQL Server, SQL Server Analysis Services, SQL Server Reporting Services, Release Manager, TF Build, SharePoint 2013. So a big fast instance is needed. The local VM recommends 6GB RAM and 1 core.

    ![clip_image010](images/clip-image0101-10-10.png "clip_image010")
    { .post-img }

    After some experimenting I will be going with D2. D2 gives you the 2 cores and 7GB ram, but also adds SSD disks. This should give the students roughly the same performance that I get running the VM locally on my Surface.

    ## Creating many instances of your VM

    So we are good for creating one or two instances, but what if I need 16 (or 30) instances. I don’t want to have to do it manually so we need a little bit of PowerShell. My first attempt at this was to do:

    ```
    Add-AzureAccount
    New-AzureQuickVM -Windows -ServiceName ALM-Training -Name ALMP13-HESA-01 -ImageName 12683aa2-b30f-4cde-8b4a-281faafb7a57__Image__BKVM2013.3 -AdminUsername nakedalm –Password P2ssw0rd -location "West Europe"
    ```

    > But this resulted in a nasty error.
    >
    > New-AzureQuickVM : CurrentStorageAccountName is not accessible. Ensure the current storage account is accessible and
    >
    > in the same location or affinity group as your cloud service.
    >
    > At line:1 char:1
    >
    > \+ New-AzureQuickVM -Windows -ServiceName ALM-Training -Name ALMP13-HESA ...
    >
    > \+ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    >
    > \+ CategoryInfo : CloseError: (:) \[New-AzureQuickVM\], ArgumentException
    >
    > \+ FullyQualifiedErrorId : Microsoft.WindowsAzure.Commands.ServiceManagement.IaaS.PersistentVMs.NewQuickVM

    This error is a red hearing and confused me for ages as I could not figure out the issue.

    ![clip_image011](images/clip-image0111-12-12.png "clip_image011")
    { .post-img }

    I first looked to see if I had the cloud service in the right location. Yup… "West Europe".

    ![clip_image012](images/clip-image012-13-13.png "clip_image012")
    { .post-img }

    Maybe I put my storage in the wrong location? Nope… "West Europe". Dam, whats the problem…

    ```
    Get-AzureSubscription
    ```

    > SubscriptionId : 12683aa2-b30f-4cde-8b4a-281faafb7a57
    >
    > SubscriptionName : Pay-As-You-Go
    >
    > Environment : AzureCloud
    >
    > SupportedModes : AzureServiceManagement,AzureResourceManager
    >
    > DefaultAccount : martin@nakedalm.com
    >
    > Accounts : {martin@nakedalm.com}
    >
    > IsDefault : True
    >
    > IsCurrent : True
    >
    > CurrentStorageAccountName :
    >
    > SubscriptionId : 011bb48f-345c-4096-be52-d84c0efb7c3c
    >
    > SubscriptionName : MSDN Dev/Test Pay-As-You-Go
    >
    > Environment : AzureCloud
    >
    > SupportedModes : AzureServiceManagement,AzureResourceManager
    >
    > DefaultAccount : martin@nakedalm.com
    >
    > Accounts : {martin@nakedalm.com}
    >
    > IsDefault : False
    >
    > IsCurrent : False
    >
    > CurrentStorageAccountName :

    …Well after hunting around for a while it turns out that there was no default storage on the subscription. As the VM is added to the subscription, there needs to be a default store. A very misleading error, I would have preferred "Error: no storage specified".

    ```
    Set-AzureSubscription -SubscriptionName "Pay-As-You-Go" -CurrentStorageAccountName trainingeu -PassThru
    ```

    Now we have some storage wired up we can go ahead and create an instance with PowerShell…

    ```
    New-AzureQuickVM -Windows -ServiceName ALM-Training -Name ALMP13-HESA-01 -ImageName 12683aa2-b30f-4cde-8b4a-281faafb7a57__Image__BKVM2013.3 -AdminUsername nakedalm -Password P2ssw0rd -location "West Europe"

    ```

    > VERBOSE: 13:17:07 - Begin Operation: New-AzureQuickVM
    >
    > VERBOSE: 13:17:08 - Completed Operation: New-AzureQuickVM
    >
    > New-AzureQuickVM : Service already exists, Location cannot be specified.
    >
    > At line:1 char:1
    >
    > \+ New-AzureQuickVM -Windows -ServiceName ALM-Training -Name ALMP13-HESA ...
    >
    > \+ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    >
    > \+ CategoryInfo : CloseError: (:) \[New-AzureQuickVM\], ApplicationException
    >
    > \+ FullyQualifiedErrorId : Microsoft.WindowsAzure.Commands.ServiceManagement.IaaS.PersistentVMs.NewQuickVM

    Baws, what now. Ok… remove the location.

    ```
    New-AzureQuickVM -Windows -ServiceName ALM-Training -Name ALMP13-HESA-01 -ImageName 12683aa2-b30f-4cde-8b4a-281faafb7a57__Image__BKVM2013.3 -AdminUsername nakedalm -Password P2ssw0rd

    ```

    > VERBOSE: 14:41:25 - Begin Operation: New-AzureQuickVM
    >
    > VERBOSE: 14:41:25 - Completed Operation: New-AzureQuickVM
    >
    > VERBOSE: 14:41:26 - Begin Operation: New-AzureQuickVM - Create Deployment with VM ALMP13-HESA-01
    >
    > New-AzureQuickVM : BadRequest: The image name is invalid: Consecutive underscores as image name
    >
    > 12683aa2-b30f-4cde-8b4a-281faafb7a57\_\_Image\_\_BKVM2013.3 is not allowed.
    >
    > At line:1 char:1
    >
    > \+ New-AzureQuickVM -Windows -ServiceName ALM-Training -Name ALMP13-HESA ...
    >
    > \+ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    >
    > \+ CategoryInfo : CloseError: (:) \[New-AzureQuickVM\], CloudException
    >
    > \+ FullyQualifiedErrorId : Microsoft.WindowsAzure.Commands.ServiceManagement.IaaS.PersistentVMs.NewQuickVM

    Auch…pish… ok… so the image name should be the friendly name and not the actual name… that was obvious…

    ```
    New-AzureQuickVM -Windows -ServiceName ALM-Training -Name ALMP13-HESA-01 -ImageName BKVM2013.3 -AdminUsername nakedalm –Password P2ssw0rd
    ```

    > PS C:> New-AzureQuickVM -Windows -ServiceName ALM-Training -Name ALMP13-HESA-01 -ImageName BKVM2013.3 -AdminUsername na
    >
    > kedalm -Password P2ssw0rd
    >
    > VERBOSE: 14:42:11 - Begin Operation: New-AzureQuickVM
    >
    > VERBOSE: 14:42:11 - Completed Operation: New-AzureQuickVM
    >
    > VERBOSE: 14:42:12 - Begin Operation: New-AzureQuickVM - Create Deployment with VM ALMP13-HESA-01
    >
    > VERBOSE: 14:43:18 - Completed Operation: New-AzureQuickVM - Create Deployment with VM ALMP13-HESA-01
    >
    > OperationDescription OperationId OperationStatus
    >
    > \-------------------- ----------- ---------------
    >
    > New-AzureQuickVM 12fdcbb0-9f4d-1c96-b321-76983692da6e Succeeded

    Woot! I now have a new VM created kinda automated, or at least with the possibility.

    ![clip_image013](images/clip-image013-14-14.png "clip_image013")
    { .post-img }

    Poo… its an A1 instance. A piddley wee scrawny server that would have no hope of lifting SharePoint's fat ass, and that will defiantly get beaten up by Analysis Services. In fact the minimum for TFS is 2GB and we do not even meet that.

    ```
    New-AzureQuickVM -Windows -ServiceName ALM-Training -Name ALMP13-HESA-01 -ImageName BKVM2013.3 -InstanceSize D2 -AdminUsername nakedalm -Password P2ssw0rd
    ```

    Now I have more memory, more processor and lovely SSD's underpinning my student box.

    ![clip_image014](images/clip-image014-15-15.png "clip_image014")
    { .post-img }

    Phew..

    Now I could do all out and automate creating the VM's, as well as the Stop-AzureVM that will be needed at the end of the day and inexorable Start-AzureVM the next. However it may not be worth it. I can copy, paste 16 times and get the same result. If I was doing this a lot I might do a little more PowerShell.

    ## Conclusion

    With all the hassle of setting up and configuring local computers this service is an absolute dream. Now all that matters is the cost. Luckily there is a [handy dandy Azure pricing calculator](http://azure.microsoft.com/en-us/pricing/calculator).

    ![clip_image010[1]](images/clip-image01011-11-11.png "clip_image010[1]")
    { .post-img }

    This is the monthly cost for the fast SSD D-Series virtual machines and for 16 of them (one for each student) it looks like it would be around £2207.60 per month. That’s £2.97 per hour for all 16. The course is 16 hours so if I am careful it will be about £50 for a two day course. Of course if I forget to turn them off in the evening then it could hit £142.56 for 48 hours.

    The future is cloud…
  FilePath: C:\Users\MartinHinshelwoodNKD\source\repos\NKDAgility.com\site\content\resources\blog\2014\2014-10-07-creating-training-virtual-machines-azure\index.md
  FolderPath: C:\Users\MartinHinshelwoodNKD\source\repos\NKDAgility.com\site\content\resources\blog\2014\2014-10-07-creating-training-virtual-machines-azure
- FrontMatter:
    title: 'Agility and Windows 10: Upgrading my Surface Pro 2'
    description: Join Martin Hinshelwood as he shares his experience upgrading a Surface Pro 2 to Windows 10 Technical Preview, exploring agility and user feedback in tech.
    ResourceId: yn8SQ53TMaP
    ResourceType: blog
    ResourceContentOrigin: Human
    ResourceImport: true
    ResourceImportId: 10746
    ResourceImportSource: Wordpress
    ResourceImportOriginalSource: Wordpress
    date: 2014-10-02
    weight: 1000
    creator: Martin Hinshelwood
    layout: blog
    resourceTypes: blog
    slug: agility-windows-10-upgrading-surface-pro-2
    aliases:
    - /resources/yn8SQ53TMaP
    aliasesArchive:
    - /blog/agility-windows-10-upgrading-surface-pro-2
    - /agility-windows-10-upgrading-surface-pro-2
    - /agility-and-windows-10--upgrading-my-surface-pro-2
    - /blog/agility-and-windows-10--upgrading-my-surface-pro-2
    - /resources/blog/agility-windows-10-upgrading-surface-pro-2
    tags:
    - Windows
    categories:
    - Uncategorized
    preview: nakedalm-windows-logo-12-12.png
  BodyContent: |
    You know me, I am always willing to take a little risk to get some gain. As such I will be upgrading my Surface Pro 2 to Windows 10 Technical Preview on the train on the way back from teaching the Professional Scrum Foundations in Cheltenham.

    Yesterday, 1st September 2014, Microsoft released the first Technical Preview for Windows 10 to the general population. Anyone can give it a spin by going to [http://preview.windows.com](http://preview.windows.com). This preview program, and Windows 10, is different from any other Windows release from Microsoft. They are moving to continuous delivery for Windows.

    Within any company it can be hard to validate the value of a move towards agility. It's scary, its unknown, and it just will not work here. This is a fiction. It is a fiction created, and maintained, by your organisational culture, and culture is the hardest thing to change. It takes one group to change, and then the change will sweep across your organisation. In the case of Microsoft the group large enough to be a catalyst for that change was the Developer Devision. They have taken Visual Studio and Team Foundation Server from a ship frequency of every two years to one of only three weeks.

    If you want to know more about how Microsoft has managed to [scale agile across the enterprise](http://stories.visualstudio.com/scaling-agile-across-the-enterprise/) they have a fantastic case study.

    This change however, and the huge success that the team has wrote, both internally and in user interaction, is something that is in envy of every product team at Microsoft. Now the Windows team want a piece of that success. They want to not ever end up in the same position as Windows 8 where they made design decisions that by the time that they got feedback it was too late.

    Now Windows is moving to an agile process where they ship new features directly to end users every 4 weeks. Patch Tuesday just became Feature Tuesday.

    ![clip_image001](images/clip-image001-1-1.png "clip_image001")
    { .post-img }

    This new model, however, is hard to achieve. There were some spectacular disasters for the TFS team with TFS 2012 Update 1 and Update 2. Those were not fin times, but the teams had the courage to keep going and the transparency to maintain trust with their user base. And at the other end? They have an incredibly vocal user base that is willing and able to take the latest version of the product and give candid feedback.

    True continuous delivery is no more hotfixes, no more service packs, just new versions of your platform with the latest features and fixes included. This is what Windows wants a piece of. They need an engaged user base, they need an engaged enterprise community. They need that feedback.

    If you want to help them shape Windows 10, the last big Windows release, then you should join the Technical Preview.

    ![clip_image002](images/clip-image002-2-2.png "clip_image002")
    { .post-img }

    You do however need to be wary and if you are not comfortable with participating in preview programs or will be unhappy if it all blows up then you should stay away.

    > A preview for PC experts
    >
    > Windows Technical Preview is here today, but it’s a long way from done. We’re going to make it faster, better, more fun at parties...you get the idea. [Join the Windows Insider Program](http://go.microsoft.com/fwlink/?LinkId=507619) to make sure you get all the new features that are on the way. If you’re okay with a moving target and don’t want to miss out on the latest stuff, keep reading. Technical Preview could be just your thing.
    >
    > Download and install the preview only if you
    >
    > - Want to try out software that’s still in development and like sharing your opinion about it.
    > - Don’t mind lots of updates or a UI design that might change significantly over time.
    > - Really know your way around a PC and feel comfortable troubleshooting problems, backing up data, formatting a hard drive, installing an operating system from scratch, or restoring your old one if necessary.
    > - Know what an ISO file is and how to use it.
    > - Aren't installing it on your everyday computer.
    >
    > We're not kidding about the expert thing. So if you think BIOS is a new plant-based fuel, Tech Preview may not be right for you.

    That said, even my dad loves being part of the early adopter programs and playing with the new bits. He is my sanity test for the non-technical user and has already been part of the Windows 8 Consumer Preview and the Windows Phone 8.1 beta program. If you do want to participate in the program you do need to understand that there is limited ability to go back. However unlike the Windows Phone preview program you can wipe your computer and start over.

    ![clip_image003](images/clip-image003-3-3.png "clip_image003")
    { .post-img }

    Once you have signed in and 'joined the insider' program you will get a link to install the Technical Preview. While it would be awesome if we could just click a web installer this is not currently an option and you will need to download the bits. We will likely get a web / update installer from the Consumer preview due in Q1 2015.

    ![clip_image004](images/clip-image004-4-4.png "clip_image004")
    { .post-img }

    I downloaded both the UK English ([proper English](http://nkdagility.com/powerpointissue-i-spell-it-as-favourite-and-you-as-favorite/)) versions for x86 and x64. I have a Dell Venue 8 Pro, and a Surface Pro 2 that I will be installing on. First up was the Venue 8 Pro which has a 32 bit OS and only comes with 32 bit drivers (no idea why, it is a 64 bit platform). Once you have downloaded the ISO you can use WinRar or 7Zip to unpack it. If you are installing on Windows 8 you can just double-click it to 'mount' it as a virtual CD drive.

    ![clip_image005](images/clip-image005-5-5.png "clip_image005")
    { .post-img }

    As The Dell Venue 8 only has 64GB drive space I opted to for the USB option. I formatted an old USB drive to save space. Just copy the files over to the USB. You can then insert the USB and run the setup.exe in the root.

    ![clip_image006](images/clip-image006-6-6.png "clip_image006")
    { .post-img }

    Once the install is running it takes a little while. It took about 30 minutes on my Dell Venue 8 Pro and over 60 minutes on the Surface Pro 2. This is likely as the Surface Pro has a bunch more apps installed and the settings need ported. My Surface is my primary production computer and has Visual Studio among other things installed.

    ![clip_image007](images/clip-image007-7-7.png "clip_image007")
    { .post-img }

    Once you have the files copied your system will reboot a bunch of times with prolonged setup in between. I installed on the Venue 8 in the morning at breakfast in the hotel before the second day of the PSF course I was just running, and the Surface on the train on the way to the airport. I am typing this up on the Surface at said airport since my flight has been delayed by 3 hours (always on FlyBe).

    ![clip_image008](images/clip-image008-8-8.png "clip_image008")
    { .post-img }

    Surface Pro 2

    Once you get installed you will get the new Windows user experience all over again. When it askes to add a new user you can safely skip it and just login with your existing account. With the Dell Venue 8 Pro I got to continue using the Start Screen, which I incidentally like, and with the Surface I got the new Start Menu that is a little more retro.

    ![clip_image009](images/clip-image009-9-9.png "clip_image009")
    { .post-img }

    First thing you should always do, even after installing an OS that just came out, is look for updates. There are always last minute bug fixes coming out.

    Once in Windows 10 there are a few things that I noticed immediately.

    ![clip_image010](images/clip-image010-10-10.png "clip_image010")
    { .post-img }

    You can only open modern apps in window mode and have to deliberately switch to full screen. This while fine on my Surface Pro is a pain on my Venue 8. I understand that the Windows team is still working on optimising the touch experience so I am willing to give them plenty of leeway.

    Another painful loss in the inability to close an app when in full screen by swiping down as you would on Windows 8. This is incredibly frustrating as I am really used to this model. However they are building this to entice the folks that don’t like Windows 8 over so, meh…

    Note: I did meet someone today that did not know that you could swipe from the top to close an app and was keeping the task manager open so they could close it! So maybe there needs to be a change.

    You can swipe from the left to get the task manager below and click the close so maybe all if good. It is two actions where one will do but it may make a comeback.

    ![clip_image011](images/clip-image011-11-11.png "clip_image011")
    { .post-img }

    Swipe from Left does not just switch apps any more but instead presents you with the task switcher. This incorporates the new multi-desktop mode as well. Click on the "add desktop" option to add more. We will see how useful this is.

    That’s all I got from a first play, except to say that all my day job things worked just fine after the upgrade. The only issue I have hit so far is that SnagIt sometimes crashes when taking a screenshot. As SnagIt is very dependent on the UI integration, and the work they has to do to make it work on Windows 8 in the first place, I am not surprised it is a little confused.

    While I take no responsibility for any issues that you may have if you install Windows 10 I would recommend that anyone technically minded does so. And that they run it on their main computer. Take a little risk and provide feedback. The worst that can happen is that you need to reinstall Windows 8 / 7.

    Go on… be a kid again…
  FilePath: C:\Users\MartinHinshelwoodNKD\source\repos\NKDAgility.com\site\content\resources\blog\2014\2014-10-02-agility-windows-10-upgrading-surface-pro-2\index.md
  FolderPath: C:\Users\MartinHinshelwoodNKD\source\repos\NKDAgility.com\site\content\resources\blog\2014\2014-10-02-agility-windows-10-upgrading-surface-pro-2
- FrontMatter:
    title: Install of TFS 2013.3 with SharePoint 2013 on Windows Server 2012 R2 Update 1
    description: Learn how to install TFS 2013.3 with SharePoint 2013 on Windows Server 2012 R2. Follow expert tips for a smooth setup and avoid common pitfalls!
    ResourceId: IHKBrsNm0Cp
    ResourceType: blog
    ResourceContentOrigin: Human
    ResourceImport: true
    ResourceImportId: 10727
    ResourceImportSource: Wordpress
    ResourceImportOriginalSource: Wordpress
    date: 2014-09-30
    weight: 840
    creator: Martin Hinshelwood
    layout: blog
    resourceTypes: blog
    slug: install-tfs-2013-3-sharepoint-2013-windows-server-2012-r2-update-1
    aliases:
    - /resources/IHKBrsNm0Cp
    - /resources/blog/install-of-tfs-2013.3-with-sharepoint-2013-on-windows-server-2012-r2-update-1
    aliasesArchive:
    - /blog/install-tfs-2013-3-sharepoint-2013-windows-server-2012-r2-update-1
    - /install-tfs-2013-3-sharepoint-2013-windows-server-2012-r2-update-1
    - /install-of-tfs-2013-3-with-sharepoint-2013-on-windows-server-2012-r2-update-1
    - /blog/install-of-tfs-2013-3-with-sharepoint-2013-on-windows-server-2012-r2-update-1
    - /resources/blog/install-tfs-2013-3-sharepoint-2013-windows-server-2012-r2-update-1
    - /resources/blog/install-of-tfs-2013.3-with-sharepoint-2013-on-windows-server-2012-r2-update-1
    tags:
    - Install and Configuration
    - System Configuration
    - Windows
    - Troubleshooting
    - Software Development
    categories:
    - Uncategorized
    preview: nakedalm-experts-visual-studio-alm-27-27.png
  BodyContent: |
    I have been onsite in Athens for the last month to do a migration from StarTeam to TFS. We did an install of TFS 2013.3 with SharePoint 2013 on Windows Server 2012 R2 Update 1. This was my first install of TFS 2013.3 from scratch so I thought that it would be a good idea to walk through the process.

    When onsite customer's often think that it is a good idea to pre-install things like SQL and TFS. However it is not. Even if their infrastructure department is awesome they will not read the TFS install documentation and will not install SQL or TFS to spec. When I get onsite the perfect scenario is a clean copy of Windows with all of the Updated installed. And I do mean all of them… not just the ones that have been approved by your WSUS department.

    ![clip_image001](images/clip-image0011-1-1.png "clip_image001")
    { .post-img }

    In this case I found a preinstalled Windows Server 2008 R2 with SQL 2012 and TFS 2013. So I would have to patch Windows, apply service packs to SQL (and likely reconfigure to get full text search), as well as Patch and reconfigure TFS.

    However my first suggestion was to upgrade the OS to Server 2012 R2. Server 2008 R2 is three versions old and mainstream support ends in January. So instead of leaving the customer in the situation of having to upgrade in January I just asked for a new server with Server 2012 R2. They did however install and configure SQL… more on that later.

    ![clip_image002](images/clip-image0021-2-2.png "clip_image002")
    { .post-img }

    Now internet access in Greece is not particularly fast so downloading fresh bits was out of the question. I often got just 32/64 kbs downloads…. Not fun.. However, like a good TFS consultant I brought all of the bits with me and was ready to go.

    ![clip_image003](images/clip-image0031-3-3.png "clip_image003")
    { .post-img }

    I really like that since Windows 8 and Server 2012 you can just double click on an ISO to have it automatically mounted. This was way past time to be out of the box.

    ![clip_image004](images/clip-image0041-4-4.png "clip_image004")
    { .post-img }

    Update 2013.4 is in CTP so you can't run it in production, but as soon as it comes with a Go-Live I would recommend upgrading for any customer. For now… its Team Foundation Server 2013 Update 3.

    ![clip_image005](images/clip-image0051-5-5.png "clip_image005")
    { .post-img }

    The default install is a Standard Server and while that should work perfectly I ran into a SharePoint issue and had to go with Advanced.

    ![clip_image006](images/clip-image0061-6-6.png "clip_image006")
    { .post-img }

    In the advanced configuration wizard you get to pick extras like choosing to not install SharePoint or using a non-standard SQL instance, or even a remote one. I mostly pick the advanced option anyway as I can always just click next if I don’t want to configure that screen.

    ![clip_image007](images/clip-image0071-7-7.png "clip_image007")
    { .post-img }

    When you pick your SQL server you will get options to prefix your databases. This allows you to co-host multiple TFS instances on the same SQL server. You would get "tfs_inst2_Configuration" and so forth. You can also configure 'always-on'. If you don’t know what it is then don’t tick it… but it can give you much better availability by automatically failing over. Its kind of like a cluster, but without the pain and suffering.

    ![clip_image008](images/clip-image0081-8-8.png "clip_image008")
    { .post-img }

    Why do operations teams always give you tiny primary drives? I want all of my throw away stuff on the primary disc with the OS. If there is a failure then I don’t need it anyway. This goes for Installation, cache, and other volatiles. Microsoft recommends about 50GB for the cache so I had to switch it to the secondary drive that has more room.

    ![clip_image009](images/clip-image009-9-9.png "clip_image009")
    { .post-img }

    You can choose to have reporting services integration here. If you are installing Reporting Services and Analysis services you can select this option. You then get both a Data Warehouse and multi-dimensional cube built out of the TFS data. Fantastic for trend analysis of your setup. Cubes make it really easy to pull together reports with Code Coverage plotted with Test Results and Code Churn. There are simpler alternatives for reporting with Excel and in the TFS Team dashboards but if you want it you can have it. This customer already has their own Business Intelligence department so they will be comfortable with MDX (Eeew) and creating reports.

    ![clip_image010](images/clip-image010-10-10.png "clip_image010")
    { .post-img }

    If you want your TFS server to be on a friendly URL then you probably also want to select it for reporting too. Beyond the default server name you will need to go to the Reporting Services console and add the additional URL's as host headers.

    ![clip_image011](images/clip-image011-11-11.png "clip_image011")
    { .post-img }

    The reason the advanced wizard asks you all of these questions is so that you can implement large scale TFS instances. You may choose to have each of the services running on their own instance or even a farm. You can find out from the TFS documentation when you need to scale out but is really beyond 300 users.

    ![clip_image012](images/clip-image012-12-12.png "clip_image012")
    { .post-img }

    For reporting services to function correctly you should have a 'report reader' account. This account is used to execute the reports and to keep things simple, if I am deploying all on one box, I usually setup a local account. That way it has no permission outside of the server.

    ![clip_image013](images/clip-image013-13-13.png "clip_image013")
    { .post-img }

    Anyone that knows me will know my favourite Scottish proverb.

    "First you have a problem, you solve it with SharePoint. Now you have two problems."

    However I do love SharePoint as a user, I just don't want to have to administer it. Here with SharePoint Foundation we get some good capabilities with little pain. SharePoint Enterprise is another storey….

    ![clip_image014](images/clip-image014-14-14.png "clip_image014")
    { .post-img }

    It was with the configuration of SharePoint that I ran into my first issue. The version of SharePoint that ships with TFS 2013.3 does not support Server 2012 R2. I had t manually \[download and install SharePoint Foundation 2013 Service Pack 1\] which does support the new OS.

    I am fine with installing this separately and would be happy with just a link here to get the latest version of SharePoint and install it manually before proceeding.

    As I had to reboot anyway to resolve the TF255466 installed update issue I had to rerun the wizard back to this location anyway. You can however just hit 're-run checks' once you have fixed most issues to allow you to proceed.

    ![clip_image015](images/clip-image015-15-15.png "clip_image015")
    { .post-img }

    My second issue came at the end where TFS runs the Readiness Checks. These checks are awesome and make it a painless (or as painless as you can get) experience to configure TFS. It goes out and checks as many things as it can that could cause a problem with the configuration before you get half way through and have to start again. It’s a time and frustration saver.

    And that is how I found out that while the operations team had installed but not configured Reporting Services. Although I don’t have a screenshot they had to also add full text search as this was missing from the SQL install and required by TFS.

    ![clip_image016](images/clip-image016-16-16.png "clip_image016")
    { .post-img }

    Configuring reporting services is easy and we really want all default options. As Reporting Services was not configured with the default options we have to go and give it the defaults. Literally you open the Configuration Manager and select "Web Services URL" and then "Apply" to take the defaults.

    Note: At this point you can also add your friendly URL

    ![clip_image017](images/clip-image017-17-17.png "clip_image017")
    { .post-img }

    Then the same for the "Report Manager URL"… just head on over and click "Apply" to create the default bits.

    ![clip_image018](images/clip-image018-18-18.png "clip_image018")
    { .post-img }

    Creating the databases is the only part that has any complexity, however, it is just "Create Databases"…

    ![clip_image019](images/clip-image019-19-19.png "clip_image019")
    { .post-img }

    …"Next"…

    ![clip_image020](images/clip-image020-20-20.png "clip_image020")
    { .post-img }

    …"Next"…

    ![clip_image021](images/clip-image021-21-21.png "clip_image021")
    { .post-img }

    …"Next"…

    ![clip_image022](images/clip-image022-22-22.png "clip_image022")
    { .post-img }

    …"Next"…

    ![clip_image023](images/clip-image023-23-23.png "clip_image023")
    { .post-img }

    …"Next"…

    ![clip_image024](images/clip-image024-24-24.png "clip_image024")
    { .post-img }

    …"Finish"…

    Remove this hassle and when you install Reporting Services pick the default "Install and configure".

    ![clip_image025](images/clip-image025-25-25.png "clip_image025")
    { .post-img }

    Flip back to the TFS configuration wizard and re-run the readiness checks to make all your errors disappear.

    ![clip_image026](images/clip-image026-26-26.png "clip_image026")
    { .post-img }

    And that’s it. When you click "Configure" TFS will go off and create all the bits it needs and setup your default collection. At the end of this process, if you get a green tick, you have a fully operational TFS Instance.

    Good luck with your install…
  FilePath: C:\Users\MartinHinshelwoodNKD\source\repos\NKDAgility.com\site\content\resources\blog\2014\2014-09-30-install-tfs-2013-3-sharepoint-2013-windows-server-2012-r2-update-1\index.md
  FolderPath: C:\Users\MartinHinshelwoodNKD\source\repos\NKDAgility.com\site\content\resources\blog\2014\2014-09-30-install-tfs-2013-3-sharepoint-2013-windows-server-2012-r2-update-1
- FrontMatter:
    title: Yorkhill Ice Bucket Challenge
    description: Join Martin Hinshelwood in the Yorkhill Ice Bucket Challenge to support ALS and Yorkhill Children's Charity. Discover his inspiring journey and donate today!
    ResourceId: dALLwF2kL-U
    ResourceType: blog
    ResourceContentOrigin: Human
    ResourceImport: true
    ResourceImportId: 10682
    ResourceImportSource: Wordpress
    ResourceImportOriginalSource: Wordpress
    date: 2014-08-24
    weight: 1000
    creator: Martin Hinshelwood
    layout: blog
    resourceTypes: blog
    slug: yorkhill-ice-bucket-challenge
    aliases:
    - /resources/dALLwF2kL-U
    aliasesArchive:
    - /blog/yorkhill-ice-bucket-challenge
    - /yorkhill-ice-bucket-challenge
    - /resources/blog/yorkhill-ice-bucket-challenge
    tags: []
    categories:
    - Uncategorized
    preview: yorkhill-ice-bucket-challange-5-5.png
  BodyContent: |
    Yesterday [Buck Hodges](http://blogs.msdn.com/b/buckh/archive/2014/08/21/the-als-ice-bucket-challenge.aspx) took the ALS Ice Bucket Challenge. He was challenged by [Brian Harry](http://blogs.msdn.com/b/bharry/archive/2014/08/19/als-ice-bucket-challenge.aspx) who was in turn by [Adam Cogan](https://www.youtube.com/watch?v=P_4FM9laAl0). As I know all three of these folks it was only a matter of time before I got nominated. My torturer of the day is Buck. Really… how bad can it be? I did Tough Mudder this weekend my favourite obstacle is [Arctic Enema](https://toughmudder.co.uk/obstacles/arctic-enema) :p!!

    This ALS Ice Bucket Challenge has been around for a few weeks now it has been hugely successful for ALS. This is a good thing and I that it can benefit other charities as well. In 2010 [my son Kaiden had to have brain surgery](http://kaiden.hinshelwood.com/2010/07/operation.html). He had an arachnoid cyst that was blocking the drainage of fluid from his brain. This was effectively like a blocked sink, causing hydrocephalus, or 'water on the brain'.

    ![kaiden-hinshelwood-arachnoid-cyst-hydrocephalus](images/kaiden-hinshelwood-arachnoid-cyst-hydrocephalus-794x450-3-4.png)
    { .post-img }

    He was treated at the Southern General Hospital until we moved to the USA, then Seattle Children's Hospital for a few years, and now Yorkhill Children's Hospital in our home town of Glasgow in Scotland. Yorkhill is taking care of him now and I will be giving to the [Yorkhill Children's Charity](http://www.yorkhill.org/). So I guess it’s the **Yorkhill Ice Bucket Challenge**.

    I am currently onsite in Norway with no transport and had to go on a mission to find some Ice. So I donned my running gear and sped down to the local supermarket. You would think that in Norway of all places there would be a ready supply of ice in stock...

    ![clip_image001](images/clip-image001-1-1.jpg "clip_image001")
    { .post-img }

    ... So with no Ice in Norway I decided that it would be best to do it at home on Saturday instead. Although I work in London, Oslo, Utrecht, and Athens during the week I always make sure that I am home for as many weekends as I can with my kids. Eva and Kai are awesome and I though that they would really enjoy poring ice over daddy.

    ![clip_image002](images/clip-image002-2-2.jpg "clip_image002")
    { .post-img }

    We went on a mission to find ice and to my surprise many of then shops were all out. Apparently there are a few folks doing Ice Bucket challenges. ASDA and Tesco were both out so I headed to the more snobby (and thus less likely to be dry) Sainsbury's where we indeed found some ice. Wohoo...

    \[embed\]https://www.youtube.com/watch?v=w4tFJxAaxoc\[/embed\]

    You will notice that evil Eva bided her time and when I thought that all the water had been pored, when in fact it was only Kai's, she got her turn. If you listen carefully at the start of the video you can hear her manic and decidedly malevolent excitement at the prospect...

    As I said, I am giving my £100 to the Yorkhill Childress Charity to the benefit of the new Yorkhill Hospital for Sick Children. To carry on I nominate [David Starr](http://courses.scrum.org/about/david-starr) from Scrum.org, David Hinshelwood, and [Iain Frame](http://uk.linkedin.com/pub/iain-frame/0/558/b77).
  FilePath: C:\Users\MartinHinshelwoodNKD\source\repos\NKDAgility.com\site\content\resources\blog\2014\2014-08-24-yorkhill-ice-bucket-challenge\index.md
  FolderPath: C:\Users\MartinHinshelwoodNKD\source\repos\NKDAgility.com\site\content\resources\blog\2014\2014-08-24-yorkhill-ice-bucket-challenge
- FrontMatter:
    title: Migrating source from Perforce to Git on VSO
    description: Learn how to migrate from Perforce to Git on VSO with expert tips and strategies for a smooth transition to modern version control. Start your journey now!
    ResourceId: bvuzuOYg8gs
    ResourceType: blog
    ResourceContentOrigin: Human
    ResourceImport: true
    ResourceImportId: 10677
    ResourceImportSource: Wordpress
    ResourceImportOriginalSource: Wordpress
    date: 2014-08-20
    weight: 640
    creator: Martin Hinshelwood
    layout: blog
    resourceTypes: blog
    slug: migrating-source-perforce-git-vso
    aliases:
    - /resources/bvuzuOYg8gs
    aliasesArchive:
    - /blog/migrating-source-perforce-git-vso
    - /migrating-source-perforce-git-vso
    - /migrating-source-from-perforce-to-git-on-vso
    - /blog/migrating-source-from-perforce-to-git-on-vso
    - /resources/blog/migrating-source-perforce-git-vso
    tags:
    - Azure DevOps
    - Software Development
    - Modern Source Control
    - Install and Configuration
    - Azure Repos
    - Pragmatic Thinking
    categories:
    - Uncategorized
    preview: naked-alm-git-1-1.png
  BodyContent: |
    I have been working with a customer in London this week that is using TFS 2010 for work item tracking and Perforce for source control. Here is how I got on migrating source from Perforce to Git on VSO.

    It is rare for European companies to be OK with cloud but these guys are very progressive. They create software that the legal profession uses and even have a cloud offering of their own. They currently use Office 365 and don't really want to have to run anything locally. They have a last few servers in a rack in their office which only serves to heat it up in the summer. Perforce is one of those last local servers.

    There is a script that part of the Git codebase to migrate history from Perforce to Git. However, you would have to take your code as is and would easily run into the problems that are described below with the conflicting workflows of Server based and Node based source control.

    _Note: If you are moving from TFVC to Git then you can do the same using a tool called Git-TF. You [clone a TFVC repository and push to Git](http://nkdagility.com/migrating-source-code-with-history-to-tfs-2012-with-git-tf/)._

    ## Migrating source from Perforce to Git on VSO

    Currently the Source Control adapters in the TFS Integration tools do not support Git migration and I do not believe they ever will. And that is how it should be. Why you might ask... Well if you are moving to Git then you are moving to an entirely new premise for version control. Instead of living in the linear world of server based source control you will be moving to the node based source control where there just happens to be a bit that sits on the server. Trust me, its a big deal.

    With Git you can check in locally many times before you push those changes to the server. Indeed you can merge your changes with one of your colleagues local repositories and when you both eventually push to the server it will just all work out (mostly). The other awesome features of Git, like local branches or pull requests, are beyond the scope of this post, however they are all things that you are going to want and soon. You are going to hear tell, from other developers, of this wonderful world where merging and branching is easy and you can roll-back changes locally. You are going to want it even if you don't yet know you will. Short circuit this and plan your move today...

    There are however a number of caveats that make an automated tool, or taking history difficult if not impossible.

    - **Repository Size** - You really want to keep the repository small for Git. This means that we need to break large code bases down into smaller parts. This is going to be often hard, but will make things more manageable in the long run.
    - **No binaries** - you should not have any binaries stored in Git. It dramatically reduces performance. However getting that crape out of source control can only benefit everyone from Coders to Testers, Build servers and DevOps.

    I worked with one of my clients (Ted) who understood his product to break it down into this component parts. Components that we can build together and separately. We started by looking for something in our applications that many things depended on. In this case we had a set of Core components that many applications used. You need to start at the bottom of the dependency tree for your application and see what logical groupings you can make.

    In their VSO account we created a single Team Project called 'main' within which all of their applications and teams will reside. They are starting with two teams, the Development Team and the Business Team, and they may very well end up with product specific teams for backlog management specific to a product. There is a simple formula for doing a Git migration:

    1. **Create new Git repository in VSO** - I tend to create one of the same name as the solution that I want in there. And, yes, if you are new to Git then stick to the 'one-solution-one-repository' mantra.
    2. **Clone the Git repository** - This puts a local copy of the new and empty repository on disk. If you connect to the repository in Visual Studio then you will be offered a clone button.
    3. **Copy/Create Solution and Projects** - Getting the files into Git is as easy as copying t hem in. In this case we were picking about 10 of the 30 projects to go into a new Core solution.
    4. **Get your Solution to build** - While I have seen many errors before and can help speed up the process of diagnostics I don't know your code base. My guy (Ted) is an expert in his own code and was tasked with getting everything to build. There are a few common errors here. First is reference errors. We had a bunch of places where the projects referenced the output of the project rather than the project as a reference. Remove as many direct dell references as you can. Once you can get your application to build its time to move to the next stage.
    5. **Strip binaries to NuGet** - Performance in Git degrades as the repository size increases so don't overly burden it with binaries. Get them out by moving them to a NuGet Repository. If you can then you should replace all your manual references with public NuGet ones ([http://nuget.org](http://nuget.org)). Of the five references to assorted other DLL's we found two of them on NuGet, so an easy replace. For another we created a brand new NuGet packages with the DLL's we needed and published temp to a private feed on [http://myget.com](http://myget.com). MyGet is a hosted NuGet Repository that you can quickly spin up and is cloud based. If you are an enterprise they do offer an on-premises version. We did encounter one problem of needing to have a signed version of an unsigned assembly. We had a batch file to sign it ourselves and we manually solved it, but there are more elegant ways. If we had tens of requirements for this then we could have built a PowerShell that downloaded the NuGet Packages, signed them, re-packaged them with a 'signed' postfix, and uploaded them to our private repository.
    6. **Get your Solution to build** - Again, now that we have all of the dependencies replaced you may have broken our solution. We did, a bunch, and found that building after every change helped identify issues, and their cause, early.
    7. **Commit and Push** - now that we have the solution building we can check in. This is done in Git with a Commit to your local repository and then a Push to the TFS Git Repository.
    8. **Create CI Build** - With code now in the repository we can create a CI build to make sure we have everything right, and keep it right. We quickly used up the 50 minutes of build for free a month so we configured a private build server in an Azure VM. This is just like setting up an on-premises build server except the machine runs on Azure. Just remote desktop in and install the bits and dependencies. You may have to do this if you have custom components that you need to install on the build server. These guys use InstalliShield so they would always have had to go down this road.

       _Note: if your vendor does not provide a 'no-install' version for build servers you should put pressure on them to change their ways. If they will not then consider changing vendor. WIX is an open source installer product that is used by Microsoft to build its own installers. It will build on a build server out-of-the-box._

    9. **Get your solution to build on the build server** - Build servers can be a little more... Unforgiving... Than local builds. Paths are different and if things that are installed locally on the developer workstation don't exist in source you can hit issues. Remember that NuGet and Chocolatey are your friends. NuGet for internal dependencies and Chocolaty for the external ones.
    10. **DONE**

    While there is some work in this approach it is eminently worth it. In order to move to Git you need to think more about how your application is composed. You need to streamline the build process and fix all of the dependencies. Your builds should be faster, your components more robust, and you should start getting fewer bugs. All this from a slightly different workflow with your source. Its easy to check in a pile of crap to a server based source control system... But a little more though and deliberation is required from a node based one.

    _Note: If you are migrating ChangeSets then you may want to write a script to move all of the Check-in to work item associations to the comments as #\[ID\]. When you push to Git on the server TFS will automatically link any hashtag work item ID's to the Check-in. Good for reporting._

    ## Conclusion

    There are huge benefits from moving to VSO and Git from an on-premises TFS that start with cloud infrastructure. You get AD integration through Azure so you can integrate with your local AD as well as being able to add foreign principals (Microsoft ID) as well. This gives you easy control over external resources with ease. Add to that features appear first, as much as three months to a year before they are available on-premises you can get ahead of the curve.

    _Note: If you are in Europe and concerned about the patriot act look up the recent court cases with Microsoft going to bat, all in, for data privacy in Europe on this exact issue. Microsoft has vowed (along with the other cloud providers) to fight the US state department on this with the assertion (correct in my opinion) that US law ends at US borders._

    All in I would recommend any organisation that can move to VSO to do so.
  FilePath: C:\Users\MartinHinshelwoodNKD\source\repos\NKDAgility.com\site\content\resources\blog\2014\2014-08-20-migrating-source-perforce-git-vso\index.md
  FolderPath: C:\Users\MartinHinshelwoodNKD\source\repos\NKDAgility.com\site\content\resources\blog\2014\2014-08-20-migrating-source-perforce-git-vso
- FrontMatter:
    title: You can't use WITADMIN on versions older than TFS 2010
    description: Discover why WITADMIN can't be used on TFS versions older than 2010 and learn how to resolve compatibility issues for a smoother migration experience.
    ResourceId: UF4o8raFE4r
    ResourceType: blog
    ResourceContentOrigin: Human
    ResourceImport: true
    ResourceImportId: 10667
    ResourceImportSource: Wordpress
    ResourceImportOriginalSource: Wordpress
    date: 2014-08-13
    weight: 840
    creator: Martin Hinshelwood
    layout: blog
    resourceTypes: blog
    slug: cant-use-witadmin-versions-older-tfs-2010
    aliases:
    - /resources/UF4o8raFE4r
    - /resources/blog/you-can-t-use-witadmin-on-versions-older-than-tfs-2010
    aliasesArchive:
    - /blog/cant-use-witadmin-versions-older-tfs-2010
    - /cant-use-witadmin-versions-older-tfs-2010
    - /you-can't-use-witadmin-on-versions-older-than-tfs-2010
    - /blog/you-can't-use-witadmin-on-versions-older-than-tfs-2010
    - /resources/blog/cant-use-witadmin-versions-older-tfs-2010
    - /resources/blog/you-can-t-use-witadmin-on-versions-older-than-tfs-2010
    tags:
    - Troubleshooting
    categories:
    - Uncategorized
  BodyContent: |
    I encountered a bit of a red herring today when I was trying to rename a Work Item Type Definition (WITD) and received the message that you can't use WITADMIN on versions older than TFS 2010. However the server was TFS 2010.

    I am onsite in London this week doing a migration from TFS 2010 and Perforce to Visual Studio Online (VSO) and hit a confusing error message. My Surface only has Visual Studio 2013 installed so I am calling the 2013 version of WITADMIN against the TFS 2010 server. Since TFS 2010 is fully supported this should work with no issues. However instead of working I got a strange message:

    ![clip_image001](images/clip_image001-1-1.png "clip_image001")
    { .post-img }

    > witadmin.exe : You cannot change the display name of a work item type. The feature is not supported on versions earlier than Team Foundation Server 2010.

    What do you mean visions older than TFS 2010! This is TFS 2010 dam it… so I bit the bullet and spun up my TFS 2012 box that has Visual Studio 2012 and the 2012 version of WITADMIN.

    ![clip_image002](images/clip_image0022-2-2.png "clip_image002")
    { .post-img }

    And lo and behold I got the very same message. This made me think that there was something wrong with the TFS server. The server was a native 2010 (no upgrades) so there should be no issues. I logged onto the server to take a look and what did I find? TFS2012 RTM.

    So.. First things first I need to update the server. I will be using the TFS Integration Tools to move to VSO, OpsHub do not support changing Team Project name, but its so much easier when you have the same process template and I really need to update it. I was thinking of updating strate to 2013 but that would require an upgrade of SQL Server. I thought of upgrading to TFS 2012 but that would require a Service Pack for SQL Server. The least dangerous option in the end was to apply TFS 2010 Service Pack 1…

    ![clip_image003](images/clip_image0032-3-3.png "clip_image003")
    { .post-img }

    And after the upgrade?

    ![clip_image004](images/clip_image004-4-4.png "clip_image004")
    { .post-img }

    Now I can run WITADMIN commands again.

    You should always make sure that you have the latest version of whatever software that you want to use to make sure that you get compatibility with the tools. Even if you can't upgrade a full version you should never have less than TFS 2010 SP1, TFS 2012.4, or TFS 2013.2.
  FilePath: C:\Users\MartinHinshelwoodNKD\source\repos\NKDAgility.com\site\content\resources\blog\2014\2014-08-13-cant-use-witadmin-versions-older-tfs-2010\index.md
  FolderPath: C:\Users\MartinHinshelwoodNKD\source\repos\NKDAgility.com\site\content\resources\blog\2014\2014-08-13-cant-use-witadmin-versions-older-tfs-2010
- FrontMatter:
    title: Avoid the Bug as Task anti-pattern in Azure DevOps
    description: Learn to avoid the Bug as Task anti-pattern in Azure DevOps to enhance team communication, transparency, and software quality. Improve your agile processes now!
    ResourceId: ppfe8CkVNf0
    ResourceType: blog
    ResourceContentOrigin: Human
    ResourceImport: true
    ResourceImportId: 10662
    ResourceImportSource: Wordpress
    ResourceImportOriginalSource: Wordpress
    date: 2014-08-06
    weight: 430
    creator: Martin Hinshelwood
    layout: blog
    resourceTypes: blog
    aliases:
    - /resources/ppfe8CkVNf0
    aliasesArchive:
    - /blog/avoid-bug-task-anti-pattern-tfs
    - /avoid-bug-task-anti-pattern-tfs
    - /avoid-the-bug-as-task-anti-pattern-in-azure-devops
    - /blog/avoid-the-bug-as-task-anti-pattern-in-azure-devops
    - /blog/avoid-bug-task-anti-pattern-azure-devops/
    - /resources/blog/
    tags:
    - Software Development
    - Azure DevOps
    - Transparency
    - Pragmatic Thinking
    categories:
    - Uncategorized
    preview: NKDAgility-technically-BugAsATask-5-5.jpg
  BodyContent: |
    For the last few months, I have been working with an enterprise customer that has been steadily adopting Work Item Tracking in TFS. I have learned that you should avoid the Bug as a Task anti-pattern.

    ![](images/image-1.png)
    { .post-img }

    Updated

    - 2023-10-10 - Updated to reflect Azure DevOps, and spelling fixes.

    The customer that I have been working with is fairly large and there are quite a few DevOps Consultants helping them out. My task was to onboard one department of about 120 people and ~10 teams. Incidentally, these teams are, for the most part, Java teams. As with any enterprise, there are things that departments can decide for themselves and things that are handed down from on high and they are often fond of 'laterals'. A 'lateral' is a group of people who get together to make arbitrary and unilateral decisions about how a process should be implemented. Problems arise when these groups start prescribing the "how" and not just the "what". They start making technology decisions oh "how" things should be implemented, not necessarily understanding the technical ramifications.

    To be fair in enterprise most 'laterals' take ownership of the process and leave the implementation of that process to the consultants on the ground. They push down responsibility for the 'how', while maintaining the 'what', to those best placed to make those decisions. However in this case the testing lateral has maintained ownership of the 'how' as well. Although they inherited the Bug as Task anti-pattern from the choice of the Agile for MSF template, having ownership of the 'how' means that those implementing do not have the lateral to make changes. This often causes a certain amount of friction as the teams adopt the tool. Agile teams, or teams moving to agile, are often restricted by the bug as a task anti-pattern.

    ## Why is Bug as a Task an anti-pattern?

    Now you might be forgiven for coming down on the side of governance as 'we need some guidance to come from somewhere'. With many having organisational mandates to move towards agility, they are however unknowingly making decisions that are making the tooling a friction point for that move.

    ![image](images/image-3-3.png "image")
    { .post-img }

    These decisions are often the result of the traditional silo-based delivery experience that has them not avoiding the Bug as a Task anti-pattern in TFS. If you are doing all of the development before you test then this is a necessary configuration. However, as we know that this is inherently dysfunctional making this compromise is sweeping issues under the rug rather than dealing with them. So why is having a Bug in the same domain as a Task a bad thing?

    - **Not transparent** - if you do not have bugs on the backlog, then you no longer have a single unified, visible, and transparent list of things that you are working on. If you have two places to go, backlog and buglog, then how do you know priority? Suddenly you have two masters, the list of things hat the PO wants and the list of stuff we did not finish. In extreme circumstances, this buglog is not shown to the Product Owner. This is then getting pretty close to the line of fraud. Customers should always be fully aware of bugs.

    - **Promotes low quality** - it's a little weird, but in my experience, if you have a way to be pedantic, then people will use it. Provide a way for the teams not to communicate and you may very well find them trying to communicate through work items. Avoid this at all costs. They are not there as a crutch for poor team communication. They are there to provide long-term traceability and to support the team's efforts to maintain priority.

    - **No predictability of planning** - no really. If you have this many hidden bugs, your PO is not going to be able to understand what it takes to complete anything. If your bug rate is high then your technical debt is high and you are not really ever done. If you are not really done then you should get no points for that item.

    So if there are all of these downsides why would you want to have bugs as a task. Well, maybe I do want to hid my bugs from a customer, but apart from the nefarious reasons, I can think of only one good reason: low quality.

    ## Where does the Bug as a Task anti-pattern come from?

    If your software quality is so low that you repeatedly and consistently have many hundreds of small bugs then I can imagine the pain of always having to create one Bug and then one Task to fulfil that Bug. I see time and again teams creating PBI's called 'Bug Bucket' to hold the many smaller bugs so that the PO can prioritise them and so that they appear on the boards. If you are a product owner then do not put up with this! Ever! You should be able to prioritize one bug over another, or choose not to fix a bug at all. And if the bugs are too small for you to care about why are they not just fixed rather than spending time tracking and updating them?

    Now that we understand the problem, we can dive a little deeper. We have discussed the symptoms of the bugs as tasks anti-pattern above but what about some of the causes and by inversion, solutions?

    - **Poor communication** - If your testers and your coders don't talk then a lot of small things will drop through the cracks. Poor communication is a people problem and can only be solved there. Don't try to paper over the issue. Solve it...

    - **Testing outside of the sprint** - Even if your team has good communication if the testing is not happening at the same time as the coding then things will get left behind. Your coders wild not be in the same headspace as when they originally wrote it, and then the testers are re-testing out of context. Poo..

    - **No test first** - We need to have an idea of what we will be testing for before the developer begins to code. If you don't know enough to create at least one functional test then you should not be bringing it into the sprint. Think about it. The functional tests are the things that you use to measure whether the coder built what the customer asked for. If it is the measure you use to determine the correct outcome, then now can the coder ever be able to meet it without having it at the start? Make everyone's lives easier and eliminate rework by creating functional tests upfront. I often have to work with teams to help them understand what this looks like, however, they can very quickly get going with a little coaching.

    ## How do we deal with the Bug as a Task anti-pattern?

    While the solutions above will work, they need to be owned and implemented by the Product Owner and the Development Team working together. Make things open and transparent and push the problem down to the people causing it. Yes, the development teams should have to create a Bug at the PBI level and then break it down into Tasks. Yes, even if they have to create a Bug and a Single task for a 10-minute piece of work. This is their penalty for not communicating effectively. If the PO is not providing enough detail in the PBI then their penalty is having to prioritize tonnes of little meaningless bugs.

    ![image](images/image1-4-4.png "image")
    { .post-img }

    By forcing the teams to treat all bugs as backlog items you force them to take a long, hard look at when they create a bug and how they deal with unfinished in-sprint work.

    ## Conclusion

    Avoid the Bug as a Task anti-pattern in TFS at all costs. It promotes dysfunctional teams and will create friction for your teams that are doing agile. If you are still trying to [decide in the process template](http://nkdagility.com/agile-vs-scrum-process-templates-team-foundation-server/) or you have realised your mistake and [want to fix your process template](http://nkdagility.com/upgrading-your-process-template-from-msf-for-agile-4-to-visual-studio-scrum-2-x/) I have some posts to help. If you are on the right path, then awesome; however, resist all pressure to create Bug as a Task and focus instead on creating awesome agile requirements that include tests written upfront.
  FilePath: C:\Users\MartinHinshelwoodNKD\source\repos\NKDAgility.com\site\content\resources\blog\2014\2014-08-06-avoid-bug-task-anti-pattern-tfs\index.md
  FolderPath: C:\Users\MartinHinshelwoodNKD\source\repos\NKDAgility.com\site\content\resources\blog\2014\2014-08-06-avoid-bug-task-anti-pattern-tfs
- FrontMatter:
    title: Merge Team Projects into one in TFS
    description: Learn how to merge multiple Team Projects in TFS effectively. Discover tools, tips, and strategies to streamline your workflow and reduce complexity.
    ResourceId: -0YnGYCeikc
    ResourceType: blog
    ResourceContentOrigin: Human
    ResourceImport: true
    ResourceImportId: 10638
    ResourceImportSource: Wordpress
    ResourceImportOriginalSource: Wordpress
    date: 2014-07-30
    weight: 1000
    creator: Martin Hinshelwood
    layout: blog
    resourceTypes: blog
    slug: merge-many-team-projects-one-tfs
    aliases:
    - /resources/-0YnGYCeikc
    aliasesArchive:
    - /blog/merge-many-team-projects-one-tfs
    - /merge-many-team-projects-one-tfs
    - /merge-team-projects-into-one-in-tfs
    - /blog/merge-team-projects-into-one-in-tfs
    - /resources/blog/merge-many-team-projects-one-tfs
    tags:
    - Software Development
    - Install and Configuration
    - System Configuration
    categories:
    - Uncategorized
    preview: nakedalm-experts-visual-studio-alm-8-8.png
  BodyContent: |
    In TFS 2012 the product team introduced the concept of Teams into TFS. Before this many organisations created multiple Team Projects and now want to merge Team Projects into one, or at least fewer. There are many reasons you might have done this in the past but there is no reason to live with this.

    The simplest way to merge Team Projects is to create a new Team Project, add all of your teams and start from scratch. However for many organisations this sort of disruption is just infeasible and they would rather work with the dysfunctional and limiting layout rather than start again. For them there is another way. I will however warn you now… pain and suffering lies ahead if you choose to proceed.

    I am going to use the TFS Integration Tools to move consolidate Team Projects. You can use it to move Work Item and Source Control from one TFS server to another. I have used it to move work between collections, between team projects in the same collection, and to push data between TFS instances. Indeed I have used it to move TFS data to and from Visual Studio Online and a local TFS. While this tool is flexible it is difficult to use.

    Note If you try to move source from the same TFS Server back to itself you will run into many workspace issues. It is VERY hard to resolve this in the current tools.

    I would recommend that you do one or more dry runs for some of your more complicated code (branch and rename complicated) and see how it goes. If it's really hard you may need to give up and migrate without history or with limited history. If you like you can get a consultant in (hi) who may be able to do more, but often will hit the same issues. This post is to document some of the ways you can merge many TFS team projects into one and mitigate some of the pain.

    ## Installing TFS Integration Tools

    Make really sure you use the version from the [Visual Studio Gallery](http://visualstudiogallery.msdn.microsoft.com/eb77e739-c98c-4e36-9ead-fa115b27fefe) rather than the one Codeplex. While the Codeplex one is newer it is not supported by Microsoft. Always go the fully supported route.

    ![clip_image001](images/clip_image0011-1-1.png "clip_image001")
    { .post-img }

    When you run the installer it will ask for a SQL Server location. This SQL Server will be used to host the tfs_Integration database and really should be local to the server. Nothing slows this tool down like a network between you and the DB. I recommend installing [SQL Server Express](http://www.hanselman.com/blog/DownloadVisualStudioExpress.aspx) locally. You need to also make sure that you have at least one version of TFS Client API's installed. You will only be able to select adapters that have access to the relevant API. So if you need the TFS 2010 adapter then you should install the TFS 2010 API's.

    1. If you get an error when installing that you do not have Team Explorer when you do you likely installed just Team Explorer and not full Visual Studio. Unfortunately there is a bug in the Integration Tools that prevent it from detecting it. Same the following code as a .reg file and double click to solve your issue.

       ```
        Windows Registry Editor Version 5.00
       [HKEY_LOCAL_MACHINE\SOFTWARE\Wow6432Node\Microsoft\VisualStudio\11.0\InstalledProducts\Team System Tools for Developers]
       @="#101"
       "LogoID"="#100"
       "Package"="{97d9322b-672f-42ab-b3cb-ca27aaedf09d}"
       "ProductDetails"="#102"
       "UseVsProductID"=dword:00000001

       ```

    2. Do not install the 'service' option. If you do the Integration Tools get installed in a move that will only use that rather than self-hosting. It is better to do manual runs with the tool window open. Better for debugging as well.

    ## Creating your Configuration

    Once you have the TFS Integration Tools installed and you open them for the first time you will need to create a new configuration. This is a large XML format definition that sets all of the properties and settings for the migration. You will do a lot of editing of the XML directly as there is no nice UI for most of the cool and important stuff.

    To create your first configuration you use the "Create New" link from the left navigation. This will pop a template selection box. All of the templates are xml files stored on disk. Once you have selected a template, the one below is the Work Item Only template.

    Your first task will be to configure both the source and the destination. Here you select the adapter for the left and right sources. If you do not have the adapter you want listed please refer to 'pain mitigation #2' below.

    ![clip_image002](images/clip_image0021-2-2.png "clip_image002")
    { .post-img }

    Above I have configures both the left and the right source to be different Team Projects but within the same collection. Now, as I am moving between team projects it is possible that I could have the Scrum template on one and the CMMI template on another. While you can create a complex mapping file between the template, and I have had to do this many a time, you should try to avoid it. Use pain mitigation #3.

    You can create some pretty complex migrations even within the bounds above. But let's look at some of the custom configurations that matter here.

    1. If you don’t have the desired source and destination version of TFS you may have neglected to install the appropriate version of Team Explorer.
    2. If you don't see a TFS 2013 adapter listed even though you have Team Explorer 2013 installed don't panic. Install Team Explorer 2012 and use the TFS 11 adapter. I am trying to get MSFT to \[open source the TFS Integration Tools\] so that I and others can fix these issues but so far they have not been forthcoming.
    3. You should get the source and destination Work Item Type definitions into sync. This means that you may need to [migrate your process template](http://nkdagility.com/upgrading-your-process-template-from-msf-for-agile-4-to-visual-studio-scrum-2-x/). While not required it makes things much easier.

    ### Creating a custom configuration for work item tracking

    The first thing to configure is work item tracking. I would recommend that you always pic "continuous manual" rather than the default of "One-way" migration as continuous manual lets you complete a migration, have folks check it out and then push any changes they have made in the interim. As long as changes are not being made in both places this works great.

    The other major configuration I recommend is to enable bypass rules. Effectively the TFS Integration Tools work in two modes. The first is through the API's. If you are using the API you have to abide by all of the rules of work items. So all of the Work items that you try and save MUST be valid. This can be difficult if there are any differences between the source and the destination process. Remember that we are pushing history so all revision values must be valid. If we ever upgraded our process template the old values are still in there. Remember we are also migrating the revisions. For this we need to be able to bypass the rules and this can only be done through accessing the web services directly. While not strictly supported the TFS Product Team made a special case of the TFS Integration Tools.

    If you add a custom setting of EnableBypassRuleDataSubmission and set it to 'true' like above you will enable this ability.

    The next biggest thing is the ability to create composite or aggregate mappings. Specifically for the Area and Iteration paths. If I am merging many existing team propjets then where I had an area path of "\\MyArea" I want that to be translated to "\\TeamProjectA\\MyArea". I may also want to add other fields into the area path and this gives me that ability. When you get to the mapping of fields as you see above you need to add and Aggregated Fields section under your Field Maps one.

    Here I am making sure that I do not have conflicts with the data by placing all migrated data under a new node called "TeamProjectA". You may want to do more specific field and value mappings but this is where I always start and is mostly good enough.

    The only other non-standard thing I am doing here is that I am moving from a Team Project that does not use \[Team Field\] to one that does.

    For that my field map contains some values, specifically my new Company.Team field, that only need to exist in the new template. I literally do a \* to "OldTeamProject" mapping. To do this you use @@MISSINGFIELD@@ to tell the integration tools not to go looking for it on the left.

    We can then have a simple, and out only, value map of everything to "OldTeamProject".

    ![clip_image003](images/clip_image0032-3-3.png "clip_image003")
    { .post-img }

    If you are only configuring Work items then you can click start and execute the migration. Note that you can't delete work items per say. So once you migration you are done with no do-over. Technically you can use the Power Tools to delete one work item at a time however that is a little bit cumbersome if you have just pushed 30k work items and need to delete them. To help out I created a command line tool to [delete work items from TFS or VSO](http://nkdagility.com/delete-work-items-tfs-vso/).

    ![clip_image004](images/clip_image0042-4-4.png "clip_image004")
    { .post-img }

    Again I have done tones of migrations and consolidations this way and while it is never what you might call 'fun' it can and does do the job. The results can be mixed but if you persevere and learn the tool you can make magic.

    Note: I would only recommend this for more than.. Say… 1000 work items to migrate. Less than 1000 you should consider a flat Excel migration.

    ![clip_image005](images/clip_image0051-5-5.png "clip_image005")
    { .post-img }

    I currently have 14 teams that have all migrated into a single team project. Some of those teams were already in TFS and needed to come across into a single Team Project. Others had [work items in Excel or SharePoint](http://nkdagility.com/import-excel-data-into-tfs-with-history/) or Quality Centre.

    1. At this point, if you have enabled the bypass rules switch you will need to add the account that the TFS Integration tools are running under to the "Service Accounts" group on your TFS Server. You can do this through the [tfsecurity command line](http://nkdagility.com/tfs-integration-tools-issue-tfs-wit-bypass-rule-submission-is-enabled/). No, just giving the users the "on behalf of others" permission is not enough as the TFS Integration Tools check that specific group on the server. You will also need to add the account to both ends, source and destination servers if they are different.
    2. Practice, practice, practice. Use a separate collection or even a complete test instance of TFS to run, re-run, and run again the migration to make sure the end result is what you want. You can use a query to scope the dry runs if you have many work items.

       The filter above is for everything under the Team Project but you can use any WIQL you like. If you don’t know WIQL you can create a query in Team Explorer and "Save as" a local XMLO file then nick the contents.

    3. I usually create an area called "NewTeamProject\\\_Delete". If I have an unsucessfull migration in production I move all of the work items into this location. I can then use the API in either C#, VB, or PowerShell to load all work items under that Area Path and for each one call WorkItemStore.DeleteWorkItem(id). There is a command line tool for calling this but you need to log onto the TFS server to use it and I find this way quicker.
    4. If you have Test Cases in your migration and they have Shared Test Steps then the link gets screwy. Devesh Nagpal from the product team has [a command line tool to fix the broken links](http://blogs.msdn.com/b/broken_shared_steps_link_after_migration_from_tfs_integration_platform/archive/2012/11/05/broken-shared-steps-link-after-migration-from-tfs-integration-platform.aspx) after the migration.

    ### Creating a custom configuration for source control

    Most of the time a Source migration is pretty strait forward. However there are quite a few things that are supported in Source Control that gives the Integration Platform fits. One is large check-ins. If you have a point in time when someone did a bulk check-in of many thousands of files you may want to run a partial migration to that point and then manual deal with the issue before continuing. The other option is out of memory exceptions. Another is a complex interweave of branches. If you have branches within branches within branches then like as not you are going to have to leave history behind. If you encounter an ItemNotFoundException you may have found some spaghetti branching that you never knew was there.

    On option we have, all being well, when you do a migration is to rearrange you source on the way in. If you want to try this you should do lots of practice somewhere you can't do any damage. This can mess up quick and can be traumatic.

    [Willy-Peter](http://blogs.msdn.com/b/willy-peter_schaub/) has a perfect rule of thumb when migrating source and contemplating how long it will take:

    It will take about as long as it took to check in in the first place.

    That can be a considerable length of time if you have a lot of check-ins, however for most teams you can scope a large codebase down to individual applications to make things a little quicker.

    Note Really you should do everything in you power to convince folks that they just need the 'tip'. No-one, really needs history.

    ![clip_image006](images/clip_image0061-6-6.png "clip_image006")
    { .post-img }

    In order to do a migration you have to add mapping like you can see above to the list. In this case I am trying (I failed by the way, with the ItemNotFoundException exception I mentioned) to change the layout of the source. For some reason many applications and branches ended up under the R1.0 folder on the left and we really want each application to have a R1 folder. I have done this before successfully but unfortunately this set of source is managed and worked on by 6 ALM consultants that think that they are smart (yes, I am in there too) and thus the migration failed. Sometime that’s just tough and you have to find another way forward. In the case of this source I just repeated it without the multi-mapping.

    ![clip_image007](images/clip_image0071-7-7.png "clip_image007")
    { .post-img }

    When you migrate your source and work items together the Integration Platform will maintain the relationships between the code and work. This can be invaluable and is worth maintaining if at all possible.

    Let's take a little look at the configuration file and the key elements that matter.

    Here we have the basic mapping; a source folder from your left source to a folder on the right source. Because we are mapping from many team projects to our new uber team project you will likely want to have the old team project name as a sub folkder of the new team project. Above we are creating that mapping.

    1. If you are mapping Source between two team projects within the same collection then expect some pain and suffering with workspace collisions. Patience is the only way to solve this one…some magic fairy dust would not go amiss either.

    You may have noticed the "Neglect" attribute. Well it’s a little reverse sociology and can be translated as "!Cloak". So "true" would therefore men that the folders should be clocked from the migration. This can be handy if there is a subfolder that you don’t want or you run into issues with a particular folder structure.

    ## Conclusion

    And that’s really all there is to it. Don’t expect to get a successful migration the first time. Or the second, or even the third. But if you persevere you can do many migrations quickly. I have [migrated 20-30 small projects](http://nkdagility.com/one-team-project-collection-to-rule-them-allconsolidating-team-projects/) into one in only a few days, however I was luckily with the low complexity and small check-ins.

    Go fourth and consolidate your Team Projects….
  FilePath: C:\Users\MartinHinshelwoodNKD\source\repos\NKDAgility.com\site\content\resources\blog\2014\2014-07-30-merge-many-team-projects-one-tfs\index.md
  FolderPath: C:\Users\MartinHinshelwoodNKD\source\repos\NKDAgility.com\site\content\resources\blog\2014\2014-07-30-merge-many-team-projects-one-tfs
- FrontMatter:
    title: Maven release perform tries to do a Get to a workspace sub folder in TFS
    description: Discover how to resolve Maven release issues with TFS when switching from SVN. Learn best practices for a smoother build and release process.
    ResourceId: yXZtGRWsXfS
    ResourceType: blog
    ResourceContentOrigin: Human
    ResourceImport: true
    ResourceImportId: 10620
    ResourceImportSource: Wordpress
    ResourceImportOriginalSource: Wordpress
    date: 2014-07-23
    weight: 790
    creator: Martin Hinshelwood
    layout: blog
    resourceTypes: blog
    slug: maven-release-perform-tries-get-workspace-sub-folder-tfs
    aliases:
    - /resources/yXZtGRWsXfS
    aliasesArchive:
    - /blog/maven-release-perform-tries-get-workspace-sub-folder-tfs
    - /maven-release-perform-tries-get-workspace-sub-folder-tfs
    - /maven-release-perform-tries-to-do-a-get-to-a-workspace-sub-folder-in-tfs
    - /blog/maven-release-perform-tries-to-do-a-get-to-a-workspace-sub-folder-in-tfs
    - /resources/blog/maven-release-perform-tries-get-workspace-sub-folder-tfs
    tags:
    - Troubleshooting
    - Software Development
    categories:
    - Uncategorized
    preview: naked-alm-jenkins-logo-2-2.png
  BodyContent: |
    If you are using TFS and specifically switching from SVN to TFS then you might run into the issue that your Maven release perform tries to do a Get to a workspace sub folder. This will not work as TFS has a validation exception to trying to map a sub folder inside an existing workspace. That could be disastrous in a real situation.

    Could you imagine if, while in the middle of a compile, another build kicked off and overwrite your files? Worse if that other build was executing an only slightly different version of your codebase and it created a successful compile of goodness knows what disaster waiting to happen. (shiver.)

    To be honest I am not really a fan of the whole compile and release in one tool. The problem is that if I have just deployed and validated a version of the code I want that version deployed. Not a new one, which is exactly what I get it if I compile again. Compiled output always have meta data associated with them and that changes every time you build. So your output, while likely extremely code to your previous compile if the code has changes will always be different. No sensible QA manager would ever sign of on that. A more secure and consistent deployment can be gained by \[creating a binary release pipeline\] that starts at DEV.

    Anyway, this issue was about the maven release perform stage where my build that I am trying to configure to use TFS tries to do a GET of the label it just created (what value there is in this I do not know) to a sub folder of the current workspace. This does not error, but does result in a missing folder and thus a missing .pom file.

    ```
    [INFO] Scheme - https
    [INFO] files: /appl/data/ci-test/jenkins/jobs/TFS-TestProject/workspace
    [INFO] Command line - /bin/sh -c cd /appl/data/ci-test/jenkins/jobs/TFS-TestProject/workspace && tf checkin -login:hinshelwoodmjh,****** -noprompt '-comment:[maven-release-plugin] prepare for next development iteration' /appl/data/ci-test/jenkins/jobs/TFS-TestProject/workspace/pom.xml /appl/data/ci-test/jenkins/jobs/TFS-TestProject/workspace/TestProjectLibrary/pom.xml /appl/data/ci-test/jenkins/jobs/TFS-TestProject/workspace/TestProjectWeb/pom.xml /appl/data/ci-test/jenkins/jobs/TFS-TestProject/workspace/TestProjectEar/pom.xml /appl/data/ci-test/jenkins/jobs/TFS-TestProject/workspace/TestProjectDistribution/pom.xml
    [INFO] err -
    [INFO] Release preparation complete.
    [INFO]
    [INFO] --- maven-release-plugin:2.5:perform (default-cli) @ TestProject ---
    [INFO] Checking out the project to perform the release ...
    [INFO] scmUrl - https://tfs.comapny.com/tfs/DefaultCollection::$/MainProject/VisualStudioALM/JavaTestProject
    [INFO] Scheme - https
    [INFO] Command line - /bin/sh -c cd /appl/data/ci-test/jenkins/jobs/TFS-TestProject/workspace/target/checkout && tf get -login:hinshelwoodmjh,********** -recursive -force -version:LTestProject-1.4.10 /appl/data/ci-test/jenkins/jobs/TFS-TestProject/workspace/target/checkout
    [INFO] err -
    [INFO] Executing goals 'javadoc:jar deploy'...
    [INFO] [INFO] Scanning for projects...
    [INFO] [INFO] ------------------------------------------------------------------------
    [INFO] [INFO] BUILD FAILURE
    [INFO] [INFO] ------------------------------------------------------------------------
    [INFO] [INFO] Total time: 0.100s
    [INFO] [INFO] Finished at: Fri May 23 14:42:40 CEST 2014
    [INFO] [INFO] Final Memory: 5M/10M
    [INFO] [INFO] ------------------------------------------------------------------------
    [INFO] [ERROR] The goal you specified requires a project to execute but there is no POM in this directory (/appl/data/ci-test/jenkins/jobs/TFS-TestProject/workspace/target/checkout). Please verify you invoked Maven from the correct directory. -> [Help 1]
    [INFO] [ERROR]
    [INFO] [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
    [INFO] [ERROR] Re-run Maven using the -X switch to enable full debug logging.
    [INFO] [ERROR]
    [INFO] [ERROR] For more information about the errors and possible solutions, please read the following articles:
    [INFO] [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MissingProjectException
    [INFO] ------------------------------------------------------------------------
    [INFO] Reactor Summary:
    [INFO]
    [INFO] Test Project ...................................... FAILURE [42.119s]
    [INFO] Test Project - Common Library ..................... SKIPPED
    [INFO] Test project - Web Application .................... SKIPPED
    [INFO] Test Project - Enterprise Application for web application  SKIPPED
    [INFO] Test project - Distribution project ............... SKIPPED
    [INFO] ------------------------------------------------------------------------
    [INFO] BUILD FAILURE
    [INFO] ------------------------------------------------------------------------
    [INFO] Total time: 43.130s
    [INFO] Finished at: Fri May 23 14:42:40 CEST 2014
    [INFO] Final Memory: 14M/31M
    [INFO] ------------------------------------------------------------------------
    ```

    This understandably results in a failed build/release. Not only did this one take a while to understand I had to consult with an awesome Java expert in the form of [Corstijan Kortsmit](http://nl.linkedin.com/pub/corstijan-kortsmit/74/ba2/a9b/en) who identified the solution in about three seconds. It was one of those "Oh… that… you just…" sort of answers.

    ![clip_image001](images/clip_image001-1-1.png "clip_image001")
    { .post-img }

    If you head over to the configuration of your build and look for a section entitled "Build Environment". Here there are two important sections. We have a couple of boxs to put additional command line parameters for the maven release plugin. We need to add at least a "-DworkingDirectory" parameter with a specific folder for where to do the get.

    \-Dresume=false release:prepare release:perform -DworkingDirectory=/appl/data/temprelease -Dgoals=clean install

    The additional issues to think about are around multiple users as well as multiple builds working simultaneously and multiple users.

    Multiple builds is solved by adding parameters for something unique to the instance. In Team Build each 'Agent' can only run one build at a time but you can run multiple agents on a single server. Each agent gets a unique ID and by default TFS created a distinct workspace for each instance that can be shared across instances of the same build. This makes the second build potentially faster on any given agent as you don’t need to always clean workspaces as you can sync them. Here I could not see an agent ID so we went with the Build Instance ID. The down side to this is that you have to do a full get for every build and it might be slower.

    Multiple users is an easier issue to solve. We added pre-build commands to create a workspace in the folder above and we made it 'public' meaning that any user can use it. By default a workspace is created as 'private' and another user can't use that folder. With a 'public' workspace we can have the first user create it and then reuse it time again for other users.

    This got the build working. Our only outstanding issue now is that build from SVN have a Tag created. In TFS this is done as a label, however labels are mutable. They can be changed after the fact with no audit record. We will likely solve this by creating a read-only branch instead of a label.

    Let me know how you get on with your migrations to TFS.
  FilePath: C:\Users\MartinHinshelwoodNKD\source\repos\NKDAgility.com\site\content\resources\blog\2014\2014-07-23-maven-release-perform-tries-get-workspace-sub-folder-tfs\index.md
  FolderPath: C:\Users\MartinHinshelwoodNKD\source\repos\NKDAgility.com\site\content\resources\blog\2014\2014-07-23-maven-release-perform-tries-get-workspace-sub-folder-tfs
- FrontMatter:
    title: Avoid the pick-n-mix branching anti-pattern
    description: Discover how to avoid the pick-n-mix branching anti-pattern in software development. Learn effective strategies to enhance code quality and team trust.
    ResourceId: IVl4r4gpkQ6
    ResourceType: blog
    ResourceContentOrigin: Human
    ResourceImport: true
    ResourceImportId: 10649
    ResourceImportSource: Wordpress
    ResourceImportOriginalSource: Wordpress
    date: 2014-07-14
    weight: 205
    creator: Martin Hinshelwood
    layout: blog
    resourceTypes: blog
    slug: avoid-pick-n-mix-branching-anti-pattern
    aliases:
    - /resources/IVl4r4gpkQ6
    aliasesArchive:
    - /blog/avoid-pick-n-mix-branching-anti-pattern
    - /avoid-pick-n-mix-branching-anti-pattern
    - /avoid-the-pick-n-mix-branching-anti-pattern
    - /blog/avoid-the-pick-n-mix-branching-anti-pattern
    - /resources/blog/avoid-pick-n-mix-branching-anti-pattern
    tags:
    - Pragmatic Thinking
    - Release Management
    - Technical Excellence
    - Technical Mastery
    - Modern Source Control
    - Operational Practices
    - Software Development
    - Engineering Practices
    - Product Delivery
    - Technical Debt
    - Working Software
    - Continuous Delivery
    - Troubleshooting
    categories:
    - Engineering Excellence
    preview: nakedalm-experts-visual-studio-alm-4-4.png
  BodyContent: |
    For the last few days I have been working with a customer in the UK on a grass roots engagement to help them solve their source control issues. They have ended up with a pick-n-mix branching anti-pattern and could not see the way out.

    The pick-n-mix branching anti-pattern is something I encounter quite often and on the surface it looks like it should work and give the business flexibility. However the real result is lost code, failed releases, technical debt, unhappy customers, and an erosion of trust with the business.

    For the last few release they had noticed that they had been missing things from the release and this had resulted in some major bugs and egg of the face of IT. This is obviously not acceptable and they were looking for advice on how to move forward. There current source code management model was not working for them.

    ![clip_image001](images/clip_image0011-1-1.jpg "clip_image001")
    { .post-img }

    In their current model they were doing something that I like to call pick-n-mix branching. In the diagram above you can see the pick-n-mix branching anti-pattern where in scenario #1 we are leaving behind some of the changeless. However in scenario #2 we are taking a couple of changeless that are dependant on some things that were left behind. This may result in a compile time error, or worse in the case of a website or some asset types. What if these were SQL changes... How hard would it be to debug why the development code line works and Test does not. Worse again when you compound the result and this happens over many years it may work out for a while but even just a year down the line how much code is there in development that was never pushed to test. And how to you know which is which when you are adding new features to development?

    The pick-n-mix branching anti-pattern is a symptom of poor planning and almost always results in a code promotion model. In a code promotion model you are promoting source code from Development->Test->Production. This has the unfortunate consequence of complicating your quality assurance. You do not test code, you test the binaries that are a result of the compilation process. You might have clever packaging but ultimately you are releasing those binaries. However the reality is that if you create new binaries you are going to need to test them all over again to get any kind of assurance. Even without pick-n-mix code promotion is not the best of ideas but with! Now we are easily in a world of hurt.

    On small projects with few developer this can work. I even had a customer that create a software tool around this so that they could scale. However this is dysfunctional behaviour and results in a number of issues:

    - **Technical Debt (or an un-hedged fund)** - More and more code will ley unfinished in the Development branch which results in an inability to have 100% working software every build.
    - **Merge Chaos** - Over time the difference between development and production will increase leaving developers with less and less idea of what dependencies they can take. This results in total chaos at merge time, and it will only get worse as time goes on.
    - **Retest required** - if you are creating new binaries then you have to re-execute all of your tests even if you think nothing has changed. You have just created a brand new version of your application.
    - **Business Trust** - As your software erodes you will more often have failed rollouts and more major bugs in production. The result is a total lack of trust in your ability to deliver by your business. This often results in a crazy and convoluted gated model for deployments with loads of ass-covering paperwork.

    The pick-n-mix branching anti-pattern is used as a crutch for poor planning. The solution for this is a single branch line and using feature flippers (toggles) to determine what code is run. However this requires engineering work and cannot be turned on with a switch. It will take some time, often a considerable amount of time, to engineer into the product. The only way to mitigate this in the short term is to switch to a Feature branching model.

    ![clip_image002](images/clip_image0021-2-2.png "clip_image002")
    { .post-img }

    In a Feature branching model we have single main (or trunk) line with multiple branches for each feature. Features are one or more things that will ship together. There will be no pick-n-mix and everything on that feature line will merge to the 'main' line at the same time. This will require a little more planning as you need to separate your features early. And a little more coordination from your management to make features distinct. This however gives you the capability to continue to release some features and not others without changing your code too much.

    There are still a few downsides:

    - **Multiple instances** - we need an instance of our application per feature branch. This can be expensive and time consuming.
    - **Break in binary pipeline** - as we are still on a separate branch and promoting code we can't have a seamless binary promotion model. We can however have one per feature and then an end-to-end one pipeline for Integration

    In an ideal world you need to change your code, using agile engineering practices, to allow you to be able to turn any feature off at any point in the release cycle. This would allow you to not ship a new feature as late as the day of release (or later) without having to rebuild, change a single line of code, or run your full regression again. When you are able to do this then you can move to a more advanced branching model where you maintain either a single source line, or for business reasons you may have a step structure for major releases.

    ![clip_image003](images/clip_image0031-3-3.png "clip_image003")
    { .post-img }

    In the step model you go back and fix bugs on the oldest version that you support that has the bug. You can then push forward from parent to child in the clear safety of never having to choose between bugs and new features.

    When you are in the zone with the need for withholding features between major versions for marketing reasons then you may need to work on separate branch lines. This is most effectively solved by using a step, or branch by release, model. This is not a branch for every bug fix or small change but instead is relegated only for the major changes. This allows your branch line to move through various states at different times depending on the criteria that you set. You may still have the idea of 'dev', 'test', and 'prod'. However the difference between the states is likely only one of rules-of-engagement.

    However this is still just feature branching except we are looking at major releases. As you organisation realises the power of continuous delivery and the speed with which it can get function to market they will start to change and accept the reality. When that happens you can move to the most optimal branching model of all. The single line.

    ## Conclusion

    If you are currently in a pick-n-mix branching model you need to move immediately to a Feature branching model to reduce risk and promote stability. It may take a while to get to this point but it will make your life easier and allow you to start building more trust with your business as you are more likely to be successful with releasing. At this point you can move to binary promotion on your main line to reduce your test matrix.

    Once you are there you should look at implementing the engineering necessary in your product to implement Feature flippers to whatever level of granularity makes sense in your application. This may be by feature, user, or account and should provide you with the flexibility that your business require and is willing to pay for. Now we can move towards true binary promotion from our development phase all the way through to production.

    Even while in a single branching model with binary promotion you may find the need to have a stepped model where you need to support multiple versions of your product. This can be achieved without crippling your teams by having your branching flow forward from parent to child as you move through major releases of your software.

    Ultimately there is no excuse for using the pick-n-mix branching anti-pattern for branching. Step up, be professional, and fix this one for good.
  FilePath: C:\Users\MartinHinshelwoodNKD\source\repos\NKDAgility.com\site\content\resources\blog\2014\2014-07-14-avoid-pick-n-mix-branching-anti-pattern\index.md
  FolderPath: C:\Users\MartinHinshelwoodNKD\source\repos\NKDAgility.com\site\content\resources\blog\2014\2014-07-14-avoid-pick-n-mix-branching-anti-pattern
- FrontMatter:
    title: The value of an independent Scotland for me
    description: Explore the personal value of an independent Scotland through self-determination, economic control, and a brighter future for generations. Join the debate!
    ResourceId: kmgdXtKVzGS
    ResourceType: blog
    ResourceContentOrigin: Human
    ResourceImport: true
    ResourceImportId: 10655
    ResourceImportSource: Wordpress
    ResourceImportOriginalSource: Wordpress
    date: 2014-07-13
    weight: 1000
    creator: Martin Hinshelwood
    layout: blog
    resourceTypes: blog
    slug: the-value-of-an-independent-scotland
    aliases:
    - /resources/kmgdXtKVzGS
    aliasesArchive:
    - /blog/the-value-of-an-independent-scotland
    - /the-value-of-an-independent-scotland
    - /the-value-of-an-independent-scotland-for-me
    - /blog/the-value-of-an-independent-scotland-for-me
    - /resources/blog/the-value-of-an-independent-scotland
    tags: []
    categories:
    - Uncategorized
    preview: metro-yes-scotland-128-link-1-1.png
  BodyContent: |
    For the last wee while some of you may have noticed some news coverage about Scottish independence. This post is about the value of an independent Scotland for me, so if you are not interested in politics then move along. It will all be over in a few months and you will likely never have to hear about it outside of history again.

    If you follow me on Facebook or twitter then you will have seen a plethora of posts about this subject. I have mostly kept this subject away from my company except for a small post about donating to Yes Scotland. I think that you can guess which side I come down on. However, there is a nefarious dynamic to the debate.

    ![Scottish-independence-6348782](images/Scottish-independence-6348782-2-2.jpg "Scottish-independence-6348782")
    { .post-img }

    I have been appalled by the bias of the BBC and the mainstream media in general. Steven Borg warned me that my faith in the BBC was misplaced but I failed to listen. I am listening now, and even the BBC's own internal regulatory body has sanctioned it more than once for its position. Indeed, as we enter the final 200 days, there are tight rules forcing all media to be impartial. We can already see many that are flouting the rules. If its negative for the Independence debate, then it gets plenty of airtime, if its positive, it gets ignored.

    ![Screen%20shot%202014-03-27%20at%2010_38_39](images/Screen20shot202014-03-2720at2010_38_39-3-3.png "Screen%20shot%202014-03-27%20at%2010_38_39")
    { .post-img }

    Just look for coverage of the circa 2000 people who protested outside the BBC, or the [fake reports of 34,000 people turning up for Armed Forces Day](http://wingsoverscotland.com/conflicting-reports/)! Now that was a farce.

    ## The value proposition

    For me, the question of independence is one of value. Value for me and for my children and their children. This is not a decision that I take on the basis of short term financial gain / or loss, nor even one of the wants and needs of my ancestors. I believe that Scotland should be an independent country. I believe that there is more value in being independent and being able to control our treaties with everyone, including England.

    Some key issues for me:

    1. **Self-determination** - I currently look at countries like Iceland, Norway, and even Ireland that are doing very well as small countries. Go and ask them if they would rather secede their self determination to another power and see how they would respond. Would Norway re-join with Sweden? Would New Zealand use its legal right to join with the Australian territories? Would the USA re-join the British union it has suggested is a good thing? No... None of them would. Even a US state has more tax raising and varying powers than Hollyrood has in Scotland .
    2. **Scottish money spent in Scotland** - I work hard all over Europe and I pay taxes in Scotland. Those taxes are sent to Westminster and then a portion, that they determine, is allocated back to Scotland. Some of that money is spent on Trident and some is spent on HS2. Hell, some of that money is spent on Schools in London and Hospitals in Birmingham. Now while I am happy for them to get money, there is no value to me of a hospital in Birmingham or a School in London. What value is it for me or my kin to have a high speed rail link between London and Manchester? Not much I would wager.
    3. **Less of a target** - Both on the geo-political level and the "war on terror" Britain's foreign policy, which is closely related to US policy, has made Scotland a target. Luckily for us [Al-Qaeda sent there dumbest guys our way](http://www.youtube.com/watch?v=7gMJBQoHJ4E), but they did send them. Even if I was willing to accept that this was just the cost of doing business in the modern world, I would draw the line at Westminster basing all of its [nuclear arsenal 20 miles outside of Glasgow](http://wingsoverscotland.com/map-ref-55n-5w/)! So, not just a target of modern warfare, but old school warfare as well. You never hear of Al-Qaeda saying "those damn Norwegians".
    4. **More Immigration** - Scotland will not survive without an even higher immigration level than we have now. Its just reality that we are a small country with a small but aging population. Many immigrants come to our country and work for many years paying into the state pensions. Those folks pay taxes, and then when they become eligible for a pension, they move back to their country of origin with their nest egg. This leaves a ton of money that they paid into the pension. More immigrants please.
    5. **Get a better deal** - Right now, if Scotland has utility or services that other parts of the UK wants, then it is negotiated by Westminster in favour of Westminster. This is an inherent conflict of interest, and even though the same can be said of Hollyrood, at least it will be Scots taking advantage of Scots and not another country taking that advantage. Its easier to sack our own.

    ## Into the unknown

    Yes, the future is unknown. In fact, I would suggest that I have a much greater chance of knowing what the future holds with iScotland than with rUK. Both the Scottish government and the Westminster parties have laid down what they believe the future will look like. It is, however, just a prediction as no one can know what is around the corner.

    If you take all of the scare stories out of the picture what will change? What do we know for sure?

    - **In an independent Scotland (Yes)** - Well, we will not have a Tory government. We will have a government that we voted for. We will likely have more tax to pay for the higher level of services demanded by the Scottish people. We will likely still have a public NHS. We will have free higher education and prescriptions. We will be in the EU, and naval boats will continue to be built in iScotland. Oh and fracking will be banned, and a similar points based immigration system to Australia implemented.
    - **In UK (No)** - Privatisation of the NHS is already underway in England, so Scotland would likely follow. We will have more austerity measures. Free prescriptions and free higher education will end. Immigration, so sorely needed to maintain the Scottish economy, will be reduced. Fracking has already been allowed, and £30 billion to be allocated to replace Trident. In short there will be less investment in out future.

    How about a little exercise?

    - **If yer Scottish** - Lets assume that in the long run shit always works out which ever way we decide in September. Take your current knowledge of both Westminster and the Scottish government positions. I want you to imagine a future world where you are bouncing your grand kids on your knee. Where would you rather be? iScotland or rUK?
    - **If your American** - This is [not the same as North Dakota separating from the USA](http://wingsoverscotland.com/an-actual-letter-from-america/) and If, as your President recently attested, the Union looks to be working and should stick together. Why don't you give up your sovereignty and re-join the Union? Indeed, why did you leave the Union in the first place 100 years after it was formed?

    I get asked, time and again, by both Europeans and Americans, which way I think it will go. While I support [Yes Scotland](http://www.yesscotland.com) (with money not just with platitudes and 'likes') I really do not know. The polls are so close, 3-5 points in it, with a large group of undecided and it is really hard to tell. This is a contest that will go to the wire, and have both sides pulling out the stops as we get closer to a result. It will either be a vista of democratic beauty or a brawl of epic proportions. I know what I am expecting, and we all need to be aware of the dangers ahead as well as the values.

    I don't expect it to be pretty, but I am looking forward to Alex Salmond, the First Minister of Scotland, debating Alistair Darling, the chairman on the No campaign. Whatever happens on September 18th there will be a lot of unhappy people above and below the border.
  FilePath: C:\Users\MartinHinshelwoodNKD\source\repos\NKDAgility.com\site\content\resources\blog\2014\2014-07-13-the-value-of-an-independent-scotland\index.md
  FolderPath: C:\Users\MartinHinshelwoodNKD\source\repos\NKDAgility.com\site\content\resources\blog\2014\2014-07-13-the-value-of-an-independent-scotland
- FrontMatter:
    title: Maven release prepare fails with detected changes in Jenkins
    description: Discover how to resolve Maven release prepare failures in Jenkins due to detected changes. Learn to use .tfignore for smoother TFS integration.
    ResourceId: gtoRjWgSmKe
    ResourceType: blog
    ResourceContentOrigin: Human
    ResourceImport: true
    ResourceImportId: 10579
    ResourceImportSource: Wordpress
    ResourceImportOriginalSource: Wordpress
    date: 2014-07-09
    weight: 690
    creator: Martin Hinshelwood
    layout: blog
    resourceTypes: blog
    slug: maven-release-prepare-fails-with-detected-changes-in-jenkins
    aliases:
    - /resources/gtoRjWgSmKe
    aliasesArchive:
    - /blog/maven-release-prepare-fails-with-detected-changes-in-jenkins
    - /maven-release-prepare-fails-with-detected-changes-in-jenkins
    - /resources/blog/maven-release-prepare-fails-with-detected-changes-in-jenkins
    tags:
    - Troubleshooting
    - Software Development
    categories:
    - Uncategorized
    preview: naked-alm-jenkins-logo-9-9.png
  BodyContent: |
    If you are using Team Explorer Everywhere 2012 or 2013 your Maven release prepare fails with detected changes, however it worked when you were using SVN.

    As you may have noticed I have had a few posts on Jenkins integration with TFS recently. My current customer is migrating away from SVN and Jenkins to TFS 2012 to take advantage of the cool ALM feature however we need to stage in, taking one thing at a time. They have quite a few builds in Jenkins and moving them will take time. The idea is that we can move all of the source over and it is a fairly simple process to re-point Jenkins and Maven to TFS. This allows the teams to take advantage of relating their Source and Work Item while allowing us to create parallel builds and validate the output.

    [![image[2]](images/image2_thumb-3-3.png "image[2]")](http://nkdagility.com/wp-content/uploads/2014/06/image2-4-4.png)
    { .post-img }

    Our initial problem was around [Configuring Jenkins to talk to TFS 2013](http://nkdagility.com/configuring-jenkins-talk-tfs-2013/) and then [Mask password in Jenkins when calling TEE](http://nkdagility.com/mask-password-in-jenkins-when-calling-tee/). As with all migration projects you get past one problem and get hit by another. The next issue was that the Release builds would always fail. Looking at the logs it is obvious why.

    ```
    [INFO] Command line - /bin/sh -c cd /appl/data/ci-test/jenkins/jobs/TFS-TestProject/workspace && tf status -login:username,********** -recursive -format:detailed '$/main/VisualStudioALM/JavaTestProject'
    [DEBUG] line -
    [DEBUG] line --------------------------------------------------------------------------------
    [DEBUG] line -Detected Changes:
    [DEBUG] line --------------------------------------------------------------------------------
    [DEBUG] line -$/main/VisualStudioALM/JavaTestProject/release.properties
    [DEBUG] line -  User:       Martin Hinshelwood (MrHinsh)
    [DEBUG] line -  Date:       22-May-2014 14:33:52
    [DEBUG] line -  Lock:       none
    [DEBUG] line -  Change:     add
    [DEBUG] line -  Workspace:  Hudson-TFS-TestProject-MASTER
    [DEBUG] line -  Local item: [zsts490716.eu.company.com] /appl/data/ci-test/jenkins/jobs/TFS-TestProject/workspace/release.properties
    [DEBUG] line -
    [DEBUG] line -0 change(s), 1 detected change(s)
    [INFO] err -
    [DEBUG] Iterating
    [DEBUG] /appl/data/ci-test/jenkins/jobs/TFS-TestProject/workspace/release.properties:added

    ```

    Here the release build is checking for changes after a get to validate the output and it finds a "release.properties" file sitting there. Now in the days of Server workspaces where you had to explicitly check out from the server you would not even see an issue. The file would not even be detected let alone pended to the server unless you ran a specific command. In the wonderful world of Local workspaces where changes to local workspaces are detected automatically this is an issue.

    We need some way to tell TFS that we want it to ignore these release.properties files. Well, the TFS team thought of this and have added .tfignore files that operate just like the .gitignore one that you might be used to. However adding a .whatever files does not seem to be very easy in Widnows.

    [![image[5]](images/image5_thumb-5-5.png "image[5]")](http://nkdagility.com/wp-content/uploads/2014/06/image5-6-6.png)
    { .post-img }

    My first attempts to add the file resulted in a "you must type a file name" error and no matter what I did I could not get that .tfignore file created. I headed to the internet and eventually found that while you are blocked in Explorer you can open notepad and save a file of the required name. That’s a little poopy but needs must. I guess only power users really need to create files that begin with a dot and this protects the rest of them.

    [![image[8]](images/image8_thumb-7-7.png "image[8]")](http://nkdagility.com/wp-content/uploads/2014/06/image8-8-8.png)
    { .post-img }

    So we create and add a .tfignore file with a line that matches the pattern we want to ignore. Just listing the explicit file name will result in all instances, recursively, being ignored.

    ```
    ######################################
    # Ignore all release files from Maven release process
    release.properties
    ```

    You can get quite complicated with this file but here I have very simple needs. To get the file into TFS the easyest way is to go to the folder where you want it in your local workspace and add it to the file system. We then need to right click in the empty space of the folder and select "Add Files to folder" which will pop the "Add to Source Control" dialog above with any files listed that it can't see already. If you have the Power Tools installed you can also just right-click the file and add it to source control right from Windows explorer.

    [![image[11]](images/image11_thumb-1-1.png "image[11]")](http://nkdagility.com/wp-content/uploads/2014/06/image11-2-2.png)
    { .post-img }

    There may be other files that you need to ignore and I ended up with:

    ```
    ######################################
    # Ignore all release files from Maven release process
    release.properties
    *.releaseBackup
    target/

    ```

    All we need to do now is execute a new build and see that light turn green. This is however a "dry run" build and we still have some work to do to get the rest of the process working, however this is progress. At least I don’t have generated files ruining my day.
  FilePath: C:\Users\MartinHinshelwoodNKD\source\repos\NKDAgility.com\site\content\resources\blog\2014\2014-07-09-maven-release-prepare-fails-with-detected-changes-in-jenkins\index.md
  FolderPath: C:\Users\MartinHinshelwoodNKD\source\repos\NKDAgility.com\site\content\resources\blog\2014\2014-07-09-maven-release-prepare-fails-with-detected-changes-in-jenkins
- FrontMatter:
    title: Traveling for work and the Dell Venue 8
    description: Discover how the Dell Venue 8 enhances travel productivity for professionals. Join Martin Hinshelwood as he shares tips and insights from his journeys.
    ResourceId: Y8cHV08YFNc
    ResourceType: blog
    ResourceContentOrigin: Human
    ResourceImport: true
    ResourceImportId: 10645
    ResourceImportSource: Wordpress
    ResourceImportOriginalSource: Wordpress
    date: 2014-07-07
    weight: 1000
    creator: Martin Hinshelwood
    layout: blog
    resourceTypes: blog
    slug: traveling-work-dell-venue-8
    aliases:
    - /resources/Y8cHV08YFNc
    aliasesArchive:
    - /blog/traveling-work-dell-venue-8
    - /traveling-work-dell-venue-8
    - /traveling-for-work-and-the-dell-venue-8
    - /blog/traveling-for-work-and-the-dell-venue-8
    - /resources/blog/traveling-work-dell-venue-8
    tags: []
    categories:
    - Uncategorized
    preview: nakedalm-windows-logo-4-4.png
  BodyContent: |
    I travel a lot and I have been carting around both a Surface 2 Pro and a Surface 2 on my travels. I have been feeling recently that this was a little silly and I wanted to reduce my load and increase my flexibility. So just last week I purchased a Dell Venue 8.

    When I say that I travel a lot I mean that I generally fly out to somewhere in Europe on Sunday afternoon and then back on Friday night. Every week... I have been doing this since November 2013 and apart from a few weeks it has been fairly constant. In my travels I have found a few things. The first is that stuff can be important and that its not when you travel occasionally. If I go for a few days I generally ditch all but the most essential of stuff. However if I, like now, am on my twenty-sixth back to back week away I tend to miss some of the home comforts. Home comforts for a geek tends to be cables and gadgets but as I have recently turned into a bit of a fitness guy as well it might also be other stuff. And thus I tend to pack heavy rather than light. I do however put that heavy bag in checked baggage and want my carry on to be light. Unfortunately there are things like Tablets, batteries, and hard disks that really can't go into checked baggage. So it has to go in my backpack. I have thus decided that the Surface 2, that I got at the MVP Summit last year, was just impractical when I also carry my Surface 2 Pro. My plan was to wait for the rumoured Surface Mini but as that did not happen my second choice was the Dell Venue 8.

    ![clip_image001](images/clip_image001-1-1.jpg "clip_image001")
    { .post-img }

    I am however quite comfortable working in WinRT and don't have the same issue that many people seam to have. I have always looked at WinRT as analogous of the iPad or an Android and thus I have never expected it to be able to run Win32 apps and I don't really understand why anyone else did. To that end and as I was using the Surface 2 (even it is a crap-load lighter than the Pro) I have changed a few things in my workflow:

    - **Moved to OneNote for all writing** - I now write all of my blog posts on OneNote and then switch to my Pro for Windows Live Writer as the only viable publishing tool. Which has no analogue in WinRT
    - **Switched from SugarSync to OneDrive (was SkyDrive)** - this was a hard one for me as OneDrive is just as inferior as Box, DropBox, and all the other storage systems. The killer feature for me with SugarSync was the ability to sync any folder on my disk without needing to be in a single root. This allows me to sync my Signatures, templates, and other assorted stuff that applications insist in putting in locations other than that which you choose.
    - **Run Visual Studio on Azure** - Not a big deal unless you can't get a data connection. As an MSDN user I get enough hours on azure to run a decent Visual Studio instance for as much time as I need.

    As there are full versions of almost all of the office applicants (not Lync) on WinRT most other workflows stay the same. I think though, due to the general consumer misunderstanding of WinRT, that Microsoft will retire it slowly. But for devices like the Dell Venue 8 you really don't have to go half way. Reason being is that it is a fully fledged Intel device. The Dell Venue 8 has the latest generation of Intel's Atom processor and so far it is performing well. I am an impatient bigger at the best of times and most devices annoy me. Not so with this one so far.

    ![clip_image002](images/clip_image002-2-2.png "clip_image002")
    { .post-img }

    Now I am not really sure why the Dell Venue 8 came with a 32 bit version of windows when it has 64 bit hardware but I will do a little research when I have time and see if it is worth upgrading. Knowing Dell it will likely be a driver or sum such issue that has not yet been resolved but it is worth looking into.

    ![clip_image003](images/clip_image003-3-3.png "clip_image003")
    { .post-img }

    I also have a Acer Ionia W520 that has an Intel Atom processor and while I was impressed that it supported hyper threading on its dual core architecture this little beast actually has 4 cores. I am looking forward to getting Visual Studio on here, if only for fun.

    Note The Acer is really painful to use as the drivers are a complete disaster. It freezes and won't start all the time. If you get one then think of it as an ARM device and never run a native app. My daughter uses it for modern apps all the time with no issues.

    Now I am supposed to have a little pen for taking notes but it has not appeared. So two with the SIM card that was supposed to be in the box. I have checked and I only received two of the three expected packages so I guess a little bit is delayed. I received my packages last Friday so I have not had a response yet... I guess it will get sorted out. In the way only Dell can manage the manifest is totally incomprehensible:

    - 460-BBHK 1 Dell Tablet Folio - Dell Venue 8 Pro Model 5830
    - 750-AADT 1 Dell Active Stylus - Venue Pro tablets
    - 203-47416 1 SM003TV8P9UKMBPRO
    - 210-ABPB Dell Venue 8 Pro 5830
    - 338-BDOZ Intel Atom processor Z3740D (2MB Cache, up to 1.8GHz Quad-Core)
    - 391-BBKQ 8.0 inch IPS Display with HD (WXGA 1280 x 800) resolution with 10-pt capacitive touch
    - 319-BBCO Integrated 1.2MP HD Webcam (front) / 5MP (back)
    - 320-BBGK Black Cover for WWAN
    - 340-ACEH English,French,German,Italian,Spanish,Dutch Shipping Docs
    - 340-ACEN Placemats (English,French,German,Spanish,Brazilian Portugese)
    - 340-ABJP Direct ship process
    - 340-ACBW Additional Software 64GB Commercial
    - 370-ABFN 2GB Single Channel DDR3L-RS 1600MHz
    - 400-ACOV 64GB eMMC Hard Drive
    - 450-ABLT Power adapter - UK type
    - 451-BBGT 18Whr (4830mAh), 2-Cell Battery
    - 389-BCZS Dell Wireless 1538 Dual-Band 2X2 802.11a/b/g/n WiFi + Bluetooth(R) 4.0
    - 490-BBQG Intel HD Graphics
    - 556-BBCQ Dell NetReady mobile broadband solution (HSPA+) with O2 sim card
    - 619-ABGY Windows 8.1 Pro (32Bit) English
    - 620-AAAU OS Media Kit Not Included
    - 630-AAAU Software: Microsoft Office 2013 Trial
    - 640-BBBT Amazon Kindle Metro App
    - 525-10302 McAfee Security Center 30 day trial, Digital Delivery
    - 525-10163 Not Selected in this Configuration 1 SR
    - 525-10173 Dell Data Protection | Security Tools Digital Delivery/NB
    - 525-10283 MY DELL
    - 344-27772 1Yr Collect and Return - Minimum Warranty
    - 755-10658 1 year Accidental Damage Protection
    - 817-BBBC Not Selected in this Configuration
    - 710-53767 1Yr ProSupport with Rapid Collect and Return
    - 799-AAMQ Dell Order
    - 998-23673 Fixed Hardware Configuration

    I think that the three things with a '1' after the designation are the core packages. This should be a little simpler but really whatever floats their boat.

    ## Conclusion

    I have only been using this device over the weekend and so far it has been fantastic. I have even been typing on the screen rather than an external keyboard and found it to be just right. I also have a Microsoft Wedge Keyboard in my pack and so far I have had no reason to bring it out.

    I will be using the Dell Venue 8 for all of my note taking over the next wee while and we will see if it stands up to my lack of patience.
  FilePath: C:\Users\MartinHinshelwoodNKD\source\repos\NKDAgility.com\site\content\resources\blog\2014\2014-07-07-traveling-work-dell-venue-8\index.md
  FolderPath: C:\Users\MartinHinshelwoodNKD\source\repos\NKDAgility.com\site\content\resources\blog\2014\2014-07-07-traveling-work-dell-venue-8
- FrontMatter:
    title: How to delete work items from TFS or VSO
    description: Learn how to efficiently delete work items from TFS or VSO with expert tips and code examples. Streamline your project management today!
    ResourceId: MYXrtTYV2UD
    ResourceType: blog
    ResourceContentOrigin: Human
    ResourceImport: true
    ResourceImportId: 10597
    ResourceImportSource: Wordpress
    ResourceImportOriginalSource: Wordpress
    date: 2014-07-02
    weight: 1000
    creator: Martin Hinshelwood
    layout: blog
    resourceTypes: blog
    slug: delete-work-items-tfs-vso
    aliases:
    - /resources/MYXrtTYV2UD
    aliasesArchive:
    - /blog/delete-work-items-tfs-vso
    - /delete-work-items-tfs-vso
    - /how-to-delete-work-items-from-tfs-or-vso
    - /blog/how-to-delete-work-items-from-tfs-or-vso
    - /resources/blog/delete-work-items-tfs-vso
    tags: []
    categories:
    - Uncategorized
    preview: nakedalm-experts-visual-studio-alm-1-1.png
  BodyContent: |
    Have you ever created a bunch of work items that you decided later that you had to delete. Well I have… especially as a user of the TFS Integration Platform. And when things go wrong there they can really go wrong.

    Now while you can put stuff into the "removed" state it is still hanging around cluttering up the place. The only way out of the box to remove items is to give the ID for each work item that you want to delete and execute the command line for each one.:

    ```
    witadmin destroywi /collection:CollectionURL /id:id [/noprompt]
    ```

    _WARNING: This code can result in total loss of all work items you have if you miss key a query! Be careful… and you are on your own. Don't blame me, and no… I can’t get them back for you…_

    Well that’s just great unless you have a couple of thousand things to delete. So I knocked up a little bit of code to do it for me. Now, since I have had to knock it up a bunch of times before I thought that I had better share it. I started this blog in the first place so that I would remember things.

    ```
    using Microsoft.TeamFoundation.Client;
    using Microsoft.TeamFoundation.WorkItemTracking.Client;
    using System;
    using System.Collections.Generic;
    using System.Linq;
    using System.Text;
    using System.Threading.Tasks;
     
    namespace ConsoleApplication1
    {
        class Program
        {
            static void Main(string[] args)
            {
     
                TfsTeamProjectCollection tpc = new TfsTeamProjectCollection(new Uri("http://tfs.company.com:8080/tfs/DefultCollection"));
                WorkItemStore store = tpc.GetService();
                string query = @"SELECT [System.Id] FROM WorkItems WHERE [System.TeamProject] = 'projectName'  AND  [System.AreaPath] UNDER 'projectName\_TOBEDELETED' ORDER BY [System.Id]";
                WorkItemCollection wis = store.Query(query);
                IEnumerable x = from WorkItem wi in wis select wi.Id;
                Console.WriteLine(string.Format("DESTROY {0} work items (they really can't be resurrected): y/n?", wis.Count));
                ConsoleKeyInfo cki = Console.ReadKey();
                Console.WriteLine();
              if (cki.Key.ToString().ToLower() == "y")
                {
                try
                    {
                        Console.WriteLine("Deleting....");
                        IEnumerable y = store.DestroyWorkItems(x.ToArray());
                        Console.WriteLine("DONE");
                        foreach (var item in y)
                        {
                            Console.WriteLine(item.ToString());
                        }
                    }
                    catch (Exception)
                    {
     
                        Console.WriteLine("Things have gotten all pooped up please try again!");
                    }
            
                }
     
              Console.WriteLine("Freedom");
            }
       
        }
    }

    ```

    The first thing that you may notice is that I search for items in a specific area path. I use \_TOBEDELETED as it is obvious what is going to happen to things that end up there. Although I did work with a user who complained that all his files had gone missing. When asked where he kept them he pointed at the recycle bin on his desktop!

    Anyhoo… just in case you made a mistake it will let you know how many work items that you are deleting. It’s a simple check but I have had it say "100,000" work items… AS you can imagine I very carefully terminated the program (never trust the 'no' option).
  FilePath: C:\Users\MartinHinshelwoodNKD\source\repos\NKDAgility.com\site\content\resources\blog\2014\2014-07-02-delete-work-items-tfs-vso\index.md
  FolderPath: C:\Users\MartinHinshelwoodNKD\source\repos\NKDAgility.com\site\content\resources\blog\2014\2014-07-02-delete-work-items-tfs-vso
- FrontMatter:
    title: Run a router on Hyper-V
    description: Learn how to run a router on Hyper-V for seamless VM internet access in corporate and hotel networks. Discover efficient setup tips and solutions!
    ResourceId: 0Ly3betXY-6
    ResourceType: blog
    ResourceContentOrigin: Human
    ResourceImport: true
    ResourceImportId: 10617
    ResourceImportSource: Wordpress
    ResourceImportOriginalSource: Wordpress
    date: 2014-06-25
    weight: 1000
    creator: Martin Hinshelwood
    layout: blog
    resourceTypes: blog
    slug: run-router-hyper-v
    aliases:
    - /resources/0Ly3betXY-6
    aliasesArchive:
    - /blog/run-router-hyper-v
    - /run-router-hyper-v
    - /run-a-router-on-hyper-v
    - /blog/run-a-router-on-hyper-v
    - /resources/blog/run-router-hyper-v
    tags:
    - Install and Configuration
    - System Configuration
    categories:
    - Uncategorized
    preview: naked-alm-hyper-v-17-17.png
  BodyContent: |
    I want to run a router on Hyper-V so that I can run many VM's, each with internet access, on corporate and hotel networks. Microsoft touts Routing and Remote Access but there is no way I will go there. First it’s a total pain to setup and run. Second I need to run a whole Windows Server just to have basic DHCP and internet access. Overkill much! There must be a better way.

    The problem is that on most networks that I connect to there is some sort of one-MAC-one-IP rule. I am often onsite at companies on their Guest Wi-Fi and in hotel's. I am not sure how it is implemented but if there is a pay-wall or even just a terms and conditions acceptance then I get kicked off the network as soon as I spin up a VM. When that VM requests an IP address the network kicks all of my machines off. Poo…no internet access for me.

    ![clip_image001](images/clip_image001-1-1.jpg "clip_image001")
    { .post-img }

    For a while now I have been using an external solution. I have a little box called a [HooToo TripMate](http://nkdalm.net/HooTooTripMate). The HooToo TripMate is a bunch of things in one. First it is a battery pack, and a good one. Second it is a router, and not only a router, but it will do both network and wireless bridging. So I boot this little beauty up and connect my Surface. I can then connect to it and tell it to connect to "hotelWiFi1". I then open a browser on my Surface and sign-in, or pay, or whatever. As this little box has DHCP I can connect many devices behind it. Simples…

    ![clip_image002](images/clip_image002-2-2.png "clip_image002")
    { .post-img }

    However there are limitations. If the network I am connecting to uses certificates or AD Credentials, as many companies do, then the HooToo TripMate will not connect. Poo again… I have been looking for another solution that might work. After scowering the web every now and again over the years I finally found an interesting idea. Why can't I run a router's firmware in a VM? Holy cow! Can that actually work…

    I have been tinkering with this option for a while and I have almost gotten what I want working. Let's go through what I have gotten working first and then we can discuss where I think I can go.

    1. Create a new VHD and write the Router firmware
    2. Create a VM to Host our new router firmware

    ## Create a new VHD and write the Router firmware

    The first thing that we need is a VHD with the appropriate settings. It should be a VHD, 50MB, and fixed size.

    ![clip_image003](images/clip_image0031-3-3.png "clip_image003")
    { .post-img }

    Above you can see how you create the VHD and apply the settings on Windows 8. Make sure that you create the VHD somewhere you can remember, and we will likely be moving it later to a more permanent home.

    ![clip_image004](images/clip_image0041-4-4.png "clip_image004")
    { .post-img }

    Once you have created your empty VHD you need to take note only of the disk number. In this case it is "Disk 2". This is where we will write the image from our firmware and we need a couple of things to move forward:

    - [physdiskwrite](onenote:#CHECK%20Using%20a%20router%20to%20support%20Hyper-V&section-id={965C1CBE-C6B3-4425-B140-4B0EC0671288}&page-id={0F6DF006-4E9F-4670-8535-309194E75A43}&object-id={16DD4318-D6AD-0B9C-02B0-146BB0E9AA87}&77&base-path=https://nakedalm-my.sharepoint.com/personal/martin_nakedalm_com/Documents/nakedALMBlog/Blog/In%20Progress.one) - This is tool with both UI and Command line for taking an image and writing it to our VHD
    - [DD-WRT image for x86](http://www.dd-wrt.com/site/support/router-database) - In the search box type x86 to see a list of downloads. Look for the one called dd-wrt_public_vga.image. At the time of writing the latest version was 3744 and available from [http://www.dd-wrt.com/routerdb/de/download/X86/X86///dd-wrt_public_vga.image/3744](http://www.dd-wrt.com/routerdb/de/download/X86/X86/dd-wrt_public_vga.image/3744)

    And that’s all you need to get going. You can call physdiskwrite from the command line or you can use the UI. Either way works however the UI is in German and I have found that the command line give better feedback.

    ![clip_image005](images/clip_image005-5-5.png "clip_image005")
    { .post-img }

    Make really sure the disk number that you are selecting is correct or you might inadvertently overwrite a disk that you need. If you need to reopen Disk Management and double and triple check the number.

    Warning: I found that I get a bunch of errors the first time around and needed to recreate the VHD.

    ![clip_image006](images/clip_image006-6-6.png "clip_image006")
    { .post-img }

    Once it is complete you should have a populated disk and not just Unallocated space now. If you right click on the Disk's left hand definition you can now detach it.

    ## Create a VM to Host our new router firmware

    We can now go ahead and create a new VM to host out new router firmware. This is a simple process if you are looking at this solution anyway and you will likely have done this 100 times for your other VM's.

    ![clip_image007](images/clip_image007-7-7.png "clip_image007")
    { .post-img }

    My sure that you select "Generation 1" as we created a VHD and not a VHDX and only give the machine 32MB of RAM. This VM should be so lightweight that we can run it 24x7 and no one will notice. Leave it unconnected for now as well will need to replace the default adapters.

    ![clip_image008](images/clip_image008-8-8.png "clip_image008")
    { .post-img }

    When you are asked to connect to a virtual hard disk you should specify the VHD that we just created. If you like take a backup of that disk by copying it to another folder. This may save time alter when you want to reset the router. Remember that there is no 'reset' button and if you brick it then you need to refresh the image.

    ![clip_image009](images/clip_image009-9-9.png "clip_image009")
    { .post-img }

    Let go change the settings now. If you right click on the "Router" VM and select settings…

    ![clip_image010](images/clip_image010-10-10.png "clip_image010")
    { .post-img }

    In the settings dialog the first task is to remove the existing "Network Adapter" as it will not do the job. I am not sure why you need legacy adapters but it is likely that the version of Linux that DD-WRT is based on does not have Hyper-V drivers built in.

    ![clip_image011](images/clip_image011-11-11.png "clip_image011")
    { .post-img }

    Adding the legacy adapters is easy and you will want two. The first, and the only one we will enable, will be the "LAN" port. Later we may want to have a "WAN" port as well. More on that later…

    ![clip_image012](images/clip_image012-12-12.png "clip_image012")
    { .post-img }

    Once you have two adapters we need to configure one. You should wire one, and only one adapter into the "private" channel. I have two "Virtual Switches" configured. The first is called "Public" and is wired to my Wi-Fi adapter. This is the one that provides access to external network. The second I have called "Private" and it is configured as an "Internal" switch. When a Hyper-V switch is configured as Internal it can only be used for communication between guests and the host.

    ![clip_image013](images/clip_image013-13-13.png "clip_image013")
    { .post-img }

    If you "Connect" to the console you will see the output of the Linux router start-up sequence. Once it stabilises you should see adapters on your "Private" network get assigned IP addresses and you can then connect to the routers admin page.

    ![clip_image014](images/clip_image014-14-14.png "clip_image014")
    { .post-img }

    You can now configure your router as if it was a normal bit of kit on your network.

    ### WARNING: Binding preference may result in loss of connectivity

    If you immediately loose internet when you connect up your router you may find that the router has taken priority in your bindings list. This will result in your computer trying to send internet traffic through this connection. Whenever you have two gateways listed you may need to tweak the order. Thanks to [Marcel de Vries](http://blogs.infosupport.com/author/marcelv/) for this one as it had me stumped.

    Go to your Network Connections folder from the control panel and hit the "Alt" key to bring up the menu. Select "Advanced | Advanced Settings…" to bring up the Adapters and Bindings dialog and make your external interface have a higher priority.

    ![clip_image015](images/clip_image015-15-15.png "clip_image015")
    { .post-img }

    ## Conclusion

    The result should be a stable DHCP that makes connecting to all of your VM's over RDP easy as you don’t have to manually set IP's. In addition your adapters will no longer be listed as "public" and will now be "Private". This is important as it changes the firewall protocols that are applied. "Private" networks are trusted a little, "Public" ones are not given an inch.

    ![clip_image016](images/clip_image016-16-16.png "clip_image016")
    { .post-img }

    This works fantastically for private networks only. However as soon as you give the router above a public adapter as well we get into the same position as having all of your hyper-v servers connected to Public. This may only be two IP's on the public adapter but two is one more than most networks that I connect to will allow. At least this way if I do not add a public adapter to the router all of my Hyper-V machines all get IP addresses… however I want more. I want to be able to do the same things as the HooToo TripMate does but without requiring an additional device, oh and getting support for AD and certificate authenticated Wi-Fi networks.

    It should be as simple as enabling the WAN port, and configuring the Public Virtual Switch to not allow the host access to the internet through it. That way the only device getting an external IP is the Router.

    However I can't seem to figure out how to get the router online without taking me out with it so its the HooToo TripMate for now.. It’s a little difficult to debug when I am always on hotel and corporate networks with goodness knows what restrictions. I think I will need a couple of days on a non-limited network to figure out this last bit… I get a couple of days off next week so we will see.

    Have you managed to get this working?
  FilePath: C:\Users\MartinHinshelwoodNKD\source\repos\NKDAgility.com\site\content\resources\blog\2014\2014-06-25-run-router-hyper-v\index.md
  FolderPath: C:\Users\MartinHinshelwoodNKD\source\repos\NKDAgility.com\site\content\resources\blog\2014\2014-06-25-run-router-hyper-v
- FrontMatter:
    title: Getting a service account for VSO with TFS Service Credential Viewer
    description: Learn how to obtain a service account for Visual Studio Online using the TFS Service Credential Viewer. Streamline your automation tasks effortlessly!
    ResourceId: i10guK3jvON
    ResourceType: blog
    ResourceContentOrigin: Human
    ResourceImport: true
    ResourceImportId: 10596
    ResourceImportSource: Wordpress
    ResourceImportOriginalSource: Wordpress
    date: 2014-06-18
    weight: 840
    creator: Martin Hinshelwood
    layout: blog
    resourceTypes: blog
    slug: getting-service-account-vso-tfs-service-credential-viewer
    aliases:
    - /resources/i10guK3jvON
    aliasesArchive:
    - /blog/getting-service-account-vso-tfs-service-credential-viewer
    - /getting-service-account-vso-tfs-service-credential-viewer
    - /getting-a-service-account-for-vso-with-tfs-service-credential-viewer
    - /blog/getting-a-service-account-for-vso-with-tfs-service-credential-viewer
    - /resources/blog/getting-service-account-vso-tfs-service-credential-viewer
    tags:
    - Install and Configuration
    categories:
    - Uncategorized
    preview: nakedalm-experts-visual-studio-alm-1-1.png
  BodyContent: |
    Have you tried to get a service account for Visual Studio Online (VSO)? Did you know that you can use the TFS Service Credential Viewer to get it.

    When you join a local or azure build server to your VSO account you are asked to log in with an account that is an administrator to get credentials. However it cant continue to use your credentials as your Microsoft ID token expires after 2 days and you would have to login again. Not a good experience. However there is a little bit of code that the build server uses to get a basic service username and password that it uses instead. I have used this to create unit tests that hit the TFS API’s in VSO as well as do all sorts of automated tasks that I need.

    I created the TFS Service Credential Viewer when the service was still in Preview but it is no less required now. Its your gateway to automation with VSO.

    ### Download TFS Service Credential Viewer

    The following prerequisites are required:

    - Team Explorer 2013 Visual Studio 11 (any version)
    - .NET 4.5

    If these components are already installed, you can [launch](http://nkdagility.com/downloads/tools/tfs2012/TfsServiceCredentialViewer/TfsServiceCredentialsUI.application) the application now. Otherwise, click install below to install the prerequisites and run the application.

    ###### [install](http://nkdagility.com/downloads/tools/tfs2012/TfsServiceCredentialViewer/setup.exe) or [launch via clickonce](http://nkdagility.com/downloads/tools/tfs2012/TfsServiceCredentialViewer/TfsServiceCredentialsUI.application)

    ### How it works

    Once you have authenticated as a TFS Collection Administrator using your Microsoft ID to your hosted VSO instance we use the Access Control Service to provision a service identity that you can use for unattended connections to VSO.

    [![SNAGHTML85af783](http://i1.wp.com/blog.hinshelwood.com/files/2012/03/SNAGHTML85af783_thumb.png?zoom=1.5&resize=460%2C461 "SNAGHTML85af783")](http://i0.wp.com/blog.hinshelwood.com/files/2012/03/SNAGHTML85af783.png)  
    { .post-img }
    **Figure: A quick #1, #2 to get your credentials**

    http://youtu.be/Fkn6V0\_zz28  
    **Video: How to get your credentials**

    ### Troubleshooting

    If you are using Windows 8 you will not get an automatic launch of the application due to an extra security check called Smart Screen for applications that come from the internet.

    1.  Click or Press “Start” and Scroll all the way to the right
    2.  Select the TFS Service Credential Viewer
    3.  When the security dialog pops up click “More Info”
        [![image](http://i2.wp.com/blog.hinshelwood.com/files/2012/03/image_thumb22.png?zoom=1.5&resize=640%2C268 "image")](http://i1.wp.com/blog.hinshelwood.com/files/2012/03/image22.png)
        { .post-img }
        **Figure: Select More Info  
         **
    4.  Click “Run anyway” to launch the application and add it to the safe list
        [![image](http://i2.wp.com/blog.hinshelwood.com/files/2012/03/image_thumb23.png?zoom=1.5&resize=640%2C270 "image")](http://i2.wp.com/blog.hinshelwood.com/files/2012/03/image23.png)
        { .post-img }
        Figure;
    5.  Done

    If you encounter an exception when clicking “Connect” the most likely cause if that you do not have Team Explorer 2013 installed (it should also work with 2012).
  FilePath: C:\Users\MartinHinshelwoodNKD\source\repos\NKDAgility.com\site\content\resources\blog\2014\2014-06-18-getting-service-account-vso-tfs-service-credential-viewer\index.md
  FolderPath: C:\Users\MartinHinshelwoodNKD\source\repos\NKDAgility.com\site\content\resources\blog\2014\2014-06-18-getting-service-account-vso-tfs-service-credential-viewer
- FrontMatter:
    title: TFS Process Template migration script updated
    description: Discover how to easily migrate TFS process templates with our updated script. Follow five simple steps to streamline your Agile and Scrum processes!
    ResourceId: __k7mlKoTxt
    ResourceType: blog
    ResourceContentOrigin: Human
    ResourceImport: true
    ResourceImportId: 10558
    ResourceImportSource: Wordpress
    ResourceImportOriginalSource: Wordpress
    date: 2014-06-11
    weight: 840
    creator: Martin Hinshelwood
    layout: blog
    resourceTypes: blog
    slug: tfs-process-template-migration-script-updated
    aliases:
    - /resources/__k7mlKoTxt
    aliasesArchive:
    - /blog/tfs-process-template-migration-script-updated
    - /tfs-process-template-migration-script-updated
    - /resources/blog/tfs-process-template-migration-script-updated
    tags:
    - Azure DevOps
    - Software Development
    - Install and Configuration
    - System Configuration
    categories:
    - Uncategorized
    preview: nakedalm-experts-visual-studio-alm-1-1.png
  BodyContent: |
    Did you know that you can quite easily to do a TFS process template migration? Did you notice I used the "quite" in there. Well if you think of the Process Template as the blueprints then the Team Project that you create is the concrete instance of that blueprint.

    Warning: naked ALM Consulting provide no warranties of any type, nor accepts any blame for things you do to your servers in your environments. We will however, at our [standard consulting rates](http://nkdalm.net/ALMTerms), provide best efforts to help you resolve any issues that you encounter.

    I have written on this topic before, however it is always worth refreshing it as I discover more every time I do an update. My current customer is wanting to move from a frankintemplate (a mishmash of Agile for MSF Software Development and CMMI for MSF Process Improvement) to a more vanilla Visual Studio Scrum template. In this case it is an upgraded 2010 server with 4.x templates to the 2013.3 (downloaded from VSO) Scrum one.

    There are five simple steps that we need to follow:

    1. **Select** - Pick the process template that is closest to where you want to be (I recommend the Scrum template is all scenarios)
    2. **Customise** - Re-implement any customisations that you made to your old template to the new one taking into account advances in design , new features, and implementation changes. You may need to have duplicate fields to access old data.
    3. **Import** - simply overwrite the existing work item types, categories, and process configuration with your new one.  
       _note: if you are changing the names of Work Items (for example User Story or Requirement to Product Backlog Item) then you should do this before you do the import.  
       note: Make sure that you backup your existing work item types by exporting them from your team project._
    4. **Migrate data** - Push some data around… for example Stack Rank field is now Backlog Priority and the Story Points field is now Effort. You may also have done that DescriptionHTML in 2010 that you will want to get rid of.
    5. **Housekeeping** - if you had to keep some old fields to migrate data you can now remove them

    While it is simple, depending on the complexity and customisation of your process, you want to get #2 right to move forward easily. Indeed you are effectively committed when you hit #3. If it is so easy why can't it be scripted, I hear you shout? Well you can and I have, however I always run the script carefully block by block so that there are no mistakes. Indeed I have configured the script so that I can tweek the xml of the template and only re-import the bits that are changes. This is the script I use for #3.

    ```
    $TeamProjectName = "myTeamProject"
    $ProcessTemplateRoot = Get-Location
    $CollectionUrl = "http://mytfsserver:8080/tfs/mycollection"
    ```

    The first part is to get the variables in there. There are a bunch of things that we need in place such as Collection URL and the name of your Team Project that we will use over and over again.

    ```
    # Make sure we only run what we need
    [datetime] $lastImport
    $UpdateFilePath = ".\UpdateTemplate.txt"
    if ((Test-Path $UpdateFilePath) -eq $true)
    {
      $UpdateFile = Get-Item -Path $UpdateFilePath
      $lastImport = $UpdateFile.LastWriteTime
    } else {
      $lastImport = [datetime]::MinValue
    }
    Write-Output "Last Import was $lastImport"
    ```

    Then I do a little trick with the date. I try to load the last date and time that the script was run from a file and set a default if it does not exist. This will allow me to test to see if we have been tweaking the template and only update the latest tweaks. I generally use this heavily in my dev/test cycle when I am building out the template. I tend to create an empty project to hold my process template definition within Visual Studio so that I get access to easy source control and can hook this script up to the build button. If I was doing this for a large company I would also hook up to Release Management and create a pipeline that I can push my changes through and get approvals from the right people in there.

    ```
    $WitAdmin = "${env:ProgramFiles(x86)}\Microsoft Visual Studio 12.0\Common7\IDE\witadmin.exe"
    $tfpt = "${env:ProgramFiles(x86)}\Microsoft Visual Studio 2013 Power Tools\tfpt.exe"
    ```

    Next I configure the tools that I am going to use. This is very version specific with the above only working on a computer with 2013 editions of the product installed. Although I am only using the $WitAdmin variable I keep the rest around so that remember where they are.

    ```
    & $WitAdmin renamewitd /collection:$CollectionUrl /p:$TeamProjectName /n:"User Story" /new:"Product Backlog Item"
    & $WitAdmin renamewitd /collection:$CollectionUrl /p:$TeamProjectName /n:"Issue" /new:"Impediment"
    ```

    Once, and only once I will run the rename command for data stored in a work item type that I want to keep. For example if I am moving from the Agile to Scrum templates I will rename "User Story" to "Product Backlog Item" and "Issue" to "Impediment". The only hard part here is if you have ended up with more than one work item type that means the same thing as you can't merge types easily or gracefully.

    _Note: If you do need to merge data you have a couple of options; a) 'copy' each work item to the new type. This is time consuming and manual. Suitable for less than fifty work items; b) export to excel and then import as the new type. This leaves everything in the new state and they manually have to walk the wokflow. Suitable for less than two hundred work items; c) Spin up the TFS Integration Tools. Pain and suffering this way lies. Greater than a thousand work items only._

    ```
    $lts = Get-ChildItem "$ProcessTemplateRoot\WorkItem Tracking\LinkTypes" -Filter "*.xml"
    foreach( $lt in $lts)
    {
        if ($lt.LastWriteTime -gt $lastImport)
        {
            Write-Host "+Importing $lt"
            & $WitAdmin importlinktype /collection:$CollectionUrl /f:$($lt.FullName)
        } else {
            Write-Host "-Skipping $lt"
        }
    }
    ```

    Importing the link types tends to be unnecessary but I always do it as I have caught out a couple of times. Its mostly like for like and has no effect. If you have custom relationships, like "Releases \\ Released By" for a "Release" work item type to Backlog Items you may need this.

    ```
    $witds = Get-ChildItem "$ProcessTemplateRoot\WorkItem Tracking\TypeDefinitions" -Filter "*.xml"
    foreach( $witd in $witds)
    {
        if ($witd.LastWriteTime -gt $lastImport)
        {
            Write-Host "+Importing $witd"
            & $WitAdmin importwitd /collection:$CollectionUrl /p:$TeamProjectName /f:$($witd.FullName)
        } else {
            Write-Host "-Skipping $witd"
        }
    }
    ```

    Now I want to update the physical work items in your Team Project. This will overwrite the existing definition so make really sure that you have a backup. No really, go take a backup now by using the "witadmin exportwitd" and running it for each of your existing types. Yes.. All of them… now you can run this part of the script.

    After this you will have the correct work item types however we have not updated the categories or the process configuration so things may be a little weird in TFS until we finish up. The Work Item type provides the list of fields contained within the work item, the form layout, and the workflow of the state changes. All of these will now have been upgrade to the new version. Features will be broken at this point until we get a little further.

    ```
    $Cats = Get-Item "$ProcessTemplateRoot\WorkItem Tracking\Categories.xml"
    if ($Cats.LastWriteTime -gt $lastImport)
    {
        Write-Host "+Importing $Cats"
        & $WitAdmin importcategories /collection:$CollectionUrl /p:$TeamProjectName /f:$($cats.FullName)
    } else {
        Write-Host "-Skipping $($Cats.name)"
    }
    ```

    The categories file determines which work items are viable and what they are used for. After TFS 2010 the TFS team moved to categorising work item types so that reporting and feature implementation became both easier and less error prone. This is a simple import of a single file. Not much will change in the UI.

    ```
    $ProcessConfig = Get-Item "$ProcessTemplateRoot\WorkItem Tracking\Process\ProcessConfiguration.xml"
    if ($ProcessConfig.LastWriteTime -gt $lastImport)
    {
        Write-Host "+Importing $($ProcessConfig.name)"
        & $WitAdmin importprocessconfiguration /collection:$CollectionUrl /p:$TeamProjectName /f:"$($ProcessConfig.FullName)"
    } else {
        Write-Host "-Skipping $($ProcessConfig.name)"
    }
    ```

    If you have TFS 2013 there is only one Process Configuration file. This controls how all of the Agile Planning tools interact with your work items and many other configurations, even the colour of the work items. This is the glue that holds everything together and makes it work. Once this is updated your are effectively upgraded. If you still have errors then you have done something wrong.

    _Note: You may need to a full refresh in Web Access and on Client API's (VS and Eclipse) to see these changes._

    ```
    $AgileConfig = Get-Item "$ProcessTemplateRoot\WorkItem Tracking\Process\AgileConfiguration.xml"
    if ($AgileConfig.LastWriteTime -gt $lastImport)
    {
        Write-Host "+Importing $($AgileConfig.name)"
        & $WitAdmin importagileprocessconfig /collection:$CollectionUrl /p:$TeamProjectName /f:"$($AgileConfig.FullName)"
    } else {
        Write-Host "-Skipping $($AgileConfig.name)"
    }
    $CommonConfig = Get-Item "$ProcessTemplateRoot\WorkItem Tracking\Process\CommonConfiguration.xml"
    if ($CommonConfig.LastWriteTime -gt $lastImport)
    {
        Write-Host "+Importing $($CommonConfig.name)"
        & $WitAdmin importcommonprocessconfig /collection:$CollectionUrl /p:$TeamProjectName /f:"$($CommonConfig.FullName)"
    } else {
        Write-Host "-Skipping $($CommonConfig.name)"
    }
    ```

    If you are on TFS 2012 then you have the same thing but instead of one consolidated file there are two files… for no reason whatsoever that I can determine…which is why it's one in 2013. Same, without the colours, configuration though.

    \[code\]

    $lastImport = \[datetime\]::Now

    Out-File -filepath ".\\UpdateTemplate.txt" -InputObject $lastImport

    \[/code\]

    The final piece of the puzzle is to update the datetime file we tried to load at the start. This will allow us to update a single xml file that we imported above and the script, when re-run in part or in its entirety, will only update what it needs. It just makes things a little quicker.

    And there you have it. Contrary to popular belief you can upgrade or migrate from one process template to another in TFS. It may be because you want to use the new features or it may be because you are radically changing you process, it can be done.

    Good luck with your changes…
  FilePath: C:\Users\MartinHinshelwoodNKD\source\repos\NKDAgility.com\site\content\resources\blog\2014\2014-06-11-tfs-process-template-migration-script-updated\index.md
  FolderPath: C:\Users\MartinHinshelwoodNKD\source\repos\NKDAgility.com\site\content\resources\blog\2014\2014-06-11-tfs-process-template-migration-script-updated
- FrontMatter:
    title: Access denied user needs label permission in TFS
    description: Learn how to resolve the 'Access denied user needs label permission in TFS' error and optimize your TFS security settings for smoother project management.
    ResourceId: jdpPpvEHzGa
    ResourceType: blog
    ResourceContentOrigin: Human
    ResourceImport: true
    ResourceImportId: 10546
    ResourceImportSource: Wordpress
    ResourceImportOriginalSource: Wordpress
    date: 2014-06-04
    weight: 1000
    creator: Martin Hinshelwood
    layout: blog
    resourceTypes: blog
    slug: access-denied-user-needs-label-permission-tfs
    aliases:
    - /resources/jdpPpvEHzGa
    aliasesArchive:
    - /blog/access-denied-user-needs-label-permission-tfs
    - /access-denied-user-needs-label-permission-tfs
    - /access-denied-user-needs-label-permission-in-tfs
    - /blog/access-denied-user-needs-label-permission-in-tfs
    - /resources/blog/access-denied-user-needs-label-permission-tfs
    tags: []
    categories:
    - Uncategorized
    preview: nakedalm-experts-visual-studio-alm-3-3.png
  BodyContent: |
    There is always something new to learn with TFS and today I learned something old. I had a user today that was constantly getting the message "Access denied user needs label permission in TFS".

    Although I have configured "one-team-project" for many organisation my current customer is the first that has insisted on draconian security measures. In this case I have removed contributors from the root of the Team Project and each team is granted rights by sub-folder. If each sub-folder represents an application then this makes perfect sense. Until you try to add a label using the Visual Studio IDE.

    ![clip_image001](images/clip_image001-1-1.jpg "clip_image001")
    { .post-img }

    I spent a little while trying to debug this and finding no issues with my configuration I emailed the champs list. In a timely manner Mr Jesse Houwing replied with a "Well duh Martin… that’s how it has always worked":

    > Labels created within the graphical user interface are scoped to the root folder of the team project within which they are created. Labels created from the command line are scoped to the longest common path shared by the items specified in the label command. To specify the fully qualified name of a label, you must concatenate the label name, the '@' symbol, and the label scope, as in [Beta@$/TeamProject1](mailto:Beta@$/TeamProject1).
    >
    > \-[http://msdn.microsoft.com/en-us/library/ms181439(v=vs.80).aspx](<http://msdn.microsoft.com/en-us/library/ms181439(v=vs.80).aspx>)

    Well… poo… That does not sound like a good idea. And then I realised that the TFS team also have to support the lowest common denominator. Those developers that you meet in 2014 who have no idea what a Unit Test is (or think that it is opening the app and clicking some buttons) or what automated builds are. So if they found that they could create Label with the same name but overlapping scopes!

    Mind blown…

    ![clip_image002](images/clip_image0022-2-2.png "clip_image002")
    { .post-img }

    My solution was to just give contributors access only to labels at the root. This stops that pesky error from occurring in the IDE and really does not pose a security risk.
  FilePath: C:\Users\MartinHinshelwoodNKD\source\repos\NKDAgility.com\site\content\resources\blog\2014\2014-06-04-access-denied-user-needs-label-permission-tfs\index.md
  FolderPath: C:\Users\MartinHinshelwoodNKD\source\repos\NKDAgility.com\site\content\resources\blog\2014\2014-06-04-access-denied-user-needs-label-permission-tfs
- FrontMatter:
    title: Import Excel data into TFS with History
    description: Learn how to import Excel data into TFS with history using VBA. Simplify your workflow and ensure data integrity with this step-by-step guide!
    ResourceId: Kf-5JC3RyWF
    ResourceType: blog
    ResourceContentOrigin: Human
    ResourceImport: true
    ResourceImportId: 10541
    ResourceImportSource: Wordpress
    ResourceImportOriginalSource: Wordpress
    date: 2014-05-28
    weight: 1000
    creator: Martin Hinshelwood
    layout: blog
    resourceTypes: blog
    slug: import-excel-data-into-tfs-with-history
    aliases:
    - /resources/Kf-5JC3RyWF
    aliasesArchive:
    - /blog/import-excel-data-into-tfs-with-history
    - /import-excel-data-into-tfs-with-history
    - /resources/blog/import-excel-data-into-tfs-with-history
    tags: []
    categories:
    - Uncategorized
    preview: metro-office-128-link-3-3.png
  BodyContent: |
    Have you ever tried to push data into TFS with Excel? I have, and it can often be the opposite of sweetness and light. The idea is to import Excel data into TFS with History.

    I was working with a team this week who used a SharePoint list to store their Requests for Change (RFC). I was easily able to export them from SharePoint by using the built in functionality to export to Excel. I did however want to make sure that there was no data loss when I pushed that data into TFS. Now, short of adding all of the columns that they had into the RFC work item the only way to do this would be to add all of the data to the history.

    Note: I usually use a custom CSV adapter that I created for the TFS Integration Tools but there were only 120 items and that can be a lot of overhead.

    I looked at a few tricks to use the existing functions to create my data but it just really became too complicated. It was too easy to make mistakes in the complicated mess that is excel nested functions. If I was using my CSV adapter it creates an HTML table for all of the fields and values and writes it into the history for… well… history.

    ![clip_image001](images/clip_image0011-1-1.png "clip_image001")
    { .post-img }

    I wanted to do the same so I cracked open one of my long forgotten skills… VBA. I know, I know.. It filled me with dread as well. However it was Farley simple to create a table by iterating over the columns and rows to get at the juicy data and return a simple table.

    ```
    Function CreateTable(things As Range, headers As Range)
      'Lists the Hyperlink Address for a Given Cell
      'If cell does not contain a hyperlink, return default_value

        Dim result As String
        result = "<table>"

        For i = 0 To headers.Cells.count - 1
        result = result & "<tr>"
        result = result + "<td>" + HTMLEncode(headers.Cells(i).Value) + "</td>"
        result = result & "<td>" + HTMLEncode(things.Cells(i).Value) + "</td>"
        result = result & "</tr>"
        Next i

        CreateTable = result + "</table>"
    End Function

    Function HTMLEncode(ByVal Text As String) As String
        Dim i As Integer
        Dim acode As Integer
        Dim repl As String
        HTMLEncode = Text
        For i = Len(HTMLEncode) To 1 Step -1
            acode = Asc(Mid$(HTMLEncode, i, 1))
            Select Case acode
                Case 32
                    'repl = "&nbsp;"
                Case 34
                    repl = "&quot;"
                Case 38
                    repl = "&amp;"
                Case 60
                    repl = "&lt;"
                Case 62
                    repl = "&gt;"
                Case 32 To 127
                    ' don't touch alphanumeric chars
                Case Else
                    repl = "&#" & CStr(acode) & ";"
            End Select
            If Len(repl) Then
                HTMLEncode = Left$(HTMLEncode, i - 1) & repl & Mid$(HTMLEncode, _
                    i + 1)
                repl = ""
            End If
        Next
    End Function

    ```

    I stole the second function from somewhere online (I think it was Stack Overflow) but the first was my own creation. If I was doing it again I would create a vertical rather than a horizontal table but I only had limited time to create this. The result of adding this custom function? A simple way to add this to just reference the cells, but I had my eyes set of a little awesome table work. Since my data was already in a table I cracked open the internet and trawled the documentation for Excel.

    ```
    =CreateTable(Table_owssvr_2[@], Table_owssvr_2[#Headers])
    ```

    You can easily reference the row of a table that you are on using the "@" symbol, and if you are outside the table you can go with "tableName\[@\]". It even works if you are on another sheet. If you are on line 5 you get row 5 from the table… weird.. But OK. Even better you can represent the headers row as "tablename\[#headers\]".

    That sounds simple but it was a lot of documentation to wade through to get to that simple result.

    ![clip_image002](images/clip_image0021-2-2.png "clip_image002")
    { .post-img }

    Now that I have a way to create the history field all I have to do is create a query in TFS with the same columns as the ones I want to import, plus the History column of course.

    1. Create copy of data to import on second sheet with only the columns we will map - You can use "tableNasme\[@\[columnName\]\]" to get each value.
    2. Add the function to create the History column
    3. Create Query in TFS that matches the list of columns
    4. Create operational table linked to TFS on new sheet
    5. Copy prepared 120 rows and insert them into the TFS linked table
    6. Massage data click "Publish"

    And you are done.

    ## Conclusion

    The only thing I do not like about this method over the CSV adapter for the Integration Platform is that all of the new work items have to go through the official flow of the process template. With the CSV adapter I can bypass the work item rules and just write what data I want into there. That way I can progress the states to whatever I want even if they don't exist and fix the data afterwards… better integrity, but more effort.

    Using Excel to import data into TFS is quick and easy. Took me about an hour to import the data and another hour to create and tests the data manipulation above.
  FilePath: C:\Users\MartinHinshelwoodNKD\source\repos\NKDAgility.com\site\content\resources\blog\2014\2014-05-28-import-excel-data-into-tfs-with-history\index.md
  FolderPath: C:\Users\MartinHinshelwoodNKD\source\repos\NKDAgility.com\site\content\resources\blog\2014\2014-05-28-import-excel-data-into-tfs-with-history
- FrontMatter:
    title: Mask password in Jenkins when calling TEE
    description: Learn how to mask passwords in Jenkins when using Team Explorer Everywhere to enhance security and prevent sensitive data exposure in your build logs.
    ResourceId: rG_NN58PQym
    ResourceType: blog
    ResourceContentOrigin: Human
    ResourceImport: true
    ResourceImportId: 10538
    ResourceImportSource: Wordpress
    ResourceImportOriginalSource: Wordpress
    date: 2014-05-21
    weight: 690
    creator: Martin Hinshelwood
    layout: blog
    resourceTypes: blog
    slug: mask-password-in-jenkins-when-calling-tee
    aliases:
    - /resources/rG_NN58PQym
    aliasesArchive:
    - /blog/mask-password-in-jenkins-when-calling-tee
    - /mask-password-in-jenkins-when-calling-tee
    - /resources/blog/mask-password-in-jenkins-when-calling-tee
    tags:
    - Software Development
    - Troubleshooting
    - Install and Configuration
    categories:
    - Uncategorized
    preview: naked-alm-jenkins-logo-7-7.png
  BodyContent: |
    When you use the release build plugin in Jenkins to create a new release the plugin inadvertently leaves your password in clear text in the log files. We need to be able to mask password in Jenkins when calling Team Explorer Everywhere (TEE) so that we meet security requirements.

    As you can imagine working at a bank, they get a little…squirmy… when they see or hear about passwords being stored on viewable in the clear. If you are using TFS to do builds from Jenkins then you are likely using the command line tools that come with Team Explorer Everywhere.

    ![clip_image001](images/clip_image001-1-1.png "clip_image001")
    { .post-img }

    If you are also using the Release Plugin and you create a release build then you will see the SCM password that you enter written in the clear in the log. Bit of a shock to my banking colleagues I can tell you. So much so that they called "critical blocker" for the migration to TFVC.

    ![clip_image002](images/clip_image002-2-2.png "clip_image002")
    { .post-img }

    However during the… conversation… they did say that they had a plugin installed that was supposed to [mask the passwords](https://wiki.jenkins-ci.org/display/JENKINS/Mask+Passwords+Plugin) when you do a build. Armed with that knowledge, and little other knowledge of Jenkins, I dived in to find a solution. Maybe it just needed more configuration…

    ![clip_image003](images/clip_image003-3-3.png "clip_image003")
    { .post-img }

    So I looked through the documentation and found that you can set variables for passwords and send the variable instead. The plugin will then mask it correctly…. So I thought… that’s for me!

    ![clip_image004](images/clip_image004-4-4.png "clip_image004")
    { .post-img }

    So I dutifully created a global password veriable called "MrHinshPas" (yes, I am testing with my own account) and once saved I should be able to use "$(MrHinshPass)" in places where I want the password replaced.

    ![clip_image005](images/clip_image005-5-5.png "clip_image005")
    { .post-img }

    Running another build and, wohoo, the password gets replaced.

    However why do I need to create a variable for this occurrence when it usually replaced things for other passwords in the list. So I went hunting around… I looked at server configuration. I looked at plugins and documentation.

    Eventually I looked in the build configuration and I found this…

    ![clip_image006](images/clip_image006-6-6.png "clip_image006")
    { .post-img }

    So for each specific job you can activate the "Mask passwords" option in the Build Environment section and all passwords are magically hidden in your builds. Awesome! How did I miss that…
  FilePath: C:\Users\MartinHinshelwoodNKD\source\repos\NKDAgility.com\site\content\resources\blog\2014\2014-05-21-mask-password-in-jenkins-when-calling-tee\index.md
  FolderPath: C:\Users\MartinHinshelwoodNKD\source\repos\NKDAgility.com\site\content\resources\blog\2014\2014-05-21-mask-password-in-jenkins-when-calling-tee
- FrontMatter:
    title: Configuring Jenkins to talk to TFS 2013
    description: Learn how to configure Jenkins to integrate with TFS 2013 seamlessly, ensuring a smooth transition for your Java teams. Enhance your build process today!
    ResourceId: ynMjy3Tn7hl
    ResourceType: blog
    ResourceContentOrigin: Human
    ResourceImport: true
    ResourceImportId: 10526
    ResourceImportSource: Wordpress
    ResourceImportOriginalSource: Wordpress
    date: 2014-05-07
    weight: 690
    creator: Martin Hinshelwood
    layout: blog
    resourceTypes: blog
    slug: configuring-jenkins-talk-tfs-2013
    aliases:
    - /resources/ynMjy3Tn7hl
    aliasesArchive:
    - /blog/configuring-jenkins-talk-tfs-2013
    - /configuring-jenkins-talk-tfs-2013
    - /configuring-jenkins-to-talk-to-tfs-2013
    - /blog/configuring-jenkins-to-talk-to-tfs-2013
    - /resources/blog/configuring-jenkins-talk-tfs-2013
    tags:
    - Software Development
    - Install and Configuration
    - System Configuration
    categories:
    - Uncategorized
    preview: naked-alm-jenkins-logo-9-9.png
  BodyContent: |
    I am working quite a lot with some Java teams at the moment who are using SVN and Jenkins. We are moving them over to TFS and TF Build and we wanted to make sure that we were minimally disruptive to first I need to configuring Jenkins to talk to TFS 2013.

    At the moment all of the source code, builds, and assets are owned outright by one individual and stored on a single Linux box. This will change over time as necessitates a scalable enterprise solution but for now I just want to move the source.

    ![clip_image001](images/clip_image0013-1-1.png "clip_image001")  
    { .post-img }
    Figure: Jenkins connected to SVN

    In order to begin the move I first need to reassure all of those involved that, yes, TFS can indeed store source code. That is even before I try to convince them that TFS can do builds. To that end I intend to only change one thing. Move the Source Code to TFS…

    ![clip_image002](images/clip_image0022-2-2.png "clip_image002")  
    { .post-img }
    Figure: Jenkins connected to TFS

    The teams that use this are already using TFS for Backlog and Test management along with other things so it’s a simple step for them.

    ## Installing the Team Foundation Server Plugin for Jenkins

    It is worth noting that there is a plugin for Team Foundation Server for Jenkins. Although this plugin was built against, and is still tested against, TFS 2008 it works just fine with 2012. I have not checked for 2013, but as it just builds a command line it does not care itself. Indeed we are using the latest version of TE for Linux which was released just a few months ago…

    ![clip_image003](images/clip_image0032-3-3.png "clip_image003")  
    { .post-img }
    Figure: TFS Plugin for Jenkins

    The [Jenkins Plugin for Team Foundation Server](https://wiki.jenkins-ci.org/display/JENKINS/Team+Foundation+Server+Plugin) is old. Although it has been updated recently it has been around since the days of TFS 2005 and still works great.

    ![clip_image004](images/clip_image0041-4-4.png "clip_image004")  
    { .post-img }
    Figure: Managing Plugins in Jenkins

    If you login to Jenkins and head over to "Manage Jenkins | Manage Plugins" we can go ahead and get it installed.

    ![clip_image005](images/clip_image005-5-5.png "clip_image005")  
    { .post-img }
    Figure: Install the TFS Plugin for Jenkins

    Nicely Jenkins has an adequate interface for installing plugins that are publically hosted. Makes things a little simpler than mucking about trying to get things installed yourself.

    ## Configuring your first TFS build from Jenkins

    For the very first build I wanted a really simple app. The Java build team here had a "Test-Project" that they used to debug and test their own builds so I just reused that.

    ![clip_image006](images/clip_image006-6-6.png "clip_image006")  
    { .post-img }
    Figure: Check-in some Java code

    First step is to get the source code and check it in. We already have a directive from management to ditch the history (good call) which makes it easy to move source. I was emailed a zip file, I unpacked it and checked it in. I should note that I have very little experience with Java and almost zero with compiling it. I am just taking what I have been given and checking it in.

    ![clip_image007](images/clip_image007-7-7.png "clip_image007")  
    { .post-img }
    Figure: Configure the Jenkins Build for TFS

    Now head over to "Jenkins | Build Tools" and open the existing build (that used to work against SVN.) Scroll down to the "Source Code Management" section and select the radio button for Team Foundation Server. The new options will light up. Now this system does not understand what a collection is (introduced in 2010) and it really does not matter. Anywhere you previously (2005 or 2008) entered the 'server URL' you just enter the full URL to the collection. So here where it says 'Server URL' we just fill out the [https://my.fullyqualified.com/tfs/mycollection](https://my.fullyqualified.com/tfs/mycollection) in the box.

    Note If you are using TFS Git you will need to install a Git plugin for Jenkins to get the source.

    The project path is also fairly strait forward. It’s the full path to the folder that you want to get from TFVC. If you are used to TF Builds remember that you will not be able to configure complex mappings in here. Indeed the SVN tool has the same limitation so it looks like A Jenkins thing. For this scenario it makes things easy though. I just have one path to get.

    You will need to set the username and password that you want to connect with but that’s about it. Save and build!

    ![clip_image008](images/clip_image008-8-8.png "clip_image008")
    { .post-img }

    It actually took me 5 builds to get it right as I got the domain name wrong…

    ## Conclusion

    It is so ridiculously easy to move your Source over to TFS when you are using Jenkins that there really is no reason not to. Once there however you really want to be looking at replacing Jenkins with TF Build.

    Things we loose by not using TF Build:

    - **Associated Change sets** – Team Build automatically associates a list of change sets that are included in the build
    - **Associated Work Items** – Team Build analysis the relationships and also associates Work Items with a build. Indeed it walks the work item tree (parent) and maintains that association in the chain.
    - **Test Impact Analysis** – When tests execute TFS collects paths through the code, even across multiple servers, and diffs this with the code changed for a new build. Giving you a list of tests that you MUST run. This works with both Manual and Automated tests (and unit tests). Designed to reduce your test matrix.
    - **Unit Tests and Coverage associated with a build** – You can trend your quality over time.
    - **Manual Tests associated with a Build** – If test results are associated with a build you can see, dynamically, the current state of your test plan, build on build.

    If you only get your Source into TFS then that is just the first step. A necessary one, but only the first.
  FilePath: C:\Users\MartinHinshelwoodNKD\source\repos\NKDAgility.com\site\content\resources\blog\2014\2014-05-07-configuring-jenkins-talk-tfs-2013\index.md
  FolderPath: C:\Users\MartinHinshelwoodNKD\source\repos\NKDAgility.com\site\content\resources\blog\2014\2014-05-07-configuring-jenkins-talk-tfs-2013
- FrontMatter:
    title: Migrating to office 365 from Google Mail
    description: Navigate the challenges of migrating from Google Mail to Office 365 with expert tips and PowerShell solutions. Simplify your transition today!
    ResourceId: w2vG6eEu5BH
    ResourceType: blog
    ResourceContentOrigin: Human
    ResourceImport: true
    ResourceImportId: 10502
    ResourceImportSource: Wordpress
    ResourceImportOriginalSource: Wordpress
    date: 2014-04-30
    weight: 1000
    creator: Martin Hinshelwood
    layout: blog
    resourceTypes: blog
    slug: migrating-to-office-365-from-google-mail
    aliases:
    - /resources/w2vG6eEu5BH
    aliasesArchive:
    - /blog/migrating-to-office-365-from-google-mail
    - /migrating-to-office-365-from-google-mail
    - /resources/blog/migrating-to-office-365-from-google-mail
    tags:
    - Install and Configuration
    categories:
    - Uncategorized
    preview: metro-office-128-link-3-3.png
  BodyContent: |
    A few months ago I decided to make use of Office 365 but I have run into a bunch of roadblocks. Migrating to office 365 from Google Mail as it seams that Office 365 and Google Mail are not the best of friends. They seam to be in a state of cold war.

    There are a number of options that are available for migrating / synching your email and data from one mail system to Office365.

    - Add IMAP Account
    - Add POP account
    - Batch IMAP cutover

    Of these options I would prefer the first IMAP option. There is no way I am going through the trauma of POP so IMAP it is…

    ## Attempt 1:  IMAP Subscription through the UI

    In an ideal world I want to use IMAP between Office 365 and Gmail for a time so that I can review the new service but maintain the ability to return to the old one if its promises are not met. Unfortunately if you try and configure IMAP with Gmail you will get a message saying that the server is not supported. What? Its just an IMAP server like I use in Outlook with no problems. Delving into the documentation results in:

    > Outlook Web App supports IMAP access for most services, **except** Gmail. So please use POP instead. To allow POP access from Gmail, see [Turn on POP Access Before Connecting to Your Gmail Account](http://help.outlook.com/en-US/140/dd181952.aspx).[IMAP with Gmail not working?](http://community.office365.com/en-us/forums/158/t/1944.aspx)

    For some reason, and I am sure not a technical one, Office 365 will not support IMAP from Google.

    ## Attempt 2: IMAP Subscription through PowerShell

    So if it will not work through the UI maybe it is an advanced feature that can only be done through command line. Really I was just hoping that whomever implemented the feature added the block in the UI rather than in the actual system. I spent quite some time figuring out how to execute the required PowerShell.

    ```
    Import-Module MSOnline
    $O365Cred = Get-Credential
    $O365Session = New-PSSession –ConfigurationName Microsoft.Exchange -ConnectionUri https://ps.outlook.com/powershell -Credential $O365Cred -Authentication Basic -AllowRedirection
    Import-PSSession $O365Session
    Connect-MsolService –Credential $O365Cred
    New-IMAPSubscription -Name "Gmail_IMAP_Subscription" -Mailbox "martin@nakedalm.com" -EmailAddress martin@nakedalm.com -IncomingUserName martin@hinshelwood.com -IncomingPassword (ConvertTo-SecureString -String 'notonyournelly' -AsPlainText -Force) -IncomingServer imap.gmail.com  -IncomingSecurity Ssl -IncomingPort 993

    ```

    As with all PowerShell it is poorly documented but in the end I executed the above PowerShell and my hopes of a sucessfukll result were dashed.

    ![](images/041614_1437_Office365an1-1-1.png)
    { .post-img }

    So they did implement the block deep down in the software. Really I knew that they would have…

    ## Attempt 3: IMAP Migration through PowerShell

    The third option is to do a PowerShell IMAP Migration. Yes, that is using IMAP to do a one-time move from Google Mail to Office 365. While some of the previous PowerShell was reusable but figuring it out was even more of a pain than for a single account. Here instead of creating a single thing we have to create an async batch session that we can then hook into and see how it is going. This may take a large amount of time to run if you have a lot of mail. I have over 7GB of mail in Google.

    ```
    Import-Module MSOnline
    $O365Cred = Get-Credential
    $O365Session = New-PSSession –ConfigurationName Microsoft.Exchange -ConnectionUri https://ps.outlook.com/powershell -Credential $O365Cred -Authentication Basic -AllowRedirection
    Import-PSSession $O365Session
    Connect-MsolService –Credential $O365Cred
    $IMAPMigrationEndPoint = New-MigrationEndpoint -IMAP -Name MrHinshIMAPMigration -MaxConcurrentMigrations 1 -RemoteServer imap.gmail.com -Port 993 -Security SSL
    New-MigrationBatch -Name MrHinshIMAPMigration -SourceEndpoint $IMAPMigrationEndPoint.Identity -AutoRetryCount 4 -BadItemLimit 50 -CSVData ([System.IO.File]::ReadAllBytes(“C:\temp\test.csv”)) -AutoStart

    ```

    Once you have executed this ommand Office 365 will connect to all of the accounts stored in the CSV and move them across to Office 365. Slowly… and because it is slowly you need some way to check up on its status.

    ```
    #Get Status
    Get-MigrationBatch | fl
    ```

    When you execute the command you get a list of sync's and the number of accounts it is migrating.

    ![](images/041614_1437_Office365an2-2-2.png)
    { .post-img }

    This is really the only way to move from Google Mail to Office 365 and it does work. I say mail coming in almost immediately and it took maybe a few days for all 7gb to come across.

    ## Conclusion

    Unfortunately Microsoft are not providing the service they should which results in my being unable to move more than just myself to Office365. I have a bunch of family member, only some of which will want to move… and I can't do a partial move.

    I have now been over on Office 356 for about 3 months and I have been extremely happy with my business account. I would recommend Office 365 for both small and large organisations. I am using it for business and the integration of Lync, Outlook, and a SharePoint instance gives me lots of flexibility for only £40 for the year. Pretty good really and I believe it to be much cheaper for large organisations if they are running exchange properly.
  FilePath: C:\Users\MartinHinshelwoodNKD\source\repos\NKDAgility.com\site\content\resources\blog\2014\2014-04-30-migrating-to-office-365-from-google-mail\index.md
  FolderPath: C:\Users\MartinHinshelwoodNKD\source\repos\NKDAgility.com\site\content\resources\blog\2014\2014-04-30-migrating-to-office-365-from-google-mail
- FrontMatter:
    title: Be a kid again and upgrade to Windows Phone 8.1 Developer Preview
    description: Rediscover the joy of technology with the Windows Phone 8.1 Developer Preview. Upgrade now for exciting features and a fresh experience!
    ResourceId: CDwU__r19XS
    ResourceType: blog
    ResourceContentOrigin: Human
    ResourceImport: true
    ResourceImportId: 10515
    ResourceImportSource: Wordpress
    ResourceImportOriginalSource: Wordpress
    date: 2014-04-22
    weight: 1000
    creator: Martin Hinshelwood
    layout: blog
    resourceTypes: blog
    slug: kid-upgrade-windows-phone-8-1-developer-preview
    aliases:
    - /resources/CDwU__r19XS
    - /resources/blog/be-a-kid-again-and-upgrade-to-windows-phone-8.1-developer-preview
    aliasesArchive:
    - /blog/kid-upgrade-windows-phone-8-1-developer-preview
    - /kid-upgrade-windows-phone-8-1-developer-preview
    - /be-a-kid-again-and-upgrade-to-windows-phone-8-1-developer-preview
    - /blog/be-a-kid-again-and-upgrade-to-windows-phone-8-1-developer-preview
    - /resources/blog/kid-upgrade-windows-phone-8-1-developer-preview
    - /resources/blog/be-a-kid-again-and-upgrade-to-windows-phone-8.1-developer-preview
    tags:
    - Windows
    categories:
    - Uncategorized
    preview: nakedalm-windows-logo-6-6.png
  BodyContent: |
    I love new things not just because they are new but because they are exiting. Discovery is something that we lose as we get older but it should be nurtured so be a kid again and upgrade to Windows Phone 8.1.

    As with most people that use a Windows Phone I love it. I am however unusual in that I have had Windows running on my phone since the original Orange SPV (from HTC) arrived in 2001. That’s right… I have had a smartphone in my pocket for more than 10 years.

    It may not have been perfect, in fact Windows Mobile was most defiantly not that. However I did have a browser, the internet and a camera. Apple copied this for their devices in 2009 and arguably perfected the interactive nature of these devices and propelled them into mainstream. Now as Apple and Android struggle to innovate the underdog (Microsoft for a change) have been slowly introducing us to Windows Mobile.

    It has been a bumpy road and Windows Phone 8 was a vast improvement over Windows Phone 7. My mum still has a 7 device, that she still insists is new, and it is a stark contrast whenever I have to use it. I recently upgraded my dad to a Nokia 520 which he thinks is fabulous and I have been using a Nokia 1020 for a while now.

    ![clip_image001](images/clip_image0012-1-1.png "clip_image001")
    { .post-img }

    My home screen is pretty strait forward and I only really use a few apps. As an aside there is a fascination with the platform that has the most apps however in my experience you have to search through the sewer for the gems. The only apps I use are:

    - **NextGen** - An awesome feedreader that has seen me through Google Reader and over seamlessly to Feedly.
    - **Facebook** - You know… for posting crap and reading memes
    - **Twitter** - Another app for posting crap.
    - **Bufferapp** - This is a plugin for Windows Phone Sharing hub that adds the capability to post through Bufferapp. Bufferapp lets me share once and it schedules posts across multiple platforms like LinkedIn, Facebook, Twitter, and Google.
    - **4th & Mayor** - For checking in to a location so someone can come and rob your house while you are out.
    - **Amazon Mobile** - I see stuff, I scan it… I then get it delivered at home :) Perfect traveling companion.
    - **Endamondo** \- For tracking those long runs…
    - **Here Maps** - The best mapping app I have ever used comes from Nokia and rocks…
    - **Audible** \- I listen to a lot of audio book and this little app has kept me sane when traveling. I spend an inordinate amount of time in airports and on trains…

    And that’s really it. I have tones of other crap on there, mostly for the kids. Oh, did you know that Windows Phone has Kid Mode? Kid Mode is a little walled garden that you pick the apps and games that are listed. It has its own home screen that you can enable without entering your password. This feature was added with Windows Phone 8 but I find it so useful I thought it best to reiterate it.

    ![clip_image002](images/clip_image002-2-2.jpg "clip_image002")
    { .post-img }

    If you are running Windows Phone 8 on your phone right now then you can get the developer preview of Windows Phone 8.1 (You do not need to be a developer):

    1. Go register free at [http://appstudio.windowsphone.com](http://appstudio.windowsphone.com).
    2. Then go to "Windows Phone Preview for Developers" and install the "Preview for Developers" app.
    3. Go to Settings, then System Update and update your phone. You may need to do this more than once. My Nokia 1020 needed 2 updates before I was presented with the Windows Phone 8.1 Preview.

    ![clip_image003](images/clip_image003-3-3.jpg "clip_image003")
    { .post-img }

    Although it says 5-10 on the first update and 15-30 on the 8.1 update you need to take this with a pinch of salt. It really depends on how much data you have on your phone and how fast your internet connection is. The first two updates on my Nokia 1020 took way longer than 10 minutes (more like the 15-30) however my connection speed was pretty slow in the Alps last week. WiFi was not much better as it was a satellite link with high latency.

    ![clip_image004](images/clip_image004-4-4.jpg "clip_image004")
    { .post-img }

    If you update your phone to Windows Phone 8.1 Preview today you will at least get the RTM when it becomes available and likely there will be a couple of refreshes. Note though that this is a "Developer Preview" and don't expect everything to work. So far I have the following issues:

    - **Facebook can't post pictures** - For some reason I have been unable to post pictures with the built in Facebook app. It just gets stuck on the send.. I have been able to post videos so I can only think it is a bug. Looks like this only affects slow connections like I had last week.
    - **No Facebook Chat** - Not sure if it is just gone, or not yet implemented, but I have used the integrated chat for a while and I will miss it if it is gone.
    - **Skype will not start** - Not sure if it is because I changed my default Microsoft ID alias or if it is just a little flakie but it just exists when it tries to get my details.
    - **No Cortana** \- You can do some contortions with region and language to make it work. However who wants everything in American?

    ![clip_image005](images/clip_image005-5-5.jpg "clip_image005")
    { .post-img }

    For me the downsides are worth it and I love the new features and home screen. I [agree with Scott Hanselman](http://www.hanselman.com/blog/WindowsPhone81HasMyAttentionNow.aspx) that this not only brings Windows Phone up to the level of iOS and Android but takes it beyond.

    Go on, get Windows Phone 8.1 Preview today and influence it.
  FilePath: C:\Users\MartinHinshelwoodNKD\source\repos\NKDAgility.com\site\content\resources\blog\2014\2014-04-22-kid-upgrade-windows-phone-8-1-developer-preview\index.md
  FolderPath: C:\Users\MartinHinshelwoodNKD\source\repos\NKDAgility.com\site\content\resources\blog\2014\2014-04-22-kid-upgrade-windows-phone-8-1-developer-preview
- FrontMatter:
    title: Blogging from 2500 meters
    description: Join Martin Hinshelwood as he blogs from the French Alps, sharing insights on skiing, tech challenges, and creative writing amidst stunning mountain views.
    ResourceId: uxd-czfPeHg
    ResourceType: blog
    ResourceContentOrigin: Human
    ResourceImport: true
    ResourceImportId: 10509
    ResourceImportSource: Wordpress
    ResourceImportOriginalSource: Wordpress
    date: 2014-04-17
    weight: 1000
    creator: Martin Hinshelwood
    layout: blog
    resourceTypes: blog
    slug: blogging-2500-meters
    aliases:
    - /resources/uxd-czfPeHg
    aliasesArchive:
    - /blog/blogging-2500-meters
    - /blogging-2500-meters
    - /blogging-from-2500-meters
    - /blog/blogging-from-2500-meters
    - /resources/blog/blogging-2500-meters
    categories:
    - Uncategorized
    preview: nakedalm-logo-260-7-7.png
    tags: []
  BodyContent: |
    I am currently 2k meters up the side of a mountain in the French Alps and while skiing is fun it takes its toll on my knees that are already a bit dodgy. Thus I have been Skiing in the mornings and sunning myself in the afternoons. It is about 25 degrees Celsius here during the day and tad sunny.

    [![](images/041614_1456_Bloggingfro1-1-1.jpg)](http://nkdagility.com/wp-content/uploads/2014/04/041614_1456_Bloggingfro1-1-1.jpg)
    { .post-img }

    I can only stand so much time in the sun at once so I have been going through some of my older blog posts that I started and never published and just getting them done. I should also note that my main computer, a Surface Pro 2, is away for repairs (I dropped it) and in the mean time I only have my Surface 2 to keep me company. So no Windows Live Writer. I have OneNote, Word, and internet access to WordPress.

    ![](images/041614_1456_Bloggingfro2-2-2.jpg)
    { .post-img }

    If truth be told I also have my daughters Ionia w520 that has an Intel processor but I can only imagine the screaming if I tried to pry it from her hands long enough to publish a post.

    For quite a while now I have been writing all of my posts in OneNote. Not because I love it (although it is pretty good, and growing on me) but because I need something that works on all my devices so that I can edit everywhere. Yes, I have ideas when I only have my phone and need to note these down as well.

    ![](images/041614_1456_Bloggingfro3-3-3.png)
    { .post-img }

    Both OneNote desktop and the modern version of it give me really nice capabilities and I like the limited editing that I get in modern OneNote. This lets me focus on the content and hopefully build better posts.

    ![](images/041614_1456_Bloggingfro4-4-4.png)
    { .post-img }

    Unfortunately the only way to post from OneNote is to proxy through Word with the "Send to Blog" option. Unfortunately Word is probably one of the worst blog editors as it speckles your posts with unnecessary with hard coded text sizes and fonts.

    ![](images/041614_1456_Bloggingfro5-5-5.png)
    { .post-img }

    After publishing my most resent post it took about 10 minutes sitting editing the HTML before I could publish. But I could indeed publish. I would however like to be able to paste strait into WordPress from OneNote but you lose all of your pictures.

    ![](images/041614_1456_Bloggingfro6-6-6.png)
    { .post-img }

    Don't get me wrong, if you look at the HTML generated from Word from even 5 years ago it would be far worse. It's better to the point it is even worth editing it to post rather than writing from scratch and drawing the pictures by hand.

    Word: You can do better.
  FilePath: C:\Users\MartinHinshelwoodNKD\source\repos\NKDAgility.com\site\content\resources\blog\2014\2014-04-17-blogging-2500-meters\index.md
  FolderPath: C:\Users\MartinHinshelwoodNKD\source\repos\NKDAgility.com\site\content\resources\blog\2014\2014-04-17-blogging-2500-meters
- FrontMatter:
    title: Using multiple email alias with your existing Microsoft ID
    description: Learn how to manage multiple email aliases with your Microsoft ID, simplifying your online experience and keeping your accounts organized. Discover more!
    ResourceId: 3drabM1j0WE
    ResourceType: blog
    ResourceContentOrigin: Human
    ResourceImport: true
    ResourceImportId: 10496
    ResourceImportSource: Wordpress
    ResourceImportOriginalSource: Wordpress
    date: 2014-04-16
    weight: 1000
    creator: Martin Hinshelwood
    layout: blog
    resourceTypes: blog
    slug: using-multiple-email-alias-existing-microsoft-id
    aliases:
    - /resources/3drabM1j0WE
    aliasesArchive:
    - /blog/using-multiple-email-alias-existing-microsoft-id
    - /using-multiple-email-alias-existing-microsoft-id
    - /using-multiple-email-alias-with-your-existing-microsoft-id
    - /blog/using-multiple-email-alias-with-your-existing-microsoft-id
    - /resources/blog/using-multiple-email-alias-existing-microsoft-id
    tags: []
    categories:
    - Uncategorized
    preview: nakedalm-windows-logo-7-7.png
  BodyContent: |
    Did you know that you can have multiple email alias associated with an existing Microsoft ID (formally Live ID). The first thing that you should do if you receive a new email address, wither corporate or personal, is associate it with your current Microsoft ID.

    How many Microsoft ID's are you currently managing? Do you really have a need for more than one, or was this forced upon you. I just started working with [Inmeta Consulting](http://inmeta.no) as a freelance consultant and while they gave me an email address I really don't want to have to create a new Microsoft ID when they add me to their Visual Studio Online account or other live services. They may also provide me with an MSDN account that we [all know needs to be assigned to an individual](http://nkdagility.com/do-you-want-visual-studio-ultimate-for-free-do-you-have-msdn/) using a Microsoft ID.

    There is however hope. Microsoft added the ability to have multiple aliases to a single account to their single-sign-on account service. Indeed you can also switch which email allies is the primary one used for login. This allows you to have one account, or at most, one for business and one for personal. You can even log into the service using any one of the emails that you have associated.

    ## Adding your new email alias to your existing Microsoft ID

    In order to add an additional email you need to head over to [http://account.live.com](http://account.live.com) and select Account Aliases.

    ![](images/041614_1219_Usingmultip1-1-1.png)
    { .post-img }

    Here you can scroll down to the bottom and click 'Add alias' to add a new one. I am not sure if there is a limit to what you can have here but I have nine already so the limit would be fairly large.

    ![](images/041614_1219_Usingmultip2-2-2.png)
    { .post-img }

    You can select to create a new email address on 'outlook.com' to add to your list. In this case I am taking the other option to add an existing email address.

    ## Changing the email alias that is used by default on your existing Microsoft ID

    Although you can log in with any of the email addresses that you have defined as aliases you may want to change the default. My default, since this account was been created, was always on my @hinshelwood.com domain. It has been more than 15 years since I used a Hotmail address but you can use the same method here with those accounts as well.

    ![](images/041614_1219_Usingmultip3-3-3.png)
    { .post-img }

    We first need to head back to [http://account.live.com](http://account.live.com) where we go and see the list of aliases available. You will note a 'make primary' beside all but your default aliases listed. I want to make my @nakedalm.com email the default for display as that is the one that I use most often now.

    ![](images/041614_1219_Usingmultip4-4-4.png)
    { .post-img }

    Because this could be a fairly traumatic process you will need to confirm that this is what you want. For all of your services that are liked to live your display name may change.

    ![](images/041614_1219_Usingmultip5-5-5.png)
    { .post-img }

    Once you have accepted the change you should see a message stating that it may take 48 hours to be reflected across all of your services. Remember that this will affect MSDN, VSO, Xbox, Windows Phone, and Windows to name but a few of the services that you use Microsoft ID (formally Live ID) with.

    ## Closing an old account

    If you need you can close the love account that you are using. In this case I had previously created an account for a company that had provided me an email address. You will need to 'close account' before you can reuse that email.

    ![](images/041614_1219_Usingmultip6-6-6.png)
    { .post-img }

    I have not determined how long it takes to 'close account' but so far I can't reuse the email after 3 days. I raised a support question when I could not reuse it immediately and was told that there is a 90 day cooling off period when deleting an account.

    ## Conclusion

    There is now no reason to end up in the multi-account nightmare that many of my colleagues have allowed to happen. You can maintain the minimum you need to get the job done and even switch the primary email.
  FilePath: C:\Users\MartinHinshelwoodNKD\source\repos\NKDAgility.com\site\content\resources\blog\2014\2014-04-16-using-multiple-email-alias-existing-microsoft-id\index.md
  FolderPath: C:\Users\MartinHinshelwoodNKD\source\repos\NKDAgility.com\site\content\resources\blog\2014\2014-04-16-using-multiple-email-alias-existing-microsoft-id
- FrontMatter:
    title: Does your organisation have both Project Mangers as well as Product Owners?
    description: Explore the challenges of having both Project Managers and Product Owners in your organization. Discover how to achieve true agility and team alignment.
    ResourceId: 2s_5i0e0NBx
    ResourceType: blog
    ResourceContentOrigin: Human
    ResourceImport: true
    ResourceImportId: 10489
    ResourceImportSource: Wordpress
    ResourceImportOriginalSource: Wordpress
    date: 2014-04-10
    weight: 750
    creator: Martin Hinshelwood
    layout: blog
    resourceTypes: blog
    slug: organisation-project-mangers-well-product-owners
    aliases:
    - /resources/2s_5i0e0NBx
    aliasesArchive:
    - /blog/organisation-project-mangers-well-product-owners
    - /organisation-project-mangers-well-product-owners
    - /does-your-organisation-have-both-project-mangers-as-well-as-product-owners-
    - /blog/does-your-organisation-have-both-project-mangers-as-well-as-product-owners-
    - /resources/blog/organisation-project-mangers-well-product-owners
    tags: []
    categories:
    - Product Management
    - Scrum
    preview: nakedalm-experts-professional-scrum-5-5.png
  BodyContent: |
    This is the dysfunction of teams with Project Managers as well as Product Owners. Does your organisation have both Project Mangers as well as Product Owners? If so, to whom do your teams report? What does it depend on? Because if it does depend then you are doing a disservice to your teams and will have difficulty moving towards greater agility. I have a customer right now that has this situation with cross application teams however there are additional complications.

    ![clip_image001](images/clip_image0011-1-1.png "clip_image001")  
    { .post-img }
    Figure: Project Hierarchy

    Project Managers are assigned projects that might operate against multiple products. Product Owners are given products. This results in often an application that while it has a single Product Owner could have multiple teams run by multiple project managers. Unless all of the team members have ADHD they are going to have issues with multi-tasking.

    ![clip_image002](images/clip_image0021-2-2.png "clip_image002")  
    { .post-img }
    Figure: Team A supports two masters

    Here is where it then gets a little tricky. We have Product Owners that want a view of the work that is under way from the perspective of the Product, which is reasonable. But they definitely do not want the same view as the Project Manager who cares about work done that achieves his project. Since budget is done at the 'project' level this means that funding is in the hands of the Project Manager and there is no way that a Product Owner can be accountable and responsible for value delivery. So while the Project Manager owns a budget and work breakdown the Product Owner owns the Application and breakdown. While the PO is supposed to be accountable and responsible for the delivery of value within the scope of the Product a 'Project' might have a scope that transcends product and is the responsibility of the Project Manager. Man what a mess…

    ![clip_image003](images/clip_image0031-3-3.png "clip_image003")   
    { .post-img }
    Figure: Team has no idea who to listen to

    The result is a dysfunctional situation where a team can have many masters. Not only are they beholden unto a project manager, they may have to serve more than one Product Owner. This breaks one of the few golden rules of agile software development: The team should have but one master. If you are moving towards agility then you need to avoid having more than one person in the role of Product Owner as much as possible. The Team will end up context switching and having to split time between Product Owners backlogs even if they use one combined view to make it seam like they have one order.

    ![clip_image004](images/clip_image004-4-4.png "clip_image004")   
    { .post-img }
    Figure: Product Owner provides a single point of order

    The only way to combat this is to reorganise the organisation so that the issue does not exist. You should reform the Teams around the Product Owners (and thus the products.) The Project Managers then become another set of stakeholders that the Product Owner must listen to. This is the only way to reach agility with all of those voices pulling the team is different directions.

    However the customer is large, which is no excuse. However in large organisations these kinds of things do not change quickly. It will eventually, but that may take many years and a lot of courage from management to achieve. In the mean time you, like me, may still need still need to model this is TFS to allow Teams and Product Owners to work together and for Project Managers to understand the work that they are doing.
  FilePath: C:\Users\MartinHinshelwoodNKD\source\repos\NKDAgility.com\site\content\resources\blog\2014\2014-04-10-organisation-project-mangers-well-product-owners\index.md
  FolderPath: C:\Users\MartinHinshelwoodNKD\source\repos\NKDAgility.com\site\content\resources\blog\2014\2014-04-10-organisation-project-mangers-well-product-owners
- FrontMatter:
    title: Professional Application Lifecycle Management with Visual Studio 2013
    description: Discover the updated guide to Application Lifecycle Management with Visual Studio 2013. Enhance your software development with expert insights and new features!
    ResourceId: xeEpDhzecta
    ResourceType: blog
    ResourceContentOrigin: Human
    ResourceImport: true
    ResourceImportId: 10482
    ResourceImportSource: Wordpress
    ResourceImportOriginalSource: Wordpress
    date: 2014-04-07
    weight: 790
    creator: Martin Hinshelwood
    layout: blog
    resourceTypes: blog
    slug: professional-application-lifecycle-management-visual-studio-2013
    aliases:
    - /resources/xeEpDhzecta
    aliasesArchive:
    - /blog/professional-application-lifecycle-management-visual-studio-2013
    - /professional-application-lifecycle-management-visual-studio-2013
    - /professional-application-lifecycle-management-with-visual-studio-2013
    - /blog/professional-application-lifecycle-management-with-visual-studio-2013
    - /resources/blog/professional-application-lifecycle-management-visual-studio-2013
    tags:
    - Application Lifecycle Management
    categories:
    - Uncategorized
    preview: nakedalm-experts-visual-studio-alm-2-2.png
  BodyContent: |
    About 6 months ago I was approached by Mickey to help him on the third edition of Professional Application Lifecycle Management with Visual Studio 2013. I jumped at the chance, only to be in dismay at the amount of work, and now relieved that it is all over. I could not believe the amount of work that goes into producing a book of the calibre and while fun, deadlines were not...

    [![image](images/image8-1-1.png "image")](http://nkdalm.net/ProALMwithVS13)Professional Application Lifecycle Management with Visual Studio 2013 is now available in the USA from [Amazon.com](http://nkdalm.net/ProALMwithVS13). It will be available on 29th April 2014 in the UK on [Amazon.co.uk](http://nkdalm.net/ProALMwithVS13uk) who is currently taking pre-orders. This was a monumental effort and I would not have been able to without the support from Brian and Mickey who had both been through the trauma of writing a book before.
    { .post-img }

    It really was hard going mapping the new features in the 2013 version of the product to the previous editions as well as making sure that the recommendations stayed current. For example when the book was originally written the MSF for Agile Software Development process template was the default out of the box and there really was no implementation of Scrum in TFS. You know my [feelings on the MSF for Agile Software Development template](http://nkdagility.com/agile-vs-scrum-process-templates-team-foundation-server/). This changed with Visual Studio 2012 when the Visual Studio Scrum template became the default out of the box and I felt that it was high time that the foremost ALM book for Visual Studio reflected reality a little better and had a little more agility to it.

    > **Ramp up your software development with this comprehensive resource**
    >
    > Microsoft's Application Lifecycle Management (ALM) makes software development easier and now features support for iOS, MacOS, Android, and Java development. If you are an application developer, some of the important factors you undoubtedly consider in selecting development frameworks and tools include agility, seamless collaboration capabilities, flexibility, and ease of use. Microsoft's ALM suite of productivity tools includes new functionality and extensibility that are sure to grab your attention. _Professional Application Lifecycle Management with Visual Studio 2013_ provides in-depth coverage of these new capabilities. Authors Mickey Gousset, Martin Hinshelwood, Brian A. Randell, Brian Keller, and Martin Woodward are Visual Studio and ALM experts, and their hands-on approach makes adopting new ALM functionality easy.
    >
    > - Streamline software design and deployment with Microsoft tools and methodologies
    > - Gain a practical overview of ALM with step-by-step guides and reference material
    > - Case studies illustrate specific functionality and provide in-depth instruction
    > - Use new capabilities to support iOS, MacOS, Android and Java development
    > - Discover this comprehensive solution for modeling, designing, and coordinating enterprise software deployments
    > - Over 100 pages of new content, forward-compatible with new product releases
    >
    > _Professional Application Lifecycle Management with Visual Studio 2013_ provides a complete framework for using ALM to streamline software design and deployment processes using well-developed Microsoft tools and methodologies. _Professional Application Lifecycle Management with Visual Studio 2013_ is your guide to make use of newly-available ALM features to take your enterprise software development to the next level

    If you liked the previous editions then this new revision has been updated for 2013 with new chapters on Git and Release Management. You can [buy on Amazon.com](http://nkdalm.net/ProALMwithVS13 "Buy Professional Application Lifecycle Management with Visual Studio 2013 on Amazon.com") if you are in the US (Kindle version available as well.) If you are in the UK you can [pre-order on Amazon.co.uk](http://nkdalm.net/ProALMwithVS13uk "Buy Professional Application Lifecycle Management with Visual Studio 2013 on Amazon.co.uk") for a 30th April release.
  FilePath: C:\Users\MartinHinshelwoodNKD\source\repos\NKDAgility.com\site\content\resources\blog\2014\2014-04-07-professional-application-lifecycle-management-visual-studio-2013\index.md
  FolderPath: C:\Users\MartinHinshelwoodNKD\source\repos\NKDAgility.com\site\content\resources\blog\2014\2014-04-07-professional-application-lifecycle-management-visual-studio-2013
- FrontMatter:
    title: Upgrade your server to Windows Server 2012 R2 Update 1
    description: Upgrade your server to Windows Server 2012 R2 Update 1 with our step-by-step guide. Ensure your systems are up-to-date for optimal performance and security.
    ResourceId: KyDt7UnJIhX
    ResourceType: blog
    ResourceContentOrigin: Human
    ResourceImport: true
    ResourceImportId: 10472
    ResourceImportSource: Wordpress
    ResourceImportOriginalSource: Wordpress
    date: 2014-04-03
    weight: 1000
    creator: Martin Hinshelwood
    layout: blog
    resourceTypes: blog
    slug: upgrade-server-windows-server-2012-r2-update-1
    aliases:
    - /resources/KyDt7UnJIhX
    aliasesArchive:
    - /blog/upgrade-server-windows-server-2012-r2-update-1
    - /upgrade-server-windows-server-2012-r2-update-1
    - /upgrade-your-server-to-windows-server-2012-r2-update-1
    - /blog/upgrade-your-server-to-windows-server-2012-r2-update-1
    - /resources/blog/upgrade-server-windows-server-2012-r2-update-1
    tags:
    - Windows
    - Install and Configuration
    - System Configuration
    categories:
    - Uncategorized
    preview: nakedalm-windows-logo-6-6.png
  BodyContent: |
    With the release of Windows Server 2012 R2 Update 2 I wanted to make sure that all of my demo machines are up to date. I have a Domain Controller and a TFS server that are separate boxes but which are both running Windows Server 2012 R2.

    The new update became available yesterday for MSDN subscribers and will be generally available next Tuesday (8th April 2014). I have already completed these updates on my Surface 2 Pro and Surface 2, both of which were running Windows 8.1. Today I want to concentrate on getting all of my demo boxes up to snuff as I have some demos & presentations next week.

    Updating these boxes should be trivial, and you know that I like to make sure that I have documentation so here we go. If you download the update from MSDN you get a zip archive called "mu_windows_8.1_windows_server_2012r2_windows_embedded_8.1industry_update_x64_4046913" that contains 6 Updates that can be used on Windows Server as well as Windows. There is a separate update for Windows ARM based architectures. It is recommended to install them in the following order:

    1. KB2919442
    2. KB2919355
    3. KB2932046
    4. KB2937592
    5. KB2938439
    6. KB2949621

    Although KB2919355 contains the real meat at over 700mb and with most of the other updates are described as 'feature updates' you may or may not need to install KB2919442. KB2919355 can take a while to install and on my Surface 2 Pro it took around 30 minutes. On my Domain Controller only around 20 minutes.

    ![clip_image001](images/clip_image001-1-1.png "clip_image001")  
    { .post-img }
    Figure: More than one update needs to be applied

    Although all of my Windows 8.1 systems already had KB2919442 installed I did need it run it on all of my Windows Servers. You should run each update in the order listed above.

    ![clip_image002](images/clip_image002-2-2.png "clip_image002")  
    { .post-img }
    Figure: Each update can take some time to complete

    Each update takes various lengths of time to complete and I did notice that the times were pretty consistent with the physical size of the update. It’s a little annoying, and time consuming, to have to run each update manually and there really should be an installer for this. However it may not be worth It for the team to take the time when there updates will be going out to general release over Windows Update next Tuesday.

    ![clip_image003](images/clip_image003-3-3.png "clip_image003")  
    { .post-img }
    Figure: If asked to restart then ignore

    All of the updates (except KB2919442) requested a restart at the end of each install. I ignored this and relied on the update to decide if the pending restart required to be actioned before proceeding. All of the updates installed in one go with no required reboots in between. That has been true on all of my systems, Windows 8.1 (x64), Windows 8.1 (ARM), and Windows Server 2012 R2.

    ![image](images/image-4-4.png "image")  
    { .post-img }
    Figure: Final reboot can take some time

    After you have completed all of the updates restart your server and wait for a while… it may reboot a couple of times, however eventually it will be back. Took my VM’s around 3-5 minutes to reboot and configure the updates…

    ![image](images/image1-5-5.png "image")  
    { .post-img }
    Figure: Windows Server 2012 R2 Update 1

    Not only did all of the updates install with no issues, all of the machines came back up and all services (those I changes anyway) are functional. Now that I have my Domain Controller and Team Foundation Server upgraded to Windows Server 2012 R2 Update 1 I can continue to updating TFS to Visual Studio Team Foundation Server 2013 Update 2 which was also released yesterday.
  FilePath: C:\Users\MartinHinshelwoodNKD\source\repos\NKDAgility.com\site\content\resources\blog\2014\2014-04-03-upgrade-server-windows-server-2012-r2-update-1\index.md
  FolderPath: C:\Users\MartinHinshelwoodNKD\source\repos\NKDAgility.com\site\content\resources\blog\2014\2014-04-03-upgrade-server-windows-server-2012-r2-update-1
- FrontMatter:
    title: Should I upgrade to TFS 2013 Update 2?
    description: Discover the benefits of upgrading to TFS 2013 Update 2. Explore new features in backlog, test, and release management to enhance your development process!
    ResourceId: _FVuS9sZSA2
    ResourceType: blog
    ResourceContentOrigin: Human
    ResourceImport: true
    ResourceImportId: 10479
    ResourceImportSource: Wordpress
    ResourceImportOriginalSource: Wordpress
    date: 2014-04-03
    weight: 790
    creator: Martin Hinshelwood
    layout: blog
    resourceTypes: blog
    slug: upgrade-tfs-2013-update-2
    aliases:
    - /resources/_FVuS9sZSA2
    aliasesArchive:
    - /blog/upgrade-tfs-2013-update-2
    - /upgrade-tfs-2013-update-2
    - /should-i-upgrade-to-tfs-2013-update-2-
    - /blog/should-i-upgrade-to-tfs-2013-update-2-
    - /resources/blog/upgrade-tfs-2013-update-2
    tags:
    - Software Development
    - Windows
    - Azure DevOps
    - Release Management
    - Install and Configuration
    categories:
    - Uncategorized
    preview: nakedalm-experts-visual-studio-alm-7-7.png
  BodyContent: |
    With the launch at Build of Team Foundation Server 2013 Update 2 I wanted to make sure that the update goes seamlessly. So I am upgrading my demo box to both [Windows Server 2012 R2 Update 1](http://nkdagility.com/upgrade-server-windows-server-2012-r2-update-1/) and Team Foundation Server 2013 Update 2.

    Recently the TFS updates have become so run of the mill that it has not been worth talking about updates. I don’t mean that the features added as part of the update are simple, it is that the install and upgrade process is so straightforward that it is a non-event. I have customers that are on some of the craziest domain and network configurations that I have ever seen and still TFS is the one system that cuts through the complexity and delivers a flawless experience. But why would I take even this non-existent risk… one word… features:

    ## Backlog Management features in TFS 2013 Update 2

    In addition to general performance and navigation improvements the TFS team has been hard at work adding value to our backlog management.

    - **REST API for Tags** - I don't know how many times recently I have done either Integration Platform or even simple Excel migration of work items and hear the fateful words "but where did my tags go?". Now with Team Foundation Server 2013 Update 2 the TFS team have finally finished off tagging with an API that allows editing and management of Tags. You can now edit tags in Excel as well as through the API.
    - **Query on Tags** - With the addition of the API is now possible to create queries that target the tags.
    - **Tagging in Visual Studio** - You can apply tags to work items in Visual Studio.
    - **Tagging Permissions** - You can apply permissions for who can add new tags
    - **Non-working days** - You can configure non-working days, and these are excluded from burndown charts
    - **Cumulative Flow Start Date** - Cumulative Flow Diagram start dates are configurable
    - **Pinning Charts** - Lightweight charts can be pinned to project or team homepages

    These changes while small are things that my current customers have been clamouring for. Unfortunately they are stuck on IE8 and thus TFS 2012 is the most recent version that they can use.

    ## Test Management features in TFS 2013 Update 2

    While I go head to head with the Test team often (sorry Ravi), I only do so because I love the tools and I want them to be better. Don't tell them that they make the competition look like a bunch of bumbling buffoons who could not test their way out of a paper bag.

    - **Test Export** - This update provides to testers and test leads the ability to export test artefacts so that these can be sent by using email or as printouts and shared with stakeholders who do not have access to TFS.
    - **Shared Parameters** - This update provides to testers and test leads the ability to manage test parameter data at one place by using Shared Parameters. Any subsequent changes to parameter data can be updated at one place and all the test cases referencing the Shared Parameter are automatically updated.

    Where there are gaps in the tests tools Microsoft has many partners to step in and take up the slack by building against a world class API.

    ## Release Management features in TFS 2013 Update 2

    Release management, while new to TFS, is an awesome tool and these additions go a long way to making it enterprise ready. I have been using it at a number of organisation and the clone deployment are probably the most powerful features.

    - **Release Tags** - The tags are designed to perform the same operation across the servers. If there are server specific actions, the user can always add the specific server and the corresponding actions at that level in the deployment sequence.
    - **Shared Variables** - To configure a group of server by using same tag implies that you can set values for the whole group and all the servers in the group therefore share common values for all variables.
    - **Cluster Support** - You can easily deploy to identical or clustered servers without having to repeat the deployment sequence on each server.
    - **Clone deployment sequence** - You can Copy Tags across stages and across Templates. You can retain the same deployment sequence with all the tags and servers when copied to other stages or Release templates under the same environment.

    I am looking forward to vNext and beyond in this space.

    ## Version Management features in TFS 2013 Update 2

    As predicted most of the enhancements are within the Git space. While TFVC will be around for a very long time to come I firmly believe that Git is the way forward for version control. If you are not thinking of moving to Git then I can assure you that your developers are. If you need any form of audit, common to any enterprise, then TFS is the ONLY implementation of Git that will support you.

    - **General Git additions** - Git tools have been updated to include an annotate (blame) view, reverting a commit, amending a commit, pushing to multiple remotes, and improved progress and cancellation ability for long running operations.

    ## Updating to TFS 2013 Update 2

    As I said before the TFS install & upgrades have become so trivial that it is almost not worth mentioning… almost…

    ![image](images/image2-1-1.png "image")  
    { .post-img }
    Figure: Upgrading to TFS 2013 Update 2

    Upgrading TFS is simpley a case of installing the latest version. I would always recommend that you either take the opportunity to move to new hardware as this  gives you the ultimate rollback. If you have a single server instance that is virtual you should at least ‘Snapshot’ the server. Whatever your quick rollback solution make sure, and I mean really sure, that you have a current and up to date backup. If you are using the TFS backup tool that is built into the new

    ![image](images/image3-2-2.png "image")  
    { .post-img }
    Figure: Laying down the new files

    The Installer will automatically remove the old version of TFS and update you to the latest bits. If you have TFS 2005 you will need to go to TFS 2010 first (you can get that installer from MSDN) but if you have 2008, 2010, or 2012 you can just run the 2013 Update 2 installer to upgrade.

    ![image](images/image4-3-3.png "image")  
    { .post-img }
    Figure: Make really sure that you have a backup before upgrading to TFS 2013 Update 2

    Once the foundation has been laid you will be presented with screen asking you which database (configuration database) that you want to upgrade. You may have more than one TFS instance using the same SQL Server. To be honest I would not recomnned it, but I have seen it. You will have to check the box to say that you really do have a backup, and if you do not then go now and create a SQL backup of all of the TFS databases.

    ![image](images/image5-4-4.png "image")  
    { .post-img }
    Figure: Review your upgrade to TFS 2013 Update 2

    If you are upgrading you will probably not need to change any of the settings. In just hit ‘next’ until it asked me to re-enter the reporting account password an then reviewed the settings. If you are installing from scratch then you should hit the documenation, or a previous post for a full install guide.

    - [Installing TFS 2013 from scratch is easy](http://nkdagility.com/installing-tfs-2013-scratch-easy/ "http://nkdagility.com/installing-tfs-2013-scratch-easy/")

    After this you validate and then execute the configuration. After a while, say 5-10 minutes, you will see a bunch of green checkmarks. If you are upgrading from TFS 2008 or TFS 2010 you may have a much longer wait. Depending on the size of your database and the speed of the servers you upgrade could take minutes or hours.

    ![image](images/image6-5-5.png "image")  
    { .post-img }
    Figure: All green ticks when upgrading to TFS 2013 Update 2

    Not only did I get green ticks on the configuration, but you will get ticks for each of your collections. Even after the configuration has completed your collection updates will kick off in the background. My current customer has more than 10 collections and these will be upgraded individually. Once done you collections will be online and you can connect…

    ![image](images/image7-6-6.png "image")  
    { .post-img }
    Figure: Only 13 minutes to upgrade to TFS 2013 Update 2

    In only 13 minutes I have upgrade my demo environment from TFS 2013 RTM to TFS 2013 Update 2. This was a small environment however the process is the same regardless.

    ## Conclusion

    While a full list of features in both Visual Studio and Team Foundation Server for Update 2 can be found on [http://support.microsoft.com/kb/2927432](http://support.microsoft.com/kb/2927432) the ones above are those that I think are most important and relevant for ALM. I am really looking forward to some of the features that I know are coming in the next version of TFS and these are just wetting our appetites and rounding out some of the itches that we have right now. At this time TFS 2013 Update 2 has RTM’ed however Visual Studio and some of the other update components are still RC. That said, they are coming with a go-live licence.

    **Released to Manufacture (RTM):**

    - TFS 2013 Update 2 RTM: [http://go.microsoft.com/fwlink/?LinkId=392762](http://go.microsoft.com/fwlink/?LinkId=392762)
    - TFS Express Update 2 RTM: [http://go.microsoft.com/fwlink/?LinkId=392763](http://go.microsoft.com/fwlink/?LinkId=392763)

    **Go-Live Release Candidate:**

    - VS 2013 Update 2 RC: [http://go.microsoft.com/fwlink/?LinkId=390521](http://go.microsoft.com/fwlink/?LinkId=390521)
    - Express for Windows Update 2 RC: [http://go.microsoft.com/fwlink/?LinkID=386598](http://go.microsoft.com/fwlink/?LinkID=386598)
    - Ultimate Update 2 RC: [http://go.microsoft.com/fwlink/?LinkId=331030](http://go.microsoft.com/fwlink/?LinkId=331030)
    - Professional Update 2 RC: [http://go.microsoft.com/fwlink/?LinkId=331031](http://go.microsoft.com/fwlink/?LinkId=331031)
    - Premium Update 2 RC: [http://go.microsoft.com/fwlink/?LinkId=331032](http://go.microsoft.com/fwlink/?LinkId=331032)
    - Agents Update 2 RC: [http://go.microsoft.com/fwlink/?LinkId=393087](http://go.microsoft.com/fwlink/?LinkId=393087)
    - Release Management Update 2 RC: [http://go.microsoft.com/fwlink/?LinkId=393085](http://go.microsoft.com/fwlink/?LinkId=393085)
    - Remote Tools Update 2 RC: [http://go.microsoft.com/fwlink/?LinkId=393086](http://go.microsoft.com/fwlink/?LinkId=393086)

    For those that do not know what Go-Live is it means that the TFS team are happy to fully support its use in production but they don’t guarantee beyond RC quality. This is usually when these components have not seen a large enough install base for them to be as sue as they can be that there are no issues. Don;t be afraid of Go-Live… embrace it and go for it.

    You should go ahead and install TSF 2013 Update 2 today. If you have any issues or you want to know more about how to make the most of the new and existing features then give us a shout.
  FilePath: C:\Users\MartinHinshelwoodNKD\source\repos\NKDAgility.com\site\content\resources\blog\2014\2014-04-03-upgrade-tfs-2013-update-2\index.md
  FolderPath: C:\Users\MartinHinshelwoodNKD\source\repos\NKDAgility.com\site\content\resources\blog\2014\2014-04-03-upgrade-tfs-2013-update-2
- FrontMatter:
    title: What my father taught me about Evidence-based Management (34 years before it was invented!)
    description: Discover how my father's insights on Evidence-based Management transformed a failing business 34 years before its inception. Learn to optimize value and agility!
    ResourceId: gYcZPh6B2eg
    ResourceType: blog
    ResourceContentOrigin: Human
    ResourceImport: true
    ResourceImportId: 10446
    ResourceImportSource: Wordpress
    ResourceImportOriginalSource: Wordpress
    date: 2014-03-20
    weight: 440
    creator: Martin Hinshelwood
    layout: blog
    resourceTypes: blog
    slug: what-my-father-taught-me-about-agility-path-34-years-before-it-was-invented
    aliases:
    - /resources/gYcZPh6B2eg
    - /resources/blog/what-my-father-taught-me-about-evidence-based-management-34-years-before-it-was-invented
    aliasesArchive:
    - /blog/what-my-father-taught-me-about-agility-path-34-years-before-it-was-invented
    - /what-my-father-taught-me-about-agility-path-34-years-before-it-was-invented
    - /what-my-father-taught-me-about-evidence-based-management-(34-years-before-it-was-invented-)
    - /blog/what-my-father-taught-me-about-evidence-based-management-(34-years-before-it-was-invented-)
    - /resources/blog/what-my-father-taught-me-about-agility-path-34-years-before-it-was-invented
    - /resources/blog/what-my-father-taught-me-about-evidence-based-management-34-years-before-it-was-invented
    tags:
    - Decision Making
    - Evidence Based Leadership
    - Evidence Based Management
    - Pragmatic Thinking
    - Metrics and Learning
    - Value Delivery
    - Ability to Innovate
    - Continuous Improvement
    - Empirical Process Control
    - Organisational Agility
    - Software Development
    - Operational Practices
    categories:
    - Product Management
    preview: nakedalm-agility-index-7-7.png
  BodyContent: |
    A few weeks ago I headed out to the Scrum.org offices in Boston to participate in training to hone my skills as an Evidence-based Management Consultant. I was talking to my father about it when I got back and was surprised that he recognised many of the practices and tools.

    ![john-hinshelwood-agility-path](images/john-hinshelwood-agility-path-4-4.jpg)In the 1990's my father (John Hinshelwood) was a regional director of a large estate agency (realtor) in the UK. For many years he worked from a single shop in Glasgow running his own business. In 1988 he sold his business to a large Insurance company that was trying to get its ailing Estate Agency business to turn a profit. It was failing at this task, however it was not long before my father's talent for detail was noticed and within a few short months was appointed a directorship. Using evidence-based practices he turned a lossmaking business into a profiting business for over 5 years. Three months after he left they were making a loss again! They had not changed to embrace Evidence-based Management as an organisation and without my father's stewardship they slumped back into the old ways.
    { .post-img }

    A few weeks ago I headed out to the Scrum.org offices in Boston to participate in training to become an Evidence-based Management Consultant with Scrum.org. Evidence-based Management is a new approach to looking at the value you are delivering with your organisation that is based on evidence rather than the traditional 'gut feelings' of managers. It  allows you to really look at what you need to improve to make the most of your investments in agility.

    Organisations spend a lot of time trying to understand the value that they are getting in their pursuit to agility and often spend time pouring money into areas that are seeing minimal improvements. There may be other areas where they could get much more value for money but they either don't know or don’t understand where and what they are. These large organisations struggle to move towards agility no matter how much they might want it. Some of this is a people issue but often it is the reality of scale that gets in the way.

    There is lots of help for these organisation but how can they be sure not only that they are on the right path but that the help that they are getting is providing the value that is advertised. For this we need to do something and then look at the result to determine the value. This is what Evidence based Management is all about.

    Evidence-based Management (EBM or EBMgt)'s roots are in Evidence-based Medicine and evidence based policy. The idea is to bring scientific methods to the front of the way we do things and EBM's goal is to bring it front and centre in management of organisations.

    > Evidence-based management entails managerial decisions and organizational practices informed by the best available scientific evidence.[http://en.wikipedia.org/wiki/Evidence-based_management](http://en.wikipedia.org/wiki/Evidence-based_management)

    In order to even begin to understand where we need to go we need to understand where we are right now. If management can effectively understand where we currently are then you can much more easily prevent backsliding to the old ways once you improve. This is in essence why scientific and proven practices should replace the more traditional 'gut feeling' used prolifically in management.

    Evidence-based Management and how it applies to general organisational improvements as well as software development organisations is described well in “The Leaders Guide to Radical Management by Steven Denning”.

    We, as the software industry, have been focusing in the wrong place. The revelation from The Standish Groups research that success, as measured by traditional PBI metrics, does not equate to value to the customer, is damming. Couple that with Forester's salient advice to focus on measuring the value delivered to the customer, it is obvious that things need to change. Steven Denning looked at many successful organisations across the world, even companies that were once successful, to see if there were any commonalities between them. He found that when companies focus on the products or services that they deliver they are rarely successful. In instances when they are successful, that success tends to be very short lived, no more than 5-10 years. You only have to look at the rise and decline of many companies in the FUTSI 500 to see the impact of focusing on products and services only.

    So what is the secret? The secret is to focus on your customers. Focus on what you can do to delight them. And then identify metrics of leading or lagging indicators of customer delight. If you focus on measuring and optimising these measure of customer delight then you will create the most relevant products and services to make your company a success. And if you continue to focus on your customer, you will sustain that success for many more years that your competitors that don’t.

    While the leading metrics and evidence of high quality value delivery for an Estate Agency differ from a Software Company, there are similarity's. In fact I was once asked by a customer what the industry standard metrics were in the software industry. I had no answer at the time, and in fact failed to come up with my own that would provide the desired leading measures. But even with the proliferation of measurement systems popping up everywhere, there are few that follow the evidence-based approaches described in “The Leaders Guide to Radical Management by Steven Denning”. One such emerging dataset is the Agility Index from Scrum.org. Scrum.org have identified three key areas that we need to focus on.

    ![clip_image001](images/naked-alm-current-value-ability-to-inovate-time-to-market-5-5.png "clip_image001")
    { .post-img }

    Figure: Agility Index from Scrum.org for leading evidence for the impact of change

    We need focus on value (Business Outcomes) to make sure that we are producing something that our customers value while not alienating our employees or spending too much money doing it.

    We need to focus on lead time (Time to Market) to make sure that we can take those things of value that we are producing and quickly get them, in short order, into the hands of our customers.

    We focus on quality (Ability to Innovate) to make sure that what we do deliver works, is easy to use, has useful features, and does not have a high maintenance cycle.

    These three groups of metrics and merged into a single agility index using a weighted calculation to gives us a consistent idea of where we are now. We can then look at the trends of these figures over time and make sure that we are heading in the desired direction. This figure gives your organisation a value for return-on-investment in your process improvement efforts.

    ![clip_image002](images/clip_image002-2-2.png "clip_image002") Figure: Agility Index from Scrum.org shows Measure, Diagnose, Improve in action
    { .post-img }

    The goal now that we have our direct evidence is to diagnose, as a doctor would, what changes are most likely to bring the most value to our organisation. To do that we need some extra circumstantial evidence that we can use to diagnose which improvements will provide your organisation with the most value. There are two tools in our arsenal to help us gather that circumstantial evidence:

    - **Practice Backlog** - a indicator we can use if the maturity of the practices, tools, architectures, and standards that your organisation is using. To help measure this Scrum.org has created a practices backlog of things that we know, through long experience with software organisations, helps them become more effective at delivering value. This practices backlog contains more than 300 practices that are ordered as there are many dysfunctions that can arise from skipping over certain practices. This gives us an understanding of your organisation's practice maturity.
    - **Assess People** - another good piece of circumstantial evidence is the skills, knowledge, and understanding of the people that are involved in the delivery of the value. This can be assessed individually and Scrum.org has created assessments that it already uses to evaluate knowledge after training. Scrum.org has over 3 years of experience in measuring individuals knowledge of agility and what it takes to be agile. We can leverage that knowledge and use their existing assessments for Developers, Scrum Masters, and Product Owners to gain visibility of their maturity of knowledge.

    [Find out more on how Agility Index can help you business](http://nkdagility.com)

    Using the circumstantial evidence or organisational maturity from the practice backlog and the assessments we can better diagnose where to focus investment. With the leading direct evidence we can understand the impact of our changes. These together from the backbone of Scrum.org's implementation of Evidence-based Management for Software Organisations.

    ![Evidence-based Management for Software Organisations](images/naked-alm-evidence-based-management-for-software-organisations-6-6.png "clip_image003")
    { .post-img }

    > Evidence-based Management for Software Organisations: A framework within which people can address complex organizational change, while productively and creatively improving the enterprise and its products.
    >
    > You can find out more about Evidence-based Management for Software Organisations on [http://ebmgt.org](http://ebmgt.org)

    Your organisation probably has metrics that you use now. However ask yourself if these metrics can tell you how much value that their projects delivered last year. If not the should focus on value… and agility path includes a bunch of tools that are optimised to help you to understand and optimise that value.
  FilePath: C:\Users\MartinHinshelwoodNKD\source\repos\NKDAgility.com\site\content\resources\blog\2014\2014-03-20-what-my-father-taught-me-about-agility-path-34-years-before-it-was-invented\index.md
  FolderPath: C:\Users\MartinHinshelwoodNKD\source\repos\NKDAgility.com\site\content\resources\blog\2014\2014-03-20-what-my-father-taught-me-about-agility-path-34-years-before-it-was-invented
- FrontMatter:
    title: TFS for cross team and cross business line work item tracking
    description: Discover effective strategies for cross-team collaboration in TFS, enhancing work item tracking and project management for software delivery across divisions.
    ResourceId: l6LGPY2BGU5
    ResourceType: blog
    ResourceContentOrigin: Human
    ResourceImport: true
    ResourceImportId: 10378
    ResourceImportSource: Wordpress
    ResourceImportOriginalSource: Wordpress
    date: 2014-03-04
    weight: 440
    creator: Martin Hinshelwood
    layout: blog
    resourceTypes: blog
    slug: tfs-cross-team-cross-business-line-work-item-tracking
    aliases:
    - /resources/l6LGPY2BGU5
    aliasesArchive:
    - /blog/tfs-cross-team-cross-business-line-work-item-tracking
    - /tfs-cross-team-cross-business-line-work-item-tracking
    - /tfs-for-cross-team-and-cross-business-line-work-item-tracking
    - /blog/tfs-for-cross-team-and-cross-business-line-work-item-tracking
    - /resources/blog/tfs-cross-team-cross-business-line-work-item-tracking
    tags:
    - Pragmatic Thinking
    - Azure DevOps
    - Software Development
    - Operational Practices
    - Product Delivery
    categories:
    - Uncategorized
    preview: nakedalm-experts-visual-studio-alm-4-4.png
  BodyContent: |
    I was asked by current customer to come up with a solution, within TFS, to allow an entire division to work together in delivering software for a bank. This divisions made up of over 10 teams than work on many pieces of software. Some have simple requirements while others require harsh security and compliance. This is a standard problem and not unique to this company, however the perception still prevails with both TFS users and administrators, that one must have a single Team Project for each \[Project | Team | Product\] under way. This perception is not only incorrect but Team Foundation Server was designed to be used differently. The Developer Division (DevDiv) at Microsoft, who built the product, uses a single 20+ terabyte Team Project for their Work Items, Source Code and Builds for over 2k people. Team Foundation Server was designed and built to be used with fewer large Team Projects rather than many small Team Projects.

    The group I am working with has many Team Projects, in many cases one for each application. The teams working against these Team Project generally own more than one application and they are running into a number of issues:

    - **Moving work between \[Project | Team | Product**\] - Sometimes you find a bug in one application when testing another. Sometimes a work item is just created in the wrong place, or a Feature needs to be broken down into many Product Backlog Items that are for more than one \[Project | Team | Product\]. In order to move an item between team projects you must 'copy' the existing item to the other location and progress the original to the closed/removed state. Indeed, as not all fields and data are copied a link must be maintained between the new item in the new team project and the old item in another so that additional data, perhaps history, can be retrieved.
    - **Query across \[Project | Team | Product\]** - While you can query across team project none of the tools that are built on top of TFS support this. Specifically a Team's backlog can only contain work from the current Team Project and if you load a cross team project query in Excel it will only display data from the connected Team Project. This created confusion.
    - **Team Focus** - The Team entity within TFS exists only within a single team project and none of its features can cross the team project boundary. With features for Project Planning and Sprint Planning being invaluable for both Product Owners and Teams this can be decidedly crippling.
    - **Individual focus** - As all of the tools built on top of Team Foundation Server allow an individual to be connected to only one Team Project at a time there is a large amount of context switching for a member of multiple Team Projects. They have to switch to see what work they have and they will have multiple priorities for that work. It is hard for an individual to focus on a single backlog as their work may be spread across many.

    These issues are only those that have been presented by the teams using TFS and they have come up with a number of requirements to help facilitate the building of a solution suitable for their business line and potentially the others within the customer:

    - Must be able to assign work to anyone within a division
    - Must be able to reassign that item to anyone else within that division
    - Simple Web UI for entering 'defects'
    - Project Manager should be able to add/remove people from the correct projects
    - Sharing of Test Cases between Applications

    Note: There are a few asks for the ability to assign work to anyone within the organisation, but short of creating a single Team Project for the entire organisation (tens of offices from Singapore to London to New York.) This is just not feasible from a support perspective, especially backup/restore.

    There are two separate but related things that should be implemented to both mitigate the issues above and to support the requirements describes. These are to move everyone within the division to a single Team Project and to implement Team Field within that team project.

    A general rule of thumb: If there are shared resources (with resources defined as People, Work Items, or Source Code) then one should be in a single Team Project.

    ### Single Team Project

    The first solution is to create a larger Team Project that contains many Application as well as Teams. In Team Foundation Server 2012 we can create multiple 'Teams' within a single Team Project to compartmentalize the work. As the 'Team' entity is built upon the security principles we can also use this to secure our application code and work items to one or more 'Teams'.

    ![image](images/image-2-2.png "image")  
    { .post-img }
    Figure: Using area path to represent products in Team Foundation Server

    As there is a single Version Control repository in TFS for all Team Projects there is little change to the existing multiple Team Project functionality and we can use Area Paths mirrored with Source Control folders to identify Applications within the system.

    As Area Path is a Dimension within the data Warehouse and cube we can using it to slice our data and report by application. As it is a tree we can also select which data from the tree to include and what to exclude. This is available as both published reports in Reporting Services and ad-hoc reports in Excel.

    ### Team Field

    By default Team Foundation Server provides two dimensions for categorising work and representing it on backlogs. Iteration Path, which is for the teams cadence, and Area Path that is for categorizing work. For this division we need an additional dimension and this can be provided with Team Field.

    ![clip_image002](images/clip_image0021-1-1.png "clip_image002")  
    { .post-img }
    Figure: Using team field as a third dimension in Team Foundation Server

    Team Field adds the ability to have a separate list for team and frees up Area Path, used for Team by default, for a much-needed Product breakdown as described above.

    ![image](images/image1-3-3.png "image")  
    { .post-img }
    Figure: Using team field to represent teams in Team Foundation Server

    With Team Field in use to designate which 'backlog' items appear on we can create both a single team that owns many Applications and have multiple teams that own a single application. In addition if we want to create a roll up to a Product Owner that perhaps has many teams that work on one or more applications we can also represent that.

    ![SNAGHTML6934f7d](images/SNAGHTML6934f7d-5-5.png "SNAGHTML6934f7d")  
    { .post-img }
    Figure: Using team field to create virtual team groupings

    This can be used to create many levels however it does become a management nightmare the more levels that are added.

    ### Conclusion

    All of the requirements of the customer will fulfilled with these solutions along with the use of reporting. There has been some concern about viewing data across Team Project and indeed across Collection if there is one collection per business line (we currently have 12 collections). However these fears are unfounded as all of the data from all of the collections ends up in a single data warehouse and cube. This will allow us to report across all of the business lines with ease.

    Things are not all good and there are a few caveats to this approach:

    - **More manual support and security management** - There will be some manual creation of Teams, Folders and permissions. This actually gives us more flexibility in their creation and does benefit the teams but could easily over whelm a support team that is not fluent in PowerShell. Currently we are creating everything manually, however once we have the process solidified PowerShell scripts can be created to automate the process.
    - **Build list can get large** - When you get many hundreds of build definitions the list of them can get very unwieldy within Team Explorer and Team Web Access. However with the use of permissions to hide other teams build definitions as well as judicious use of team favourites this can be easily mitigated.
    - **Test Management lists all Test Plans** - Instead of plans being filtered by team on the web (which is not supported anyway in MTM) all are listed. This can make the list long and with use of keywords and tagging we can make it easier to find and connect to the Test Plans for each Team. For example we may begin the name of each plan with the name of the Team to which it pertains. While this is more of a pain than it needs to be for now I believe that TFS v.Next will resolve this issue.

    These issues however are small in comparison to the benefits that are gained by the single Team Project and Team Field approaches detailed above.

    How have you solved your organisational requirements in Team Foundation Server?
  FilePath: C:\Users\MartinHinshelwoodNKD\source\repos\NKDAgility.com\site\content\resources\blog\2014\2014-03-04-tfs-cross-team-cross-business-line-work-item-tracking\index.md
  FolderPath: C:\Users\MartinHinshelwoodNKD\source\repos\NKDAgility.com\site\content\resources\blog\2014\2014-03-04-tfs-cross-team-cross-business-line-work-item-tracking
- FrontMatter:
    title: Metrics that matter with evidence-based management
    description: Discover how Evidence-Based Management can enhance your metrics and KPIs for better decision-making in software development. Elevate your team's performance!
    ResourceId: ezNZJBiEwYA
    ResourceType: blog
    ResourceContentOrigin: Human
    ResourceImport: true
    ResourceImportId: 10367
    ResourceImportSource: Wordpress
    ResourceImportOriginalSource: Wordpress
    date: 2014-02-25
    weight: 315
    creator: Martin Hinshelwood
    layout: blog
    resourceTypes: blog
    slug: metrics-that-matter-with-evidence-based-management
    aliases:
    - /resources/ezNZJBiEwYA
    aliasesArchive:
    - /blog/metrics-that-matter-with-evidence-based-management
    - /metrics-that-matter-with-evidence-based-management
    - /resources/blog/metrics-that-matter-with-evidence-based-management
    tags:
    - Decision Making
    - Evidence Based Management
    - Evidence Based Leadership
    - Metrics and Learning
    - Ability to Innovate
    - Trend Analysis
    - Value Delivery
    - Continuous Improvement
    - Pragmatic Thinking
    - Operational Practices
    - Software Development
    - Empirical Process Control
    - Product Delivery
    - Working Software
    - Organisational Agility
    categories:
    - Engineering Excellence
    - Product Management
    preview: nakedalm-agility-index-24-24.png
  BodyContent: |
    I was recently asked to speak at ALM Days in Dusseldorf and more specifically to create a talk on Metrics and KPI’s for Quality. As I have been working a lot recently with evidence-based management. I am pretty sure that my session title translates as “Test management and reporting – KIP’s for better decisions” so I am going to concentrate on reporting and KPI's as the session before mine is on Agile Testing.

    - slideshare [Metrics that matter with evidence-based management @ ALM Days in Dusseldorf](http://nkdalm.net/1eVh1UP)
    - slideshare [The pursuit of Agility @ Agile into Finance in London](http://nkdalm.net/1q4ZM7R)

    ![image2](images/image21-3-3.png "image2") Figure: Testmanagement und Recording - KPIs für schnellere Entscheidungen
    { .post-img }

    There are really  two levels of metrics that I want to talk about, the first being at the team level and the second being the organisation level. If we are making decisions on the metrics that are being presented then we had better have the right metrics. I have recently been learning a lot about Evidence based Management and I think that it holds somewhat of the answer to which metrics we should be looking at and that we can consistently trend over time.

    Evidence-based Management (EBM) comes out of Evidence Based Practices that have been used extensively in medical fields. Where we have life and death being the result of our decisions we want to make very sure that our metrics are being accurate. What I think that we forget is that while not strictly life threatening the executive leadership of an organisation is making critical decisions based on the information that they get from their subordinates. If these metrics are not the right ones or totally the wrong ones then you could find a company plunged into a critical state. So what is EBM all about?

    In order to be using EBM we need to be:

    - Evaluate the current outcomes in the organisation
    - Carefully analyse the most likely contributors to these outcomes
    - Implement improvements in short cycles
    - Evaluate outcomes based on solid evidence.

    If we were to apply these principals to organisations that deliver software what might these metrics look like?

    ![image8](images/image81-19-19.png "image8") Figure: Overview of Organisational Metrics
    { .post-img }

    Out of these metrics there are really only three that I can hope to calculate at the team level and the rest are organisational or product based. The three for the team are Cycle Time, Innovation Rate and Defects which are all data points that I can get from Team Foundation Server. There is also a whole host of circumstantial evidence that we can get from TFS.

    ## Data, Data, Everywhere

    Circumstantial evidence is data that we use to support our analysis and with Team Foundation Server we get a plethora of options. We still need to make sure that we are not looking at the wrong metrics and at least TFS metrics are much more difficult to game as they come from an auditable system. However…

    ![image17](images/image171-1-1.png "image17")
    { .post-img }

    …always check the data with the source.

    ![image20](images/image201-2-2.png "image20") Figure: Data relationships in Visual Studio ALM
    { .post-img }

    There tends to be so much data in TFS that it can be difficult to sort out the information that can be useful from a project managers wet dream of exceptionally bad metrics. I bet every developer has seen metric used for the wrong results. Have you ever been measured on the number of lines of code that you have written? Or a tester by the number of bugs found? The result is people gaming the system.

    ![WP_20140225_11_42_35_Pro](images/WP_20140225_11_42_35_Pro-25-25.jpg "WP_20140225_11_42_35_Pro") Figure: Reports in Visual Studio ALM
    { .post-img }

    There is so much data available that it can be difficult to know where to start and indeed this is the reason that there are only a few reports out of the box with TFS. Not only can it be dangerous to provide to many reports that “generically” apply, you would invariable be wrong and have angry teams at your door with pitchforks and torches. And above all we need to avoid vanity metrics…

    I worked at Meryl Lynch back in the day as a developer we had to provide some data in a spread sheet up the chain. It amazed me that my box, the Lead Developer for my Team, would massage the statistics a little to make them slightly more favourable. Turning the KPI’s from just a little to many red to a little more green and orange. Not only that but his boss would take those massaged figures, merge it with all his lead developers, and then proceed to massage them himself… Can you imagine the total fiction that the person at the top of the chain gets? And what decisions are they making based on that fiction?

    ![image26](images/image26-4-4.png "image26") Figure: Project Estimate Overview from the Project Overview Report
    { .post-img }

    At one of my customers I was presented with the “Project Overview Report” that consisted of a ring bound book of data that was viewed all the way at the 3C’s level.  The keystone report was the “Project Estimate Overview” report with which the lead BA and owner of the report was very proud. The report showed the original estimate versus the actual time taken to deliver a project. I looked at the report for all of 30 seconds before I turned to him and said:

    > “Two of these projects have original estimates that are within 20%, one is within 10% and the other two are exact or almost exact matches. I call bull shit.” -MrHinsh on being presented with the report above

    The answer I got was interesting to say the least. He said:

    > “Well Martin, we were so far off with our estimates that we had to come up with a solution. We created a system that allowed the a project manager to submit a change to the original estimate. Its all recorded and above board.” -Customer

    Wait what? Where in the graph above is that reflected… well of course it is not. The whole purpose is to legalize a fake report and cover their asses should someone challenge them on it, which no one did until an external consultant came along. I just hope that no one made any decisions at the executive level that resulted in the loss of jobs based on this fiction.

    SO what data can we look at?

    ![WP_20140225_11_43_33_Pro](images/WP_20140225_11_43_33_Pro1-26-26.jpg "WP_20140225_11_43_33_Pro") Figure: Stories Overview report
    { .post-img }

    One of the key reports in TFS for quality indicators is the overview report. The overview report comes in different flavours depending on the process template that you choose. You get the Requirements Overview, Backlog Overview, and the Stories overview pictured above. Whatever the format you get to see the maturity of each of the pieces of work that your team currently has underway. You can see the number of Tests that are passing and failing as well as he outstanding bugs.

    You should generally see more completeness at the top as these are displayed in backlog priority. I would also expect to see more passing tests the more ‘complete’ the feature. If you see something that is marked as very complete but has lots of outstanding bugs, few successful test results, or no test then you can be sure that the quality will not be high.

    ![image32](images/image32-6-6.png "image32") Figure: Build Quality Indicators
    { .post-img }

    The automated build that you all obviously have on your software is another point of inspect and adaption for your code. You cant look at all of your codebase so trending some key build metrics will help us gain some general insight into the trends of your quality.

    - **Active Bugs** – This is the number of bugs that are active at the time of the build
    - **Code Churn** – The cumulative number of lines of code that have been touched between the last build and this one.
    - **Code Coverage** – The total percentage of your codebase that is covered by the tests that are executed as part of the build
    - **Test Status** – The number of unit tests and their status

    When pulled together the representation of the Build Quality above shows that while code churn has been increasing the coverage has been dropping. While circumstantial, this would likely result in a reduction in quality and technical debt that will need to be paid back later. Indeed couple that with the reduction in bug count and increase in tests passing and you could construe that the wrong tests might also be being written. This signifies something to look into…

    ![image38](images/image38-7-7.png "image38") Figure: Build Success over Time
    { .post-img }

    If however I have many build I may not want to monitor each one in that level of detail. I can use the Build Success Over Time reports to figure out which ones I might want to delve into. Having some form of KPI’s for my builds allows me to see at a glance something that I might want to investigate and you can see above that both the Code Coverage and the Main Nightly Build are having issues. However the Continuous Integration build was doing poorly and now now improving.

    ![image44](images/image44-8-8.png "image44") Figure: Martin’s Fitbit Status
    { .post-img }

    It is important to be less concerned with individual values and instead be more focused on trends. You need to know when the team has fallen off the waggon.

    ## Creating custom reports

    Being able to create custom reports is a mixed blessing. It is incredibly powerful however you then have to create custom reports…

    ![image47](images/image47-9-9.png "image47")
    { .post-img }

    With TFS you get a complete Data Warehouse and Cube. The Data Warehouse is updated every 5 minutes and the cube once every few hours. This data has everything described above and more. It has data on Source Code, Work Item Tracking, Build, and Tests that is all related.

    ![image50](images/image50-10-10.png "image50") Figure: Report Builder & Excel
    { .post-img }

    You can use Report Build to create reports in Reporting Services that you can render in the web and schedule. You can also just simply create reports in Excel and either run them locally or put them on Excel Services.

    ![image56](images/image56-11-11.png "image56") Figure: Stories Released vs Unreleased
    { .post-img }

    I am no Excel reporting expert so I asked my colleague Steven Borg from Northwest Cadence for a few Excel TFS reports and he sent me this…

    ![image59](images/image59-12-12.png "image59") Figure: Test Case Automation
    { .post-img }

    …And this…

    ![image62](images/image62-13-13.png "image62") Figure: Defect by Severity and Cumulative Defect Detection Trend
    { .post-img }

    …mmmm data… and this one…

    However we do need to remember one thing….

    ![image65](images/image65-14-14.png "image65") Figure: Team Metrics only
    { .post-img }

    These may not be the metrics that you are looking for!

    ## Beyond the team, metrics that really matter

    If we look beyond the team we need metrics that scale a little better. The team metrics of code, code coverage, test coverage, and velocity are fine for a single team, however they do not work well for multiple teams.

    ![image68](images/image68-15-15.png "image68")
    { .post-img }

    What about that division you want to measure and create KPI’s for?

    There are two things to note before we get started on which KIP’s I am going to propose that we look at…

    First is some extensive Forrester analysis on metrics that saw that we often use the wrong metrics or focus on the wrong goal, also resulting in the wrong metrics. Effectively that metrics should match the goal and the goal, if we are going to be measuring an organisation, should be the delivery of value.

    The second is that The Sandish Group while re-examining the 50k some projects that they have listed in their analysis data base found an inverse relationship between projects deemed a success under PMI, and value delivered. So your goal should be delivering value and you better not use PMI to measure success…

    ![image74](images/image74-16-16.png "image74") Figure: Organisational Metrics from Agility Index
    { .post-img }

    There are few metrics that follow the evidence-based approaches described in “The Leaders Guide to Radical Management by Steven Denning” but one such emerging dataset is the Agility Index from Scrum.org. It looks at a number of areas shown above and concentrates on Value, Lead Time, and Quality.

    Within each area we have a number of data points that we collect. Each one brings a specific balance to the equation of wither we are providing value and how quickly that value can be implemented. And very specifically, each one is a leading metric. They are metrics that are directly related to the outcome and can often precede any negative results that we are trying to avoided.

    ![WP_20140225_12_10_41_Pro](images/WP_20140225_12_10_41_Pro-27-27.jpg "WP_20140225_12_10_41_Pro") Figure: Direct Evidence is Consolidated into one indicator
    { .post-img }

    The dataset that is the outcome of collecting the previous stats are then rolled up into a single indicator called the Agility Index. Scrum.org provides a tool for capturing the data and calculating the Agility Index in a consistent manor.

    ![image80](images/image80-18-18.png "image80") Figure: Relating outcome to value creation
    { .post-img }

    There are really two types of evidence available to us:

    > Evidence, broadly construed, is anything presented in support of an assertion:
    >
    > - Strongest type of evidence is that which provides direct proof of the validity of the assertion.
    > - Weakest type of evidence is that which is merely consistent with the assertion, but doesn’t rule out contradictory assertions, as in circumstantial evidence.

    This is Evidence-based Management (EBM) and while new in the world of software delivery it has been round for quite a while in the medical world.

    While we have the direct, strong, type of evidence from the left we still need  to look at the softer evidence of the right. These are the more circumstantial pieces that may or may not provide us with an assertion of affect on the strong evidence.

    If we look at the Skills, Knowledge, and understanding of the participants and the Practices, Tools, Architectures, and Standards that they have implemented to try and gain some insight into what might be limiting the direct evidence that we have on the value delivery. We can then take a look at things that we might change. Just as a doctor would gather some data before making a diagnosis.

    So what sorts of things might we want to take a look at?

    ![WP_20140225_12_13_06_Pro](images/WP_20140225_12_13_06_Pro-28-28.jpg "WP_20140225_12_13_06_Pro") Figure: Practice domains of Agility Index
    { .post-img }

    Out of these broad domains above Scrum.org have identified over 300 practices that have been ordered and provide an organisation with a deeper, if circumstantial, understanding of where they might want to spend some investment.

    What would you want to do? Try something random or that is hot in the market? Or use Evidence-based Management to observe and analyse how work is done, asses the capabilities of the people doing the work and identify the most likely improvements to add value.

    ![image89](images/image89-21-21.png "image89") Figure: Trend analysis of evidence
    { .post-img }

    If we go around that cycle of measure, diagnose, and then improve multiple times we can build up a picture of the trends within the organisation and a relationship between activates to improve and measureable results will emerge. This allows us to not only direct activity but to see the value derived for euros spent to implement improvements.

    Have you ever been asked to justify continued spending on activities that are designed to improve your processes?

    ![image92](images/image92-22-22.png "image92") Figure: Patterns within practices
    { .post-img }

    We should then be able to delve into each of the domains and identify which practice areas are improving and which are not to more explicitly diagnose additional improvements.

    ![image95](images/image95-23-23.png "image95") Figure: The Agility Index Calculator
    { .post-img }

    The Agility Index calculator rolls all of the gathered evidence up and aggregates the values across all three dimensions into a single KPI to allow us to track trends and ROI.

    ## Conclusion

    This is a game changer for the software industry at large and create a credible set of metrics for the first time that can be used to guide process improvement initiatives regardless of the framework used to deliver that improvement. Agility Path, SAFe, or Kanban can be measured equally.

    Do you want to use Evidence-based Management to improve your processes?
  FilePath: C:\Users\MartinHinshelwoodNKD\source\repos\NKDAgility.com\site\content\resources\blog\2014\2014-02-25-metrics-that-matter-with-evidence-based-management\index.md
  FolderPath: C:\Users\MartinHinshelwoodNKD\source\repos\NKDAgility.com\site\content\resources\blog\2014\2014-02-25-metrics-that-matter-with-evidence-based-management
- FrontMatter:
    title: Team Foundation server 2013 Update 2 RC is coming, are you ready?
    description: Get ready for TFS 2013 Update 2 RC! Join Microsoft's Upgrade Weekend for expert support and discover new features to enhance your development experience.
    ResourceId: VV_h0xkmY8J
    ResourceType: blog
    ResourceContentOrigin: Human
    ResourceImport: true
    ResourceImportId: 10381
    ResourceImportSource: Wordpress
    ResourceImportOriginalSource: Wordpress
    date: 2014-02-20
    weight: 840
    creator: Martin Hinshelwood
    layout: blog
    resourceTypes: blog
    slug: team-foundation-server-2013-update-2-rc-coming-ready
    aliases:
    - /resources/VV_h0xkmY8J
    aliasesArchive:
    - /blog/team-foundation-server-2013-update-2-rc-coming-ready
    - /team-foundation-server-2013-update-2-rc-coming-ready
    - /team-foundation-server-2013-update-2-rc-is-coming,-are-you-ready-
    - /blog/team-foundation-server-2013-update-2-rc-is-coming,-are-you-ready-
    - /team-foundation-server-2013-update-2-rc-is-coming--are-you-ready-
    - /blog/team-foundation-server-2013-update-2-rc-is-coming--are-you-ready-
    - /resources/blog/team-foundation-server-2013-update-2-rc-coming-ready
    tags: []
    categories:
    - Uncategorized
    preview: nakedalm-experts-visual-studio-alm-1-1.png
  BodyContent: |
    While naked ALM Consulting can help you install and configure TFS you may want to take advantage of the upcoming TFS Upgrade Weekend from Microsoft for free.

    Just as with the [TFS 2013 Preview](http://nkdagility.com/the-great-team-foundation-server-2013-upgrade-weekend/) Microsoft is putting its money where its mouth is and setting up a raft of experts that will be on hand to help you upgrade to TFS 2013 Update 2 RC. They are so confident that you will not have any problems that on Friday 28th February and Saturday 1st March they will have experts standing by to support you. All they ask is that you register for the [http://nkdalm.net/TFS13U2UpgradeWeekend](http://nkdalm.net/TFS13U2UpgradeWeekend) so that they know how many folks are taking advantage… you know… just in case.

    You have until 26th February to sign-up and if you are planning to upgrade there really is no reason not to take advantage of this offer…

    In case you are asking yourself why you would want to update to Update 2 RC you may want to take a look at the list of new features:

    - **Query on Tags** - At last we can now create queries against our tags
    - **Tags added to Visual Studio & Excel** - Now it is not only the web that can update tags. If you are using Visual Studio or Excel you will also be able to access them.
    - **API for Tags** - Tags we read only up until Update 2. We can now edit them from the existing API and from a new REST API
    - **Control Permissions on Tags** - The TFS team has heard our shouts and has implemented a new permission with 2013 Update 2 that allows us to control who can manipulate tags.
    - **Non-working days** - you can now control which days are excluded from the burndown in the web.
    - **Pining charts** - the new 2013 charts can now be pinned to the homepage giving you much more flexibility over the presentation of the data.
    - **Export Test Plan to HTML** - You can now export your test plan to HTML for offline reading/sharing, printing, etc. You can choose the level of detail you want to include in the document.
    - **Release Management Tags** \- Instead of targeting named machines in release management you can now target them via a tagging system. This should be the foundations of future improvements to simplify configuration
    - **Annotate for Git** - The blame tool comes to Git

    In addition the team has been working on a bunch of performance improvements for the backlog and other tools. There are many more features but these are the ones that I felt most relevant.
  FilePath: C:\Users\MartinHinshelwoodNKD\source\repos\NKDAgility.com\site\content\resources\blog\2014\2014-02-20-team-foundation-server-2013-update-2-rc-coming-ready\index.md
  FolderPath: C:\Users\MartinHinshelwoodNKD\source\repos\NKDAgility.com\site\content\resources\blog\2014\2014-02-20-team-foundation-server-2013-update-2-rc-coming-ready
- FrontMatter:
    title: Building a release pipeline with Release Management with Visual Studio 2013
    description: Learn to build a scalable release pipeline with Visual Studio 2013's integrated release management. Enhance your DevOps practices for continuous delivery success!
    ResourceId: Q7mjPfg5d4Q
    ResourceType: blog
    ResourceContentOrigin: Human
    ResourceImport: true
    ResourceImportId: 10372
    ResourceImportSource: Wordpress
    ResourceImportOriginalSource: Wordpress
    date: 2014-02-18
    weight: 315
    creator: Martin Hinshelwood
    layout: blog
    resourceTypes: blog
    slug: building-release-pipeline-release-management-visual-studio-2013
    aliases:
    - /resources/Q7mjPfg5d4Q
    aliasesArchive:
    - /blog/building-release-pipeline-release-management-visual-studio-2013
    - /building-release-pipeline-release-management-visual-studio-2013
    - /building-a-release-pipeline-with-release-management-with-visual-studio-2013
    - /blog/building-a-release-pipeline-with-release-management-with-visual-studio-2013
    - /resources/blog/building-release-pipeline-release-management-visual-studio-2013
    tags:
    - Software Development
    - Continuous Delivery
    - Frequent Releases
    - Technical Excellence
    - Technical Mastery
    - Working Software
    - Release Management
    - Product Delivery
    - Engineering Practices
    - Operational Practices
    - Application Lifecycle Management
    - Deployment Strategies
    - Metrics and Learning
    - Deployment Frequency
    - Pragmatic Thinking
    categories:
    - DevOps
    - Engineering Excellence
    preview: nakedalm-experts-visual-studio-alm-4-4.png
  BodyContent: |
    With the release of Visual Studio 2013 we now have release management built right into the product that we can use to build a scalable release pipeline. It's not perfect but it does hit the spot.

    Unlike the other Application Lifecycle Management (ALM) tools on the market Visual Studio ALM actually lives up to the name. Most tools out there tend to concentrate on a single angle. Specialising in work item tracking, source control or build. Then there are tools like HP ALM or Rational that have many integrated tools within the Application Development Lifecycle Management (ADLM) world. They in effect concentrate on the development side of the process and don't look at deployment or operations. This i think is a critical gap as we bring DevOps into the fold and integrate the developers and operational work to provide better products. Indeed many of the most successful products out there were developed by the same folk that support it. Why do they continue to be successful? When there is no one else to blame for the mess you end up just cleaning it up.

    ![clip_image001](images/clip_image001-1-1.png "clip_image001")  
    { .post-img }
    Figure: Introduction to Release Management in TFS 2013

    Visual Studio ALM has implemented the next level of delivery by incorporating continuous release right into the product. Notice that I said "Continuous Release" and not simply "Continuous Deployment". I was asked recently be a colleague if I had meant deployment, and if not, why release. Well, here was my answer:

    Deployment is for those folks that can't get it into Production. Releasing continuously with high quality is the real goal.

    Today's world of software delivery is very different from that of even a few years ago. Lean-agile thinkers have been pushing continuous quality, feedback and delivery for quite some time as the only way to really deliver acceptable quality. Now, with the modern application lifecycle even large enterprises are using cloud and delivering to mobile. We are all moving towards that ideal on continuous delivery and for many companies it is already a reality. Even if your organisation is not ready for continuous release you should be releasing continuously even if the only folks that use that environment are within your team.

    ![clip_image002](images/clip_image002-2-2.png "clip_image002")  
    { .post-img }
    Figure: Application lifecycle trends

    In the past it has only been the smaller development shops that have been able to deliver continuously, however as larger organisations see those smaller organisations taking their business they also need to change, if only to keep up. This is what happened to the Visual Studio ALM team with the 2010 release, they were behind the curve and they had to aggressively innovate to get ahead of it. With a modern development platform there really is no excuse to not delivering continuously. If your organisation wants any sort of business agility then they will need to be looking at agility in their supporting departments as they can only move at the pace of their slowest part.

    One of the most important supporting processes to both your development and operations teams when it comes to your software is something called a release pipeline. Even if you don’t know you have one, you do, you just may not have formalised it. A release pipeline is the process by which you get code from its text based from all the way through all of your environments to production. How long that takes can mean success or failure in the long run for your business. In a typical release pipeline we first compile our code and run unit tests before packaging it for deployment. This package would typically take the form of one or more binaries and this is what we are going to promote through our release pipeline.

    Teams have tried for many years to work within a source code promotion model and it just does not scale to agility and indeed is one of the many any-patterns that can make agility harder than it should be. Why should test, QA or even operations need to wait for you to merge and retest your code before they get a copy? More to the point, why are you changing it after it has been tested? Did you run all of your tests again? Your full regression? Really?

    Most teams, under pressure to deliver not only cherry-pick the changes that will be promoted between branch lines but almost always cherry-pick the tests that are run again. Your code coverage is only as good as the coverage attained since the last code change.

    To combat these many issues you need a solid binary promotion model for your release pipeline.

    ![clip_image003](images/clip_image003-3-3.png "clip_image003")  
    { .post-img }
    Figure: Release Pipeline in Visual Studio ALM

    Your release pipeline is usually made up of a number of instances of your application that you need to update between your build and your production systems. I have tended towards call these environments 'Feedback' rather than the traditional Dev, QA, and UAT as the old form pigeonholes them a little to specifically for what I want. All of these intermediary environments have one purpose and one purpose only, to provide the Development Team with feedback. Who the intended target is that you want to retrieve that feedback from is the only thing that really changes between instances.

    In the Release Pipeline for Visual Studio 2013 graphic above I have depicted four environments, the first two of which are owned and managed traditionally by the development team. The one constant we have through the flow is the tooling for deployment. We are using Release Management for Visual Studio 2013 for all of our environments however we are using different tooling to gather data from these environments. Feedback comes in many forms and while feedback from people provides the most value we sometimes need feedback from systems as well.

    ## Development Realm

    When we are within the realm of the Development Teams I am looking for feedback that will help the development team understand defects and usability issues. The team needs to understand if they have met the business requirements . They need data that tells them things about their test coverage as well as which tests are most impacted by the changes that my team had delivered since the last change. I want to know how much and often the code is changing as rapidly changing areas of my code may be problem areas as well as identifying where I my need my team to spend time refactoring.

    Here under the gaze of the Development Team we are mostly interested in functionality and quality:

    - **Test Impact** - Looking at how the changes that are being made to your system affect your tests is key to reducing the number of tests that you need to run and increasing your agility.
    - **Unit Test Coverage** - As a key quality indicator test coverage still needs to be taken with a pinch of salt. Look at test courage in conjunction with other metrics like code churn.
    - **Code Churn** - Code churn helps us understand how much or codebase is changing over time. Greater change is greater risk but again it is only an indicator.
    - **Acceptance Coverage** - Based on my acceptance criteria that I have pre-agreed with the business I need to know how good I am at meeting those criteria and when I am not how important it is to the product owner.

    These are all things that we need to look at as our software travels down our release pipeline. We need to be seeing an increase in maturity as our software makes it further towards production. The more often your push an increment of your software through the pipeline the more data points that you get. The more data points you get the more accurate your measure.

    While in the development realm we should be using Lab Management environments configured to execute our tests. The deployment capabilities of Lab Management were never that mature with the team that created it focusing more on vitalization than deployment. To relive this deficiency we can create dual environments so that we can [execute test automation as part of our release pipeline](http://nkdagility.com/execute-tests-release-management-visual-studio-2013/) with the new Release Management tools.

    ## Operational Realm

    As we get closer to production we start to need a different set of data we need operational data. This data is about what matters to those that are going to support your application even if it is the same folks as those that built it. Here we need to look at scalability and stability rather than meeting functional requirements. We need to monitor server performance, perhaps under load, and look at where there are gaps between support and action. All of this needs fed back to the Product Owner so that they can prioritise as needed to achieve the desired level of quality that meets the business needs.

    In the world of Operations and DevOps we need information on performance and stability:

    - **Performance Counters** - Within your operating system there are a plethora of capture able metrics that allow you to identify where you might have problems. Getting access to this data in a way that you can slice it across multiple servers will allow you to scale quickly.
    - **Load Test Results** - to really get good metrics we need to be able to exercise our applications. While it is hard to scale UI testing it is easy to scale up the calls that are a result of that testing. You can create recordings from scratch or intercept examples using Fiddler and turn them into parameterised and scalable tests that you can use to exercise your application to the brink.
    - **Incidents** \- how many incidents are required to be responded to by the team

    Hopefully you are in the process of merging your operations and development teams but that does not mean that terms like operations and devops go away. It only means that they become more designations of types of work rather than of who will perform that work.

    ## Conclusion

    When you put these together, sometimes with a little overlap depending on how progressive your teams are, you get a release pipeline that is, and should be, a challenging gauntlet for your software. Software that makes it through the trial by fire should be stable, and scalable as well as functional.

    In other words… Quality.
  FilePath: C:\Users\MartinHinshelwoodNKD\source\repos\NKDAgility.com\site\content\resources\blog\2014\2014-02-18-building-release-pipeline-release-management-visual-studio-2013\index.md
  FolderPath: C:\Users\MartinHinshelwoodNKD\source\repos\NKDAgility.com\site\content\resources\blog\2014\2014-02-18-building-release-pipeline-release-management-visual-studio-2013
- FrontMatter:
    title: Install Release Management 2013
    description: Learn to install and configure Release Management 2013 with Visual Studio 2013 in under 10 minutes. Streamline your deployment process effortlessly!
    ResourceId: 1m73SCOy0k9
    ResourceType: blog
    ResourceContentOrigin: Human
    ResourceImport: true
    ResourceImportId: 10353
    ResourceImportSource: Wordpress
    ResourceImportOriginalSource: Wordpress
    date: 2014-02-03
    weight: 1000
    creator: Martin Hinshelwood
    layout: blog
    resourceTypes: blog
    slug: install-release-management-2013
    aliases:
    - /resources/1m73SCOy0k9
    aliasesArchive:
    - /blog/install-release-management-2013
    - /install-release-management-2013
    - /resources/blog/install-release-management-2013
    tags:
    - Install and Configuration
    categories:
    - Uncategorized
    preview: nakedalm-experts-visual-studio-alm-1-1.png
  BodyContent: |
    Just recently I have been doing a lot of work in Release Management 2013 and noticed the lack of documentation. I have done a number of installs and documented them is posts like [Installing Release Management Client for Visual Studio 2013](http://nkdagility.com/installing-release-management-client-visual-studio-2013/ "Installing Release Management Client for Visual Studio 2013") and [Installing Release Management Server for TFS 2013](http://nkdagility.com/installing-release-management-server-tfs-2013/ "http://nkdagility.com/installing-release-management-server-tfs-2013/") however there is always things that don’t quite join up. I often have to head off an fix problems or get support as part of my posts an while I almost always blog those issues there is no joined up experience.

    \[youtube=https://www.youtube.com/watch?v=NG9Y1\_qQjvg\]

    Have you seen how easy it is to install Release Management 2013 and configure a full release management suite with Visual Studio 2013? See Martin install and configure the new Visual Studio 2013 Release Management Server, Client, and Deployment Agent all in under 10 minutes. Get going in minutes with a fully fledged release management tool

    - Release Management Server for Team Foundation Server 2013
    - Release Management Client for Visual Studio 2013
    - Microsoft Deployment Agent 2013

    Have you been using Release Management in Visual Studio 2013? How have you been getting on?
  FilePath: C:\Users\MartinHinshelwoodNKD\source\repos\NKDAgility.com\site\content\resources\blog\2014\2014-02-03-install-release-management-2013\index.md
  FolderPath: C:\Users\MartinHinshelwoodNKD\source\repos\NKDAgility.com\site\content\resources\blog\2014\2014-02-03-install-release-management-2013
- FrontMatter:
    title: Installing Release Management Server for TFS 2013
    description: Learn how to install the Release Management Server for TFS 2013 with this comprehensive guide. Simplify your setup and enhance your development workflow!
    ResourceId: 9xXIHbs9zmA
    ResourceType: blog
    ResourceContentOrigin: Human
    ResourceImport: true
    ResourceImportId: 10351
    ResourceImportSource: Wordpress
    ResourceImportOriginalSource: Wordpress
    date: 2014-01-30
    weight: 840
    creator: Martin Hinshelwood
    layout: blog
    resourceTypes: blog
    slug: installing-release-management-server-tfs-2013
    aliases:
    - /resources/9xXIHbs9zmA
    aliasesArchive:
    - /blog/installing-release-management-server-tfs-2013
    - /installing-release-management-server-tfs-2013
    - /installing-release-management-server-for-tfs-2013
    - /blog/installing-release-management-server-for-tfs-2013
    - /resources/blog/installing-release-management-server-tfs-2013
    tags:
    - Install and Configuration
    - System Configuration
    categories:
    - Uncategorized
  BodyContent: |
    Unless you have been living under a rock you might have noticed that Microsoft has added a Release Management tool to its Visual Studio product line. I have been playing with it for a while now and I think I have it figured out. However as this is a new addition to the product it is extremely poorly documented.

    I have just finished writing for the Release Management chapter in Professional Application Lifecycle Management with Visual Studio 2013 \[[Amazon USA](http://www.amazon.com/gp/product/1118836588/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=1118836588&linkCode=as2&tag=martinhinshe-20 "Professional Application Lifecycle Management with Visual Studio 2013 on Amazon USA") | [Amazon UK](http://www.amazon.co.uk/gp/product/1118836588/ref=as_li_ss_tl?ie=UTF8&camp=1634&creative=19450&creativeASIN=1118836588&linkCode=as2&tag=marthinssblog-21 "Professional Application Lifecycle Management with Visual Studio 2013 on Amazon UK")\] and while I covered the ALM aspects I did not really cover how to install the components. So here goes...

    You can either download the individual components from [the Visual Studio site](http://www.visualstudio.com/en-us/downloads#d-release-management) or if you have an MSDN account you can download an ISO with all three applications inside.

    Note: If you want to take the public download offline you can run "setup.exe /layout" and all of the components will be downloaded for offline use. Good for servers not on the internet.

    ![clip_image001](images/clip_image0012-1-1.png "clip_image001")  
    { .post-img }
    Figure: ISO for Release Management for Visual Studio 2013

    Here I am simply running the rm_server.exe in the 'Server' folder on the ISO.

    ![clip_image002](images/clip_image0022-2-2.png "clip_image002")  
    { .post-img }
    Figure: Installing Release Management Server for Team Foundation Server 2013

    Selecting the usual licence agreement the only thing of note is the size of the tool. At a mere 25MB this is a ridiculously quick install. Remember this is just putting the files on disk and registering any required DLL's and is not configuring it for use.

    ![clip_image003](images/clip_image0032-3-3.png "clip_image003")  
    { .post-img }
    Figure: Install Release Management Server for Team Foundation Server 2013

    There is little drama and it’s a small install that I think finished in just a few minutes, despite the long name :). At the end of the install you get a simple Launch button to launch the configuration tool. There is a link on the start menu as well.

    ![clip_image004](images/clip_image0041-4-4.png "clip_image004")  
    { .post-img }
    Figure: After install you need to launch the configuration

    Unfortunately as I noted with the Client tool the team has modelled the configuration on the old style Test Agent and Controller configuration rather than the more modern and featureful TFS configuration. I am hoping that this will end up as a node in the main Team Foundation Server configuration wizard. I would settle for the same configuration experience as the SharePoint extensions or Build Agents but what we have now is good enough.. Se la vie, however if it has been integrated it would likely have picked up the DB server and Ports if we were installing on our Team Foundation Server, which I believe to be the common case. Hopefully the TFS team will work on this for the next realese, or even better, in an update for 2013. For now, we are just configuring this as if it was a separate thing.

    ![clip_image005](images/clip_image0051-5-5.png "clip_image005")  
    { .post-img }
    Figure: Configuring Release Management Server

    In my configuration I am keeping the defaults. As I have the Release Management Server installed on the same server as TFS I will be unlikely have locked down communication between servers and can mostly get away with Network Service.

    Note You can cheat a little here by creating an Active Directory group and adding all of the machine accounts, that how Network Service authenticates, in it. You can then give permission to that group and remove the need for passwords or password changes of service accounts across many computers.

    Now all we have to do is apply the changes..

    ![clip_image006](images/clip_image0061-6-6.png "clip_image006")  
    { .post-img }
    Figure: All Configuration tasks have completed successfully

    And low… we have a Release Management Server for Team Foundation Server 2013… First configuration is a little tricky and I covered that in [Installing Release Management Client for Visual Studio 2013](http://nkdagility.com/installing-release-management-client-visual-studio-2013/)…
  FilePath: C:\Users\MartinHinshelwoodNKD\source\repos\NKDAgility.com\site\content\resources\blog\2014\2014-01-30-installing-release-management-server-tfs-2013\index.md
  FolderPath: C:\Users\MartinHinshelwoodNKD\source\repos\NKDAgility.com\site\content\resources\blog\2014\2014-01-30-installing-release-management-server-tfs-2013
- FrontMatter:
    title: Execute Tests with Release Management for Visual Studio 2013
    description: Learn to execute tests seamlessly with Release Management for Visual Studio 2013. Simplify your automated testing and deployment processes today!
    ResourceId: Jpm_MxV4e3G
    ResourceType: blog
    ResourceContentOrigin: Human
    ResourceImport: true
    ResourceImportId: 10342
    ResourceImportSource: Wordpress
    ResourceImportOriginalSource: Wordpress
    date: 2014-01-27
    weight: 640
    creator: Martin Hinshelwood
    layout: blog
    resourceTypes: blog
    slug: execute-tests-release-management-visual-studio-2013
    aliases:
    - /resources/Jpm_MxV4e3G
    aliasesArchive:
    - /blog/execute-tests-release-management-visual-studio-2013
    - /execute-tests-release-management-visual-studio-2013
    - /execute-tests-with-release-management-for-visual-studio-2013
    - /blog/execute-tests-with-release-management-for-visual-studio-2013
    - /resources/blog/execute-tests-release-management-visual-studio-2013
    tags:
    - Release Management
    - Software Development
    - Install and Configuration
    - Azure DevOps
    - Automated Testing
    - Product Delivery
    - Test Automation
    categories:
    - Uncategorized
    preview: nakedalm-experts-visual-studio-alm-7-7.png
  BodyContent: |
    Last week I was onsite with a customer who was trying to do automated release and test with Lab Management. I showed him a better way by execute Tests with Release Management for Visual Studio 2013.

    UPDATE Make sure that your build name does not contain any spaces as there is a [bug in the PowerShell script used by Release Management](http://blogs.msdn.com/b/mpeder/archive/2014/03/03/fixing-error-in-release-management-when-using-quot-mtm-automated-tests-manager-quot-build-name-restriction.aspx)

    If you have ever used Lab Management to do an automated build and release you will know that you can use the DefaultLabTemplate and select an environment that has been configured in Lab Management to do a deployment to. In TFS 2010 you had to be using Hyper-V before you could do automated deployment and one of the enhancements that was added to TFS 2012 was the ability to use [Standard Environments for automated deployment and testing](http://nkdagility.com/standard-environments-for-automated-deployment-and-testing/).

    While this was pretty good it was a little complicated to get working and need a little love care and attention to maintain. On top of that you had to configure environments within Lab Management and this installed 'developer tools' onto the boxes. This made it completely unusable for anything outside of a lab and we really wanted to be doing things the same all of the way through our release pipeline. I wrote a post on [Release Management in TFS 2012](http://nkdagility.com/release-management-with-team-foundation-server-2012/) in which I created a standard Nuget deployment package to unify the deployment while using Lab Management for Development and Testing environments and Octopus Deploy for UAT and Production.

    This worked well but ends up being rather convoluted and does not make the best use of the features that are available in both products. With the release of Release Management for Visual Studio 2013 we get some new capabilities and greater standardisation of the process.

    ![clip_image001](images/clip_image0011-1-1.png "clip_image001")  
    { .post-img }
    Figure: Release Pipeline in Visual Studio 2013

    Back with the customer, we were trying to get the deployment, a simple Xcopy process, to work via the Lab Management tool. If you have ever tried this you will know that it is very difficult to debug and you get lots of weird stuff to deal with. If we flip over to Release Management instead for deployment we can still execute tests but we get a much easier process to debug and create. The only caveat right now is that as Release Management is a bought in tool (it was InRelease) we don’t have compatibility between Lab Management environments and those used by Release Management and we will have to duplicate. I am hoping that in a future version we can configure environments that can be used for Lab, Release or both.

    So what do we have to do?

    ![clip_image002](images/clip_image0021-2-2.png "clip_image002")  
    { .post-img }
    Figure: Running Automated Tests with Release Management

    In this scenario we have to have Microsoft Test Manager (MTM) installed on the target server. It would be nice to not have to have it there but we need to be able to execute TCM.exe and for that we need MTM to be installed. Another one for the test team to look at, but really not a big deal.

    Note: If your 'target server' is off domain you will need to use Shadow Accounts. That is you create accounts of the same username and password on all of the computers that need to authenticate and magically they do. There is however a bug in Release Management that means that you not only have to add the "\[rmServer\]\\localAccount" account but also the "\[targetServer\]\\localAccount" as well. All of the Shadow Accounts will need Release Manager and Service Account permissions.

    So, the bit that took me a while to figure out is that as there are context specific assets associated with your tests you will need to get them down to the server under test. How this is achieved is to create a component in the Release Management Client so that the bits are specified. That is why there is no "Action" for MTM out of the box… you need a component.

    Head on over to "Configure Apps | Components | New" and when you fill out the Component you should make sure that you select he root of the build directory by putting a single "\\" in the 'path to package' and you can simply name it "Automated Tests". This single component will work for any build as they will always be in the same place.

    ![clip_image003](images/clip_image0031-3-3.png "clip_image003")  
    { .post-img }
    Figure: Creating a component for your automated tests

    After that you need to set the Tool that you want to execute as "MTM Automated Tests Manager" and it will automatically populate the rest with the variables you will need. There are some customisations that you may need to do depending on what you are doing. There are some hidden options of TcmExec.ps1 and the timeout of 5 minutes might be WAY too short for you needs. Very dependent on what you are trying to do, but for your first run I would keep it simple.

    ![clip_image004](images/clip_image004-4-4.png "clip_image004")  
    { .post-img }
    Figure: Setting the MTM Automated Tests Manager tool

    Now in "Configuration Apps | Release Templates" you need to open one of your release templates, in this case "Fabriakam Call Centre" and first add the component to your list of components and then to your workflow design surface at the point in time that you want to run the tests.

    ![clip_image005](images/clip_image005-5-5.png "clip_image005")  
    { .post-img }
    Figure: Adding the Automated Tests component to your Release Template

    Once you have the Automated Tests component added to the sequence you can fill out the details. If you are executing this deployment as part of a build deployment then you have a few handy short cut variables to use. To get the TFS Collection and Team Project you can use $(TfsUrlWithCollection) and $(TeamProject) respectively. In addition you can get the build directory by using the $(PackageLocation) variable.

    ![clip_image006](images/clip_image006-6-6.png "clip_image006")  
    { .post-img }
    Figure: Configuring the Automated Tests component

    One of the final variables that we need to set is that of the Test Environment that you want to execute the tests against. If you pop over to Lab Manager and create a Standard Environment, as noted above, that is actually the same environment that has been configured in Release Management you can fill out the environment name here.

    Now you can have automated tests executed as part of your deployment… carry on testing.
  FilePath: C:\Users\MartinHinshelwoodNKD\source\repos\NKDAgility.com\site\content\resources\blog\2014\2014-01-27-execute-tests-release-management-visual-studio-2013\index.md
  FolderPath: C:\Users\MartinHinshelwoodNKD\source\repos\NKDAgility.com\site\content\resources\blog\2014\2014-01-27-execute-tests-release-management-visual-studio-2013
- FrontMatter:
    title: Move your Active Directory domain to another server
    description: Learn how to successfully move your Active Directory domain to a new server with expert tips and a detailed video guide. Simplify your migration process!
    ResourceId: koQQ-rllpsw
    ResourceType: blog
    ResourceContentOrigin: Human
    ResourceImport: true
    ResourceImportId: 10334
    ResourceImportSource: Wordpress
    ResourceImportOriginalSource: Wordpress
    date: 2014-01-20
    weight: 1000
    creator: Martin Hinshelwood
    layout: blog
    resourceTypes: blog
    slug: move-your-active-directory-domain-to-another-server
    aliases:
    - /resources/koQQ-rllpsw
    aliasesArchive:
    - /blog/move-your-active-directory-domain-to-another-server
    - /move-your-active-directory-domain-to-another-server
    - /resources/blog/move-your-active-directory-domain-to-another-server
    tags:
    - System Configuration
    - Windows
    - Install and Configuration
    categories:
    - Uncategorized
    preview: metro-server-instances_thumb-2-2.png
  BodyContent: |
    I was trying to install TFS 2013 yesterday and I found that my local demo domain was not working. After a little investigation It looks like I was running Windows Server 2012 R2 Preview and it had just expired.

    ![image](images/image-1-1.png "image")  
    { .post-img }
    Figure: Can’t install TFS if your domain is not working

    So for the second time I had to navigate the treacherous jungle that is moving an Active Directory domain from one server to another. Changing your primary domain controller (PDC) is no simple task but if you hunt around ling enough on the internet you can pull together enough information to get it done.

    \[youtube=http://www.youtube.com/watch?v=yrpAYB2yIZU\]  
    Install & Configure 301 - Move your Active Directory domain to another server

    Just so you don’t miss anything you need to move:

    - Schema Master
    - Domain Naming Master
    - The relative identifier (RID) operations master
    - The primary domain controller (PDC) emulator operations master
    - The infrastructure operations master

    And lets not forget the Global Catalogue.

    The video documents my journey of moving my demo domain from one server to another and it currently looks like everything is working. Job done…

    How did you get on moving your domain?
  FilePath: C:\Users\MartinHinshelwoodNKD\source\repos\NKDAgility.com\site\content\resources\blog\2014\2014-01-20-move-your-active-directory-domain-to-another-server\index.md
  FolderPath: C:\Users\MartinHinshelwoodNKD\source\repos\NKDAgility.com\site\content\resources\blog\2014\2014-01-20-move-your-active-directory-domain-to-another-server
- FrontMatter:
    title: Installing TFS 2013 from scratch is easy
    description: Learn how to easily install TFS 2013 from scratch with step-by-step videos. Discover basic and advanced setups for efficient configuration and management.
    ResourceId: VfADNDpkNbe
    ResourceType: blog
    ResourceContentOrigin: Human
    ResourceImport: true
    ResourceImportId: 10332
    ResourceImportSource: Wordpress
    ResourceImportOriginalSource: Wordpress
    date: 2014-01-17
    weight: 1000
    creator: Martin Hinshelwood
    layout: blog
    resourceTypes: blog
    slug: installing-tfs-2013-scratch-easy
    aliases:
    - /resources/VfADNDpkNbe
    aliasesArchive:
    - /blog/installing-tfs-2013-scratch-easy
    - /installing-tfs-2013-scratch-easy
    - /installing-tfs-2013-from-scratch-is-easy
    - /blog/installing-tfs-2013-from-scratch-is-easy
    - /resources/blog/installing-tfs-2013-scratch-easy
    tags:
    - Install and Configuration
    - System Configuration
    categories:
    - Uncategorized
  BodyContent: |
    It had been a while since I installed TFS from scratch and I had a few questions from a customer on the subject. So instead of creating yet another installing TFS post I decided to create a couple of videos instead.

    In the first video I used the Basic Install option. This installs TFS with SQL Express and is the easiest setup. Instead of having to do a bunch of manual steps you just click and go. Fully configured TFS in no time. On top of that it will even configure SharePoint Foundation 2013 for you (not supported on Server 2012 R2 until SP1.) The only thing that you are missing is Reporting and that is only because SQL Express does not support Reporting Services or Analysis Services. You can however upgrade later if you feel the need easily.

    \[youtube=http://www.youtube.com/watch?v=U7wIQk1pus0\]  
    Figure: Install & Configure 101 - TFS 2013 Basic Installation

    If that's not for you and you like things a little bit more complicated you can install SQL Server first and then use the Standard Single Server install. Here I install SQL Server 2012 with all of the trimmings, Reporting and Analysis Services. I then let TFS do all of the heavy lifting of configuration and setup of all of the features. This results in a full install of TFS with a Cube and Data Warehouse but no SharePoint as it is not supported on Server 2012 R2 until the release of SharePoint 2013 SP1.

    \[youtube=http://www.youtube.com/watch?v=U69JMzIZXro\]  
    Figure: Install & Configure 101 - TFS 2013 Standard Single Server Install

    This should give you some idea on how to install and configure TFS and how easy it is. Managing TFS is mostly, apart from configuring a backup, a leave alone statement. It mostly manages and maintains itself until you get to large database sizes. And by large I mean terabytes :)

    How did you get on with your TFS installs?
  FilePath: C:\Users\MartinHinshelwoodNKD\source\repos\NKDAgility.com\site\content\resources\blog\2014\2014-01-17-installing-tfs-2013-scratch-easy\index.md
  FolderPath: C:\Users\MartinHinshelwoodNKD\source\repos\NKDAgility.com\site\content\resources\blog\2014\2014-01-17-installing-tfs-2013-scratch-easy
- FrontMatter:
    title: Change the Release Management Server that your Client connects to
    description: Learn how to easily change the Release Management Server your client connects to, ensuring smooth transitions between different environments. Get started now!
    ResourceId: zTjRpmZ5cJQ
    ResourceType: blog
    ResourceContentOrigin: Human
    ResourceImport: true
    ResourceImportId: 10329
    ResourceImportSource: Wordpress
    ResourceImportOriginalSource: Wordpress
    date: 2014-01-13
    weight: 1000
    creator: Martin Hinshelwood
    layout: blog
    resourceTypes: blog
    slug: change-release-management-server-client-connects
    aliases:
    - /resources/zTjRpmZ5cJQ
    aliasesArchive:
    - /blog/change-release-management-server-client-connects
    - /change-release-management-server-client-connects
    - /change-the-release-management-server-that-your-client-connects-to
    - /blog/change-the-release-management-server-that-your-client-connects-to
    - /resources/blog/change-release-management-server-client-connects
    tags:
    - System Configuration
    - Install and Configuration
    categories:
    - Uncategorized
  BodyContent: |
    As a consultant I am onsite at a different customer every week and as I use my own laptop for most engagements I need to be able to change the Release Management Server that I connect to from the thick client.

    ![clip_image001](images/clip_image001-1-1.png "clip_image001")  
    { .post-img }
    Figure: The Release Management Client

    The Release Management team kindly added a UI to allow us to change which server that we are connected to. Open you RM client ad head over to "Administration | Settings| System Setting" and you can then click the "Edit" button next to the current "Release Management Server URL".

    ![clip_image002](images/clip_image002-2-2.png "clip_image002")  
    { .post-img }
    Figure: Editing the configured Release Management Server

    However if you try to open the client without being able to access that server you get an error message and you are unable to get to that screen to change the server URL. It would have been nice if it just asked us if we wanted to reconfigure and launched the original configuration dialog, however that is not the case.

    ![clip_image003](images/clip_image003-3-3.png "clip_image003")  
    { .post-img }
    Figure: Can't open Release Management Client with no Server available

    By default the port of your RM server is 1000 but you may have changed it so you need to know both the port and the server. Unfortunately if your correct release management server is unavailable then the client will error our and close.

    In order to work around this you need to change the URL that tells the Release Management Client to connect to that specifc server and it is fairly well hidden. You need to head over to the Microsoft.TeamFoundation.Release.Data.dll.config file and update it manually.

    The Release Management team however have created a handy utility that may make it a little quicker. You can run ReleaseManagementConsoleAdjustConfigFile.exe and pass in both the configuration that you want to change and the configuration property along with the value.

    > C:\\Program Files (x86)\\Microsoft Visual Studio 12.0\\Release Management\\bin\\ReleaseManagementConsoleAdjustConfigFile.exe –configfilename   .\\Microsoft.TeamFoundation.Release.Data.dll.config -newwebserverurl http://bvtirserverpod1:1000

    In this way you can update the server when you move from site to site. If you switch between client sites often it might be useful to create batch files on your desktop for launching the client with the right connection. Just call the connection change and then launch the app. Simples...
  FilePath: C:\Users\MartinHinshelwoodNKD\source\repos\NKDAgility.com\site\content\resources\blog\2014\2014-01-13-change-release-management-server-client-connects\index.md
  FolderPath: C:\Users\MartinHinshelwoodNKD\source\repos\NKDAgility.com\site\content\resources\blog\2014\2014-01-13-change-release-management-server-client-connects
- FrontMatter:
    title: Installing Release Management Client for Visual Studio 2013
    description: Learn how to install the Release Management Client for Visual Studio 2013 effortlessly. Streamline your release pipeline with this quick and easy guide!
    ResourceId: FuoEoqFDuqx
    ResourceType: blog
    ResourceContentOrigin: Human
    ResourceImport: true
    ResourceImportId: 10321
    ResourceImportSource: Wordpress
    ResourceImportOriginalSource: Wordpress
    date: 2014-01-10
    weight: 900
    creator: Martin Hinshelwood
    layout: blog
    resourceTypes: blog
    slug: installing-release-management-client-visual-studio-2013
    aliases:
    - /resources/FuoEoqFDuqx
    aliasesArchive:
    - /blog/installing-release-management-client-visual-studio-2013
    - /installing-release-management-client-visual-studio-2013
    - /installing-release-management-client-for-visual-studio-2013
    - /blog/installing-release-management-client-for-visual-studio-2013
    - /resources/blog/installing-release-management-client-visual-studio-2013
    tags:
    - Install and Configuration
    - Software Development
    - System Configuration
    categories:
    - Uncategorized
  BodyContent: |
    With the addition of the new Release Management Client for Visual Studio 2013 to the Visual Studio ALM roundup many folks are going to be giving it a go. If you might remember some of my post during the preview days of this tool there were some issue with installing it. It looks like Microsoft has gotten most of them sorted out and I can now get everything installed.

    The Release Management Client for Visual Studio 2013 allows you to create and configure all aspects of your release pipeline. You can configure environments from servers and stages of binary promotion with workflow and parameters for deployment at each stage to any environment.

    ![](images/011014_1034_READYInstal1-1-1.png)  
    { .post-img }
    Figure: Running the installer from the ISO

    You can either download the web installers from the [public website](http://www.visualstudio.com/en-us/downloads) or you can download the ISO from MSDN if you have an account. If you go with the pubic downloads you will need an active internet connection. If however you want to download the contents for later use you can use the "whatever.exe /layout" option and have the files downloaded locally for later.

    ![](images/011014_1034_READYInstal2-2-2.png)  
    { .post-img }
    Figure: Installing the Release Management Client

    Within a few seconds you will have the client installed. It is a simple SPF application and thus this is an extremely quick install.

    I like that the team managed to update the installers. With the early preview releases of Release Management, there were many issues with installation that really needed to be addresses and this was quick and painless. Like it should be.

    ![](images/011014_1034_READYInstal3-3-3.png)  
    { .post-img }
    Figure: Configured client

    When you first launch the Release Management client you will be asked to select the server and port where you installed the server. Once you have done this the client will open easily and quickly. If you get an error at this point it is likely a communication problem between you and the server.

    One thing that you should make sure of is that you add users to Release Management as soon as you can. The only user that is added initially is the account of the user that installed the server. This will be under the heading of 'Admin' in "Administration | Users". Don't get caught short and unable to access your server if you used another account to install the Server and the client.

    You should now be ready to go...
  FilePath: C:\Users\MartinHinshelwoodNKD\source\repos\NKDAgility.com\site\content\resources\blog\2014\2014-01-10-installing-release-management-client-visual-studio-2013\index.md
  FolderPath: C:\Users\MartinHinshelwoodNKD\source\repos\NKDAgility.com\site\content\resources\blog\2014\2014-01-10-installing-release-management-client-visual-studio-2013
- FrontMatter:
    title: Error adding Active Directory Group to Release Management Client in Visual Studio 2013
    description: Discover how to resolve the unhandled exception when adding Active Directory groups in Visual Studio 2013's Release Management Client. Get expert tips now!
    ResourceId: k9Ptd76Xl_1
    ResourceType: blog
    ResourceContentOrigin: Human
    ResourceImport: true
    ResourceImportId: 10316
    ResourceImportSource: Wordpress
    ResourceImportOriginalSource: Wordpress
    date: 2014-01-07
    weight: 1000
    creator: Martin Hinshelwood
    layout: blog
    resourceTypes: blog
    slug: error-adding-active-directory-group-to-release-management-client-in-visual-studio-2013
    aliases:
    - /resources/k9Ptd76Xl_1
    aliasesArchive:
    - /blog/error-adding-active-directory-group-to-release-management-client-in-visual-studio-2013
    - /error-adding-active-directory-group-to-release-management-client-in-visual-studio-2013
    - /resources/blog/error-adding-active-directory-group-to-release-management-client-in-visual-studio-2013
    tags:
    - Troubleshooting
    categories:
    - Uncategorized
  BodyContent: |
    When you try to add a group from Active Directory in the Release Management Client in Visual Studio 2013 you get an unhandled exception.

    When trying to add an Active Directory group to release management the other day I saw a little popup after adding the group that disappeared too quickly to action. I noticed that the group that I was trying to add did not end up in the list so I gave it another go.

    ![](images/010714_0741_ReadyErrora1-1-1.png)  
    { .post-img }
    Figure: Some requested groups could not be added

    > An unhandled exception has occurred in the application: Some requested groups could not be added. Either the groups are outside your domain, or it does not follow the LDAP protocol.

    ### Applies to

    - Release Management Client for Visual Studio 2013

    ### Findings

    The first annoying thing is the poor implementation of the error alerts. The toast notification is very time limited and there is no way to get top a list of errors that I can see. If you miss it its gone...

    I tried adding different groups to make sure it was not an AD issue with that group and the error persisted. I don't have multiple Active Directory implementations so I can't try other configurations but it looks to be consistent. When you click "Administration | Manage Groups | New | New from AD" the "Select Groups" box initially has the local computer selected rather than the domain.

    ![](images/010714_0741_ReadyErrora2-2-2.png)  
    { .post-img }
    Figure: New from Active Directory

    Under normal circumstances you would select "Entire Directory" and enter the entire or partial name of the group, in this case "Domain Users".

    Note: Never actually use Domain Users in a production system. I know that I only have 20 or so users in my entire AD system as it is only for testing. TFS has a limit of around 5k users, however I do not know what the Release Management Client has been tested to.

    ![](images/010714_0741_ReadyErrora3-3-3.png)  
    { .post-img }
    Figure: Select the scope of the directory search

    However if you have "Entire Directory" selected the result will be the Untangled Exception identified above. If you click "View Stack Trace" on that exception then you get the following details.

    ![](images/010714_0741_ReadyErrora4-4-4.png)  
    { .post-img }
    Figure: Some requested groups could not be added

    > An unhandled exception has occurred in the application: Some requested groups could not be added. Either the groups are outside your domain, or it does not follow the LDAP protocol.

    Obviously this is incorrect as I only selected a single group from Active Directory.

    ### Solution

    While this is annoying and should be easy to fix in the original code it obviously slipped through the test matrix and will likely be fixed on the next release. For now you can select the individual domain instead of the "Entire Directory" option.

    ![](images/010714_0741_ReadyErrora5-5-5.png)  
    { .post-img }
    Figure: Select exact domain

    In this case if I select "env.nakedalmweb.wpengine.com" as the exact domain that the group that I am trying to add exists in then the group is added with no issues.
  FilePath: C:\Users\MartinHinshelwoodNKD\source\repos\NKDAgility.com\site\content\resources\blog\2014\2014-01-07-error-adding-active-directory-group-to-release-management-client-in-visual-studio-2013\index.md
  FolderPath: C:\Users\MartinHinshelwoodNKD\source\repos\NKDAgility.com\site\content\resources\blog\2014\2014-01-07-error-adding-active-directory-group-to-release-management-client-in-visual-studio-2013

